CUDA_DEVICE_MAX_CONNECTIONS=1 torchrun --nproc_per_node=2 /opt/megatron-lm/pretrain_gpt.py \
    --init-method-std 0.014 \
    --disable-bias-linear \
    --use-rope-scaling \
    --swiglu \
    --use-rotary-position-embeddings \
    --num-layers 16 \
    --hidden-size 2048 \
    --num-attention-heads 32 \
    --ffn-hidden-size 8192 \
    --kv-channels 64 \
    --group-query-attention \
    --position-embedding-type rope \
    --attention-backend fused \
    --num-query-groups 8 \
    --normalization RMSNorm \
    --attention-dropout 0.0 \
    --hidden-dropout 0.0 \
    --tensor-model-parallel-size 1 \
    --pipeline-model-parallel-size 1 \
    --seq-length 8192 \
    --max-position-embeddings 8192 \
    --micro-batch-size 1 \
    --global-batch-size 8 \
    --train-iters 5 \
    --log-interval 1 \
    --tokenizer-type SentencePieceTokenizer \
    --tokenizer-model /opt/data/tokenizers/sentencepiece/tokenizer.model \
    --vocab-size 131072 \
    --save-interval 5 \
    --eval-interval 5 \
    --eval-iters 4 \
    --load /path/to/mbridge/ckpt \
    --save /path/to/save/ckpt \
    --ckpt-format torch_dist \
    --log-progress \
    --bf16 \
    --lr 4.5e-4 \
    --min-lr 4.5e-5 \
    --num-workers 2 \
    --tensorboard-dir /workspace/tb \
    --log-straggler \
    --log-interval 1 \
    --log-throughput \
    --no-load-optim
