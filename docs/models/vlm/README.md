# Vision Language Models

This directory contains documentation for Vision Language Models (VLMs) supported by Megatron Bridge. These models combine vision and language capabilities for multimodal AI applications.

## Available Models

Megatron Bridge supports the following VLM families:

| Model | Documentation | Description |
|-------|---------------|-------------|
| **Gemma 3 VL** | [gemma3-vl.md](gemma3-vl.md) | Google Gemma 3 Vision Language model |
| **Nemotron Nano V2 VL** | [nemotron-nano-v2-vl.md](nemotron-nano-v2-vl.md) | NVIDIA Nemotron Nano V2 Vision Language model |
| **Qwen2.5 VL** | [qwen2.5-vl.md](qwen2.5-vl.md) | Alibaba Cloud Qwen2.5 Vision Language model |
| **Qwen3 VL** | [qwen3-vl.md](qwen3-vl.md) | Alibaba Cloud Qwen3 Vision Language model |

## Quick Navigation

### I want to...

**ğŸ” Find a specific VLM model**
â†’ Browse the model list above or use the [index page](index.md)

**ğŸ”„ Convert models between formats**
â†’ Each model page includes conversion examples for Hugging Face â†” Megatron Bridge

**ğŸš€ Get started with training**
â†’ See [Training Documentation](../../training/index.md) for training guides

**ğŸ“š Understand VLM architecture**
â†’ Each model page documents vision-language architecture features

**ğŸ”§ Add support for a new VLM**
â†’ Refer to [Adding New Models](../../adding-new-models.md)

## Related Documentation

- **[Models Overview](../index.md)** - Return to main models documentation
- **[Large Language Models](../llm/index.md)** - LLM model documentation
- **[Training Guides](../../training/index.md)** - Training and customization guides
- **[Bridge Guide](../../bridge-guide.md)** - Working with Hugging Face models
- **[Adding New Models](../../adding-new-models.md)** - Extending model support

## Vision Language Model Features

VLMs typically support:

- **Image Understanding** - Processing and understanding visual inputs
- **Multimodal Fusion** - Combining vision and language representations
- **Vision-Language Tasks** - Image captioning, visual question answering, and more
- **Cross-Modal Learning** - Learning relationships between visual and textual data

---

**Ready to explore?** Choose a model from the list above or return to the [main documentation index](../../index.md).

