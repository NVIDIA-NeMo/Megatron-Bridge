model:
  tensor_model_parallel_size: 1
  sequence_parallel: false
  pipeline_model_parallel_size: 1
  context_parallel_size: 4
  crossattn_emb_size: 5120
  hidden_size: 5120
  ffn_hidden_size: 13824
  num_attention_heads: 40
  num_layers: 40
  qkv_format: thd
  seq_length: 2048 # This is not used

train:
  global_batch_size: 64
  micro_batch_size: 1
  eval_iters: 0

scheduler:
  lr_decay_style: constant
  lr_warmup_iters: 0

optimizer:
  lr: 5e-6
  min_lr: 5e-6

dataset:
  seq_length: 2048 # This is not used
  global_batch_size: 64
  micro_batch_size: 1

logger:
  log_interval: 1
