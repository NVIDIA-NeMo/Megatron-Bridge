_target_: megatron.bridge.training.config.ConfigContainer
checkpoint:
  _target_: megatron.bridge.training.config.CheckpointConfig
  async_save: false
  ckpt_assume_constant_structure: false
  ckpt_convert_format: null
  ckpt_convert_save: null
  ckpt_format: torch_dist
  ckpt_step: null
  dist_ckpt_strictness: assume_ok_unexpected
  exit_on_missing_checkpoint: false
  finetune: false
  fully_parallel_load: false
  fully_parallel_save: true
  load: null
  load_main_params_from_ckpt: false
  load_optim: true
  load_rng: true
  non_persistent_ckpt_type: null
  non_persistent_global_ckpt_dir: null
  non_persistent_local_ckpt_algo: fully_parallel
  non_persistent_local_ckpt_dir: null
  non_persistent_save_interval: null
  pretrained_checkpoint: null
  replication: false
  replication_factor: 2
  replication_jump: null
  save: /opt/Megatron-Bridge/examples/recipes/llama/nemo_experiments/default/checkpoints
  save_interval: 2000
  save_optim: true
  save_rng: true
  use_checkpoint_args: false
  use_persistent_ckpt_worker: true
comm_overlap:
  _target_: megatron.bridge.training.comm_overlap.CommOverlapConfig
  align_param_gather: null
  batch_p2p_comm: null
  bucket_size: null
  data_parallel_size: null
  defer_embedding_wgrad_compute: null
  overlap_grad_reduce: null
  overlap_p2p_comm: null
  overlap_param_gather: null
  overlap_param_gather_with_optimizer_step: null
  tp_comm_bootstrap_backend: null
  tp_comm_overlap: false
  tp_comm_overlap_cfg: null
  wgrad_deferral_limit: null
dataset:
  _target_: megatron.bridge.training.config.GPTDatasetConfig
  add_extra_token_to_sequence: true
  blend: null
  blend_per_split: null
  create_attention_mask: true
  data_sharding: true
  dataloader_type: single
  drop_last_partial_validation_sequence: true
  eod_mask_loss: false
  full_validation: null
  mid_level_dataset_surplus: 0.005
  mmap_bin_files: true
  mock: true
  multiple_validation_sets: null
  num_dataset_builder_threads: 1
  num_workers: 8
  object_storage_cache_path: null
  path_to_cache: null
  persistent_workers: false
  pin_memory: true
  random_seed: 1234
  reset_attention_mask: false
  reset_position_ids: false
  sequence_length: 8192
  split: 1,1,1
  split_matrix:
  - - 0
    - 0.3333333333333333
  - - 0.3333333333333333
    - 0.6666666666666666
  - - 0.6666666666666666
    - 1.0
  tokenizer: null
ddp:
  _target_: megatron.core.distributed.distributed_data_parallel_config.DistributedDataParallelConfig
  align_param_gather: false
  average_in_collective: true
  bucket_size: null
  check_for_large_grads: false
  check_for_nan_in_grad: true
  data_parallel_sharding_strategy: no_shard
  fp8_param_gather: false
  fsdp_double_buffer: false
  grad_reduce_in_fp32: true
  gradient_reduce_div_fusion: true
  keep_fp8_transpose_cache: false
  nccl_ub: false
  num_distributed_optimizer_instances: 1
  outer_dp_sharding_strategy: no_shard
  overlap_grad_reduce: true
  overlap_param_gather: true
  pad_buckets_for_high_nccl_busbw: false
  preserve_fp32_weights: true
  reuse_grad_buf_for_mxfp8_param_ag: false
  suggested_communication_unit_size: null
  use_custom_fsdp: false
  use_distributed_optimizer: true
  use_megatron_fsdp: false
dist:
  _target_: megatron.bridge.training.config.DistributedInitConfig
  align_grad_reduce: true
  distributed_backend: nccl
  distributed_timeout_minutes: 10
  enable_megatron_core_experimental: false
  external_gpu_device_mapping: false
  high_priority_stream_groups: null
  lazy_init: false
  local_rank: 2
  nccl_communicator_config_path: null
  sharp_enabled_group: null
  use_gloo_process_groups: true
  use_sharp: false
  use_torch_fsdp2: false
  use_tp_pp_dp_mapping: false
ft: null
logger:
  _target_: megatron.bridge.training.config.LoggerConfig
  filter_warnings: true
  log_energy: false
  log_interval: 10
  log_loss_scale_to_tensorboard: true
  log_memory_to_tensorboard: false
  log_params_norm: false
  log_progress: false
  log_throughput: false
  log_timers_to_tensorboard: false
  log_validation_ppl_to_tensorboard: false
  log_world_size_to_tensorboard: false
  logging_level: 20
  modules_to_filter: null
  set_level_for_all_loggers: false
  tensorboard_dir: /opt/Megatron-Bridge/examples/recipes/llama/nemo_experiments/default/tb_logs
  tensorboard_log_interval: 1
  tensorboard_queue_size: 1000
  timing_log_level: 0
  timing_log_option: minmax
  wandb_entity: null
  wandb_exp_name: null
  wandb_project: null
  wandb_save_dir: null
mixed_precision: bf16_mixed
model:
  _target_: megatron.bridge.models.llama.llama_provider.Llama3ModelProvider8B
  account_for_embedding_in_pipeline_split: false
  account_for_loss_in_pipeline_split: false
  activation_func:
    _call_: false
    _target_: torch.nn.functional.silu
  activation_func_fp8_input_store: false
  add_bias_linear: false
  add_qkv_bias: false
  apply_query_key_layer_scaling: false
  apply_residual_connection_post_layernorm: false
  apply_rope_fusion: true
  async_tensor_model_parallel_allreduce: false
  attention_backend:
    _args_:
    - 5
    _call_: true
    _target_: megatron.core.transformer.enums.AttnBackend
  attention_dropout: 0.0
  attention_softmax_in_fp32: false
  autocast_dtype:
    _call_: false
    _target_: torch.float32
  barrier_with_L1_time: true
  batch_p2p_comm: true
  batch_p2p_sync: true
  bf16: false
  bias_activation_fusion: true
  bias_dropout_fusion: true
  calculate_per_token_loss: false
  clone_scatter_output_in_embedding: true
  config_logger_dir: ''
  context_parallel_size: 2
  cp_comm_type: null
  cpu_offloading: false
  cpu_offloading_activations: true
  cpu_offloading_num_layers: 0
  cpu_offloading_weights: true
  cross_entropy_fusion_impl: native
  cross_entropy_loss_fusion: true
  cuda_graph_retain_backward_graph: false
  cuda_graph_scope: full
  cuda_graph_use_single_mempool: false
  cuda_graph_warmup_steps: 3
  deallocate_pipeline_outputs: true
  defer_embedding_wgrad_compute: false
  delay_wgrad_compute: false
  deterministic_mode: false
  disable_bf16_reduced_precision_matmul: false
  disable_parameter_transpose_cache: false
  distribute_saved_activations: null
  embedding_init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.01
  embedding_init_method_std: 0.01
  enable_autocast: false
  enable_cuda_graph: false
  expert_model_parallel_size: 1
  expert_tensor_parallel_size: 1
  external_cuda_graph: false
  ffn_hidden_size: 14336
  finalize_model_grads_func: null
  first_last_layers_bf16: false
  flash_decode: false
  fp16: false
  fp16_lm_cross_entropy: false
  fp32_residual_connection: false
  fp8: null
  fp8_amax_compute_algo: most_recent
  fp8_amax_history_len: 1
  fp8_dot_product_attention: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_multi_head_attention: false
  fp8_param: false
  fp8_recipe: delayed
  fp8_wgrad: true
  gated_linear_unit: true
  generation_config: null
  grad_scale_func: null
  grad_sync_func: null
  gradient_accumulation_fusion: true
  hetereogenous_dist_checkpoint: false
  heterogeneous_block_specs: false
  hidden_dropout: 0.0
  hidden_size: 4096
  hierarchical_context_parallel_sizes: null
  inference_rng_tracker: false
  init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.01
  init_method_std: 0.01
  init_model_with_meta_device: false
  is_hybrid_model: false
  kv_channels: 128
  layernorm_epsilon: 1.0e-05
  layernorm_zero_centered_gamma: false
  make_vocab_size_divisible_by: 128
  mamba_head_dim: 64
  mamba_num_groups: 8
  mamba_num_heads: null
  mamba_state_dim: 128
  masked_softmax_fusion: true
  memory_efficient_layer_norm: false
  microbatch_group_size_per_vp_stage: 1
  mlp_chunks_for_prefill: 1
  moe_apply_probs_on_input: false
  moe_aux_loss_coeff: 0.0
  moe_deepep_num_sms: 20
  moe_enable_deepep: false
  moe_expert_capacity_factor: null
  moe_extended_tp: false
  moe_ffn_hidden_size: null
  moe_grouped_gemm: false
  moe_input_jitter_eps: null
  moe_layer_freq: 1
  moe_layer_recompute: false
  moe_pad_expert_input_to_capacity: false
  moe_per_layer_logging: false
  moe_permute_fusion: false
  moe_router_bias_update_rate: 0.001
  moe_router_dtype: null
  moe_router_enable_expert_bias: false
  moe_router_force_load_balancing: false
  moe_router_fusion: false
  moe_router_group_topk: null
  moe_router_load_balancing_type: aux_loss
  moe_router_num_groups: null
  moe_router_padding_for_fp8: false
  moe_router_pre_softmax: false
  moe_router_score_function: softmax
  moe_router_topk: 2
  moe_router_topk_limited_devices: null
  moe_router_topk_scaling_factor: null
  moe_shared_expert_intermediate_size: null
  moe_shared_expert_overlap: false
  moe_token_dispatcher_type: allgather
  moe_token_drop_policy: probs
  moe_token_dropping: false
  moe_use_legacy_grouped_gemm: false
  moe_z_loss_coeff: null
  mrope_section: null
  mtp_enabled: false
  mtp_loss_scaling_factor: null
  mtp_num_layers: null
  multi_latent_attention: false
  no_rope_freq: null
  no_sync_func: null
  normalization: RMSNorm
  num_attention_heads: 32
  num_layers: 32
  num_layers_at_end_in_bf16: 1
  num_layers_at_start_in_bf16: 1
  num_layers_in_first_pipeline_stage: null
  num_layers_in_last_pipeline_stage: null
  num_microbatches_with_partial_activation_checkpoints: null
  num_moe_experts: null
  num_query_groups: 8
  output_layer_init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.00125
  overlap_moe_expert_parallel_comm: false
  overlap_p2p_comm: false
  overlap_p2p_comm_warmup_flush: false
  parallel_output: true
  param_sync_func: null
  params_dtype:
    _call_: false
    _target_: torch.float32
  perform_initialization: true
  persist_layer_norm: false
  pipeline_dtype: null
  pipeline_model_parallel_comm_backend: null
  pipeline_model_parallel_layout: null
  pipeline_model_parallel_size: 1
  position_embedding_type: rope
  qk_layernorm: false
  quant_recipe: null
  recompute_granularity: null
  recompute_method: null
  recompute_modules:
  - core_attn
  recompute_num_layers: null
  rotary_base: 500000
  rotary_interleaved: false
  rotary_percent: 1.0
  scatter_embedding_sequence_parallel: true
  seq_len_interpolation_factor: null
  seq_length: 8192
  sequence_parallel: false
  share_embeddings_and_output_weights: false
  should_pad_vocab: false
  softmax_scale: null
  symmetric_ar_type: null
  tensor_model_parallel_size: 1
  test_mode: false
  timers: null
  tp_comm_atomic_ag: false
  tp_comm_atomic_rs: false
  tp_comm_bootstrap_backend: nccl
  tp_comm_bulk_dgrad: true
  tp_comm_bulk_wgrad: true
  tp_comm_overlap: false
  tp_comm_overlap_ag: true
  tp_comm_overlap_cfg: null
  tp_comm_overlap_disable_fc1: false
  tp_comm_overlap_disable_qkv: false
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_split_ag: true
  tp_comm_split_rs: true
  tp_only_amax_red: false
  transformer_impl: transformer_engine
  transformer_layer_spec:
    _call_: false
    _target_: megatron.bridge.models.gpt_provider.default_layer_spec
  use_cpu_initialization: false
  use_fused_weighted_squared_relu: false
  use_kitchen: false
  use_mamba_mem_eff_path: true
  use_ring_exchange_p2p: false
  use_te_activation_func: false
  use_te_rng_tracker: false
  use_transformer_engine_full_layer_spec: false
  use_transformer_engine_op_fuser: null
  variable_seq_lengths: false
  virtual_pipeline_model_parallel_size: null
  vocab_size: null
  wgrad_deferral_limit: 0
  window_size: null
nvrx_straggler: null
optimizer:
  _target_: megatron.core.optimizer.optimizer_config.OptimizerConfig
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_eps: 1.0e-05
  barrier_with_L1_time: false
  bf16: true
  clip_grad: 1.0
  config_logger_dir: ''
  decoupled_lr: null
  decoupled_min_lr: null
  exp_avg_dtype:
    _call_: false
    _target_: torch.float32
  exp_avg_sq_dtype:
    _call_: false
    _target_: torch.float32
  fp16: false
  fp8_recipe: null
  hysteresis: 2
  initial_loss_scale: 4294967296
  log_num_zeros_in_grad: false
  loss_scale: null
  loss_scale_window: 1000
  lr: 0.0003
  main_grads_dtype:
    _call_: false
    _target_: torch.float32
  main_params_dtype:
    _call_: false
    _target_: torch.float32
  min_loss_scale: 1.0
  min_lr: 3.0e-05
  optimizer: adam
  optimizer_cpu_offload: false
  optimizer_offload_fraction: 0.0
  overlap_cpu_optimizer_d2h_h2d: false
  overlap_param_gather: false
  overlap_param_gather_with_optimizer_step: false
  params_dtype:
    _call_: false
    _target_: torch.float32
  pin_cpu_grads: true
  pin_cpu_params: true
  reuse_grad_buf_for_mxfp8_param_ag: false
  sgd_momentum: 0.9
  store_param_remainders: true
  timers: null
  use_distributed_optimizer: true
  use_megatron_fsdp: false
  use_precision_aware_optimizer: false
  use_torch_optimizer_for_cpu_offload: false
  weight_decay: 0.1
peft: null
profiling: null
rerun_state_machine:
  _target_: megatron.bridge.training.config.RerunStateMachineConfig
  error_injection_rate: 0
  error_injection_type: transient_error
  rerun_mode: disabled
rng:
  _target_: megatron.bridge.training.config.RNGConfig
  data_parallel_random_init: false
  inference_rng_tracker: false
  seed: 1234
  te_rng_tracker: false
scheduler:
  _target_: megatron.bridge.training.config.SchedulerConfig
  end_weight_decay: 0.033
  lr_decay_iters: 1168251
  lr_decay_steps: null
  lr_decay_style: cosine
  lr_warmup_fraction: null
  lr_warmup_init: 0.0
  lr_warmup_iters: 2000
  lr_warmup_steps: null
  lr_wsd_decay_iters: null
  lr_wsd_decay_style: exponential
  override_opt_param_scheduler: true
  start_weight_decay: 0.033
  use_checkpoint_opt_param_scheduler: false
  wd_incr_steps: null
  weight_decay_incr_style: constant
  wsd_decay_steps: null
straggler: null
tokenizer:
  _target_: megatron.bridge.training.tokenizers.config.TokenizerConfig
  image_tag_type: null
  merge_file: null
  special_tokens: null
  tiktoken_num_special_tokens: 1000
  tiktoken_pattern: null
  tiktoken_special_tokens: null
  tokenizer_model: null
  tokenizer_prompt_format: null
  tokenizer_type: NullTokenizer
  vocab_extra_ids: 0
  vocab_file: null
  vocab_size: 32000
train:
  _target_: megatron.bridge.training.config.TrainingConfig
  check_weight_hash_across_dp_replicas_interval: null
  decrease_batch_size_if_needed: false
  empty_unused_memory_level: 0
  eval_interval: 2000
  eval_iters: 32
  exit_duration_in_mins: null
  exit_interval: null
  exit_signal:
    _args_:
    - 15
    _call_: true
    _target_: signal.Signals
  exit_signal_handler: false
  exit_signal_handler_for_dataloader: false
  global_batch_size: 512
  manual_gc: true
  manual_gc_eval: 100
  manual_gc_interval: 100
  micro_batch_size: 1
  rampup_batch_size: null
  skip_train: false
  train_iters: 1168251
  train_sync_interval: null

_target_: megatron.bridge.training.config.ConfigContainer
checkpoint:
  _target_: megatron.bridge.training.config.CheckpointConfig
  async_save: false
  ckpt_assume_constant_structure: false
  ckpt_convert_format: null
  ckpt_convert_save: null
  ckpt_format: torch_dist
  ckpt_step: null
  dist_ckpt_strictness: assume_ok_unexpected
  exit_on_missing_checkpoint: false
  finetune: false
  fully_parallel_load: false
  fully_parallel_save: true
  load: null
  load_main_params_from_ckpt: false
  load_optim: true
  load_rng: true
  non_persistent_ckpt_type: null
  non_persistent_global_ckpt_dir: null
  non_persistent_local_ckpt_algo: fully_parallel
  non_persistent_local_ckpt_dir: null
  non_persistent_save_interval: null
  pretrained_checkpoint: null
  replication: false
  replication_factor: 2
  replication_jump: null
  save: null
  save_interval: 2000
  save_optim: true
  save_rng: true
  use_checkpoint_args: false
  use_persistent_ckpt_worker: true
comm_overlap:
  _target_: megatron.bridge.training.comm_overlap.CommOverlapConfig
  align_param_gather: null
  batch_p2p_comm: null
  bucket_size: null
  data_parallel_size: null
  defer_embedding_wgrad_compute: null
  overlap_grad_reduce: null
  overlap_p2p_comm: null
  overlap_param_gather: null
  overlap_param_gather_with_optimizer_step: null
  tp_comm_bootstrap_backend: null
  tp_comm_overlap: false
  tp_comm_overlap_cfg: null
  wgrad_deferral_limit: null
dataset:
  _target_: megatron.bridge.training.config.GPTDatasetConfig
  add_extra_token_to_sequence: true
  blend: null
  blend_per_split: null
  create_attention_mask: true
  data_sharding: true
  dataloader_type: single
  drop_last_partial_validation_sequence: true
  eod_mask_loss: false
  full_validation: null
  mid_level_dataset_surplus: 0.005
  mmap_bin_files: true
  mock: true
  multiple_validation_sets: null
  num_dataset_builder_threads: 1
  num_workers: 8
  object_storage_cache_path: null
  path_to_cache: null
  persistent_workers: false
  pin_memory: true
  random_seed: 1234
  reset_attention_mask: false
  reset_position_ids: false
  sequence_length: 4096
  split: 1,1,1
  split_matrix:
  - - 0
    - 0.3333333333333333
  - - 0.3333333333333333
    - 0.6666666666666666
  - - 0.6666666666666666
    - 1.0
  tokenizer: null
ddp:
  _target_: megatron.core.distributed.distributed_data_parallel_config.DistributedDataParallelConfig
  align_param_gather: false
  average_in_collective: true
  bucket_size: null
  check_for_large_grads: false
  check_for_nan_in_grad: true
  data_parallel_sharding_strategy: no_shard
  fp8_param_gather: false
  fsdp_double_buffer: false
  grad_reduce_in_fp32: true
  gradient_reduce_div_fusion: true
  keep_fp8_transpose_cache: false
  nccl_ub: false
  num_distributed_optimizer_instances: 1
  outer_dp_sharding_strategy: no_shard
  overlap_grad_reduce: true
  overlap_param_gather: true
  pad_buckets_for_high_nccl_busbw: false
  preserve_fp32_weights: true
  reuse_grad_buf_for_mxfp8_param_ag: false
  suggested_communication_unit_size: null
  use_custom_fsdp: false
  use_distributed_optimizer: true
  use_megatron_fsdp: false
dist:
  _target_: megatron.bridge.training.config.DistributedInitConfig
  align_grad_reduce: true
  distributed_backend: nccl
  distributed_timeout_minutes: 10
  enable_megatron_core_experimental: false
  external_gpu_device_mapping: false
  high_priority_stream_groups: null
  lazy_init: false
  local_rank: 2
  nccl_communicator_config_path: null
  sharp_enabled_group: null
  use_gloo_process_groups: true
  use_sharp: false
  use_torch_fsdp2: false
  use_tp_pp_dp_mapping: false
ft: null
logger:
  _target_: megatron.bridge.training.config.LoggerConfig
  filter_warnings: true
  log_energy: false
  log_interval: 1
  log_loss_scale_to_tensorboard: true
  log_memory_to_tensorboard: false
  log_params_norm: false
  log_progress: false
  log_throughput: false
  log_timers_to_tensorboard: false
  log_validation_ppl_to_tensorboard: false
  log_world_size_to_tensorboard: false
  logging_level: 20
  modules_to_filter: null
  set_level_for_all_loggers: false
  tensorboard_dir: /opt/Megatron-Bridge/examples/recipes/llama/nemo_experiments/default/tb_logs
  tensorboard_log_interval: 1
  tensorboard_queue_size: 1000
  timing_log_level: 0
  timing_log_option: minmax
  wandb_entity: nvidia
  wandb_exp_name: llama3-8b-pretrain
  wandb_project: megatron-bridge
  wandb_save_dir: /lustre/fsw/portfolios/coreai/users/boxiangw/wandb/megatron-bridge/llama3-8b-pretrain
mixed_precision: bf16_mixed
model:
  _target_: megatron.bridge.models.llama.llama_provider.Llama3ModelProvider8B
  account_for_embedding_in_pipeline_split: false
  account_for_loss_in_pipeline_split: false
  activation_func:
    _call_: false
    _target_: torch.nn.functional.silu
  activation_func_fp8_input_store: false
  add_bias_linear: false
  add_qkv_bias: false
  apply_query_key_layer_scaling: false
  apply_residual_connection_post_layernorm: false
  apply_rope_fusion: true
  async_tensor_model_parallel_allreduce: false
  attention_backend:
    _args_:
    - 5
    _call_: true
    _target_: megatron.core.transformer.enums.AttnBackend
  attention_dropout: 0.0
  attention_softmax_in_fp32: false
  autocast_dtype:
    _call_: false
    _target_: torch.float32
  barrier_with_L1_time: true
  batch_p2p_comm: true
  batch_p2p_sync: true
  bf16: false
  bias_activation_fusion: true
  bias_dropout_fusion: true
  calculate_per_token_loss: false
  clone_scatter_output_in_embedding: true
  config_logger_dir: ''
  context_parallel_size: 2
  cp_comm_type: null
  cpu_offloading: false
  cpu_offloading_activations: true
  cpu_offloading_num_layers: 0
  cpu_offloading_weights: true
  cross_entropy_fusion_impl: native
  cross_entropy_loss_fusion: true
  cuda_graph_retain_backward_graph: false
  cuda_graph_scope: full
  cuda_graph_use_single_mempool: false
  cuda_graph_warmup_steps: 3
  deallocate_pipeline_outputs: true
  defer_embedding_wgrad_compute: false
  delay_wgrad_compute: false
  deterministic_mode: false
  disable_bf16_reduced_precision_matmul: false
  disable_parameter_transpose_cache: false
  distribute_saved_activations: null
  embedding_init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.01
  embedding_init_method_std: 0.01
  enable_autocast: false
  enable_cuda_graph: false
  expert_model_parallel_size: 1
  expert_tensor_parallel_size: 1
  external_cuda_graph: false
  ffn_hidden_size: 14336
  finalize_model_grads_func: null
  first_last_layers_bf16: false
  flash_decode: false
  fp16: false
  fp16_lm_cross_entropy: false
  fp32_residual_connection: false
  fp8: null
  fp8_amax_compute_algo: most_recent
  fp8_amax_history_len: 1
  fp8_dot_product_attention: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_multi_head_attention: false
  fp8_param: false
  fp8_recipe: delayed
  fp8_wgrad: true
  gated_linear_unit: true
  generation_config: null
  grad_scale_func: null
  grad_sync_func: null
  gradient_accumulation_fusion: true
  hetereogenous_dist_checkpoint: false
  heterogeneous_block_specs: false
  hidden_dropout: 0.0
  hidden_size: 4096
  hierarchical_context_parallel_sizes: null
  inference_rng_tracker: false
  init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.01
  init_method_std: 0.01
  init_model_with_meta_device: false
  is_hybrid_model: false
  kv_channels: 128
  layernorm_epsilon: 1.0e-05
  layernorm_zero_centered_gamma: false
  make_vocab_size_divisible_by: 128
  mamba_head_dim: 64
  mamba_num_groups: 8
  mamba_num_heads: null
  mamba_state_dim: 128
  masked_softmax_fusion: true
  memory_efficient_layer_norm: false
  microbatch_group_size_per_vp_stage: 1
  mlp_chunks_for_prefill: 1
  moe_apply_probs_on_input: false
  moe_aux_loss_coeff: 0.0
  moe_deepep_num_sms: 20
  moe_enable_deepep: false
  moe_expert_capacity_factor: null
  moe_extended_tp: false
  moe_ffn_hidden_size: null
  moe_grouped_gemm: false
  moe_input_jitter_eps: null
  moe_layer_freq: 1
  moe_layer_recompute: false
  moe_pad_expert_input_to_capacity: false
  moe_per_layer_logging: false
  moe_permute_fusion: false
  moe_router_bias_update_rate: 0.001
  moe_router_dtype: null
  moe_router_enable_expert_bias: false
  moe_router_force_load_balancing: false
  moe_router_fusion: false
  moe_router_group_topk: null
  moe_router_load_balancing_type: aux_loss
  moe_router_num_groups: null
  moe_router_padding_for_fp8: false
  moe_router_pre_softmax: false
  moe_router_score_function: softmax
  moe_router_topk: 2
  moe_router_topk_limited_devices: null
  moe_router_topk_scaling_factor: null
  moe_shared_expert_intermediate_size: null
  moe_shared_expert_overlap: false
  moe_token_dispatcher_type: allgather
  moe_token_drop_policy: probs
  moe_token_dropping: false
  moe_use_legacy_grouped_gemm: false
  moe_z_loss_coeff: null
  mrope_section: null
  mtp_enabled: false
  mtp_loss_scaling_factor: null
  mtp_num_layers: null
  multi_latent_attention: false
  no_rope_freq: null
  no_sync_func: null
  normalization: RMSNorm
  num_attention_heads: 32
  num_layers: 32
  num_layers_at_end_in_bf16: 1
  num_layers_at_start_in_bf16: 1
  num_layers_in_first_pipeline_stage: null
  num_layers_in_last_pipeline_stage: null
  num_microbatches_with_partial_activation_checkpoints: null
  num_moe_experts: null
  num_query_groups: 8
  output_layer_init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.00125
  overlap_moe_expert_parallel_comm: false
  overlap_p2p_comm: false
  overlap_p2p_comm_warmup_flush: false
  parallel_output: true
  param_sync_func: null
  params_dtype:
    _call_: false
    _target_: torch.float32
  perform_initialization: true
  persist_layer_norm: false
  pipeline_dtype: null
  pipeline_model_parallel_comm_backend: null
  pipeline_model_parallel_layout: null
  pipeline_model_parallel_size: 1
  position_embedding_type: rope
  qk_layernorm: false
  quant_recipe: null
  recompute_granularity: null
  recompute_method: null
  recompute_modules:
  - core_attn
  recompute_num_layers: null
  rotary_base: 500000
  rotary_interleaved: false
  rotary_percent: 1.0
  scatter_embedding_sequence_parallel: true
  seq_len_interpolation_factor: null
  seq_length: 4096
  sequence_parallel: false
  share_embeddings_and_output_weights: false
  should_pad_vocab: false
  softmax_scale: null
  symmetric_ar_type: null
  tensor_model_parallel_size: 1
  test_mode: false
  timers: null
  tp_comm_atomic_ag: false
  tp_comm_atomic_rs: false
  tp_comm_bootstrap_backend: nccl
  tp_comm_bulk_dgrad: true
  tp_comm_bulk_wgrad: true
  tp_comm_overlap: false
  tp_comm_overlap_ag: true
  tp_comm_overlap_cfg: null
  tp_comm_overlap_disable_fc1: false
  tp_comm_overlap_disable_qkv: false
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_split_ag: true
  tp_comm_split_rs: true
  tp_only_amax_red: false
  transformer_impl: transformer_engine
  transformer_layer_spec:
    _call_: false
    _target_: megatron.bridge.models.gpt_provider.default_layer_spec
  use_cpu_initialization: false
  use_fused_weighted_squared_relu: false
  use_kitchen: false
  use_mamba_mem_eff_path: true
  use_ring_exchange_p2p: false
  use_te_activation_func: false
  use_te_rng_tracker: false
  use_transformer_engine_full_layer_spec: false
  use_transformer_engine_op_fuser: null
  variable_seq_lengths: false
  virtual_pipeline_model_parallel_size: null
  vocab_size: null
  wgrad_deferral_limit: 0
  window_size: null
nvrx_straggler: null
optimizer:
  _target_: megatron.core.optimizer.optimizer_config.OptimizerConfig
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_eps: 1.0e-05
  barrier_with_L1_time: false
  bf16: true
  clip_grad: 1.0
  config_logger_dir: ''
  decoupled_lr: null
  decoupled_min_lr: null
  exp_avg_dtype:
    _call_: false
    _target_: torch.float32
  exp_avg_sq_dtype:
    _call_: false
    _target_: torch.float32
  fp16: false
  fp8_recipe: null
  hysteresis: 2
  initial_loss_scale: 4294967296
  log_num_zeros_in_grad: false
  loss_scale: null
  loss_scale_window: 1000
  lr: 0.00025
  main_grads_dtype:
    _call_: false
    _target_: torch.float32
  main_params_dtype:
    _call_: false
    _target_: torch.float32
  min_loss_scale: 1.0
  min_lr: 2.5e-05
  optimizer: adam
  optimizer_cpu_offload: false
  optimizer_offload_fraction: 0.0
  overlap_cpu_optimizer_d2h_h2d: false
  overlap_param_gather: false
  overlap_param_gather_with_optimizer_step: false
  params_dtype:
    _call_: false
    _target_: torch.float32
  pin_cpu_grads: true
  pin_cpu_params: true
  reuse_grad_buf_for_mxfp8_param_ag: false
  sgd_momentum: 0.9
  store_param_remainders: true
  timers: null
  use_distributed_optimizer: true
  use_megatron_fsdp: false
  use_precision_aware_optimizer: false
  use_torch_optimizer_for_cpu_offload: false
  weight_decay: 0.1
peft: null
profiling:
  _target_: megatron.bridge.training.config.ProfilingConfig
  memory_snapshot_path: snapshot.pickle
  profile_ranks:
  - 0
  - 1
  profile_step_end: 10
  profile_step_start: 5
  record_memory_history: false
  record_shapes: true
  use_nsys_profiler: false
  use_pytorch_profiler: true
rerun_state_machine:
  _target_: megatron.bridge.training.config.RerunStateMachineConfig
  error_injection_rate: 0
  error_injection_type: transient_error
  rerun_mode: disabled
rng:
  _target_: megatron.bridge.training.config.RNGConfig
  data_parallel_random_init: false
  inference_rng_tracker: false
  seed: 42
  te_rng_tracker: false
scheduler:
  _target_: megatron.bridge.training.config.SchedulerConfig
  end_weight_decay: 0.033
  lr_decay_iters: 1168251
  lr_decay_steps: null
  lr_decay_style: cosine
  lr_warmup_fraction: null
  lr_warmup_init: 0.0
  lr_warmup_iters: 10
  lr_warmup_steps: null
  lr_wsd_decay_iters: null
  lr_wsd_decay_style: exponential
  override_opt_param_scheduler: true
  start_weight_decay: 0.033
  use_checkpoint_opt_param_scheduler: false
  wd_incr_steps: null
  weight_decay_incr_style: constant
  wsd_decay_steps: null
straggler: null
tokenizer:
  _target_: megatron.bridge.training.tokenizers.config.TokenizerConfig
  image_tag_type: null
  merge_file: null
  special_tokens: null
  tiktoken_num_special_tokens: 1000
  tiktoken_pattern: null
  tiktoken_special_tokens: null
  tokenizer_model: null
  tokenizer_prompt_format: null
  tokenizer_type: NullTokenizer
  vocab_extra_ids: 0
  vocab_file: null
  vocab_size: 32000
train:
  _target_: megatron.bridge.training.config.TrainingConfig
  check_weight_hash_across_dp_replicas_interval: null
  decrease_batch_size_if_needed: false
  empty_unused_memory_level: 0
  eval_interval: 2000
  eval_iters: 0
  exit_duration_in_mins: null
  exit_interval: null
  exit_signal:
    _args_:
    - 15
    _call_: true
    _target_: signal.Signals
  exit_signal_handler: false
  exit_signal_handler_for_dataloader: false
  global_batch_size: 8
  manual_gc: true
  manual_gc_eval: 100
  manual_gc_interval: 100
  micro_batch_size: 1
  rampup_batch_size: null
  skip_train: false
  train_iters: 20
  train_sync_interval: null

_target_: megatron.bridge.training.config.ConfigContainer
checkpoint:
  _target_: megatron.bridge.training.config.CheckpointConfig
  async_save: false
  ckpt_assume_constant_structure: false
  ckpt_convert_format: null
  ckpt_convert_save: null
  ckpt_format: torch_dist
  ckpt_step: null
  dist_ckpt_strictness: assume_ok_unexpected
  exit_on_missing_checkpoint: false
  finetune: false
  fully_parallel_load: false
  fully_parallel_save: true
  load: null
  load_main_params_from_ckpt: false
  load_optim: true
  load_rng: true
  non_persistent_ckpt_type: null
  non_persistent_global_ckpt_dir: null
  non_persistent_local_ckpt_algo: fully_parallel
  non_persistent_local_ckpt_dir: null
  non_persistent_save_interval: null
  pretrained_checkpoint: null
  replication: false
  replication_factor: 2
  replication_jump: null
  save: /opt/Megatron-Bridge/examples/recipes/llama/nemo_experiments/default/checkpoints
  save_interval: 2000
  save_optim: true
  save_rng: true
  use_checkpoint_args: false
  use_persistent_ckpt_worker: true
comm_overlap:
  _target_: megatron.bridge.training.comm_overlap.CommOverlapConfig
  align_param_gather: null
  batch_p2p_comm: null
  bucket_size: null
  data_parallel_size: null
  defer_embedding_wgrad_compute: null
  overlap_grad_reduce: null
  overlap_p2p_comm: null
  overlap_param_gather: null
  overlap_param_gather_with_optimizer_step: null
  tp_comm_bootstrap_backend: null
  tp_comm_overlap: false
  tp_comm_overlap_cfg: null
  wgrad_deferral_limit: null
dataset:
  _target_: megatron.bridge.training.config.GPTDatasetConfig
  add_extra_token_to_sequence: true
  blend: null
  blend_per_split: null
  create_attention_mask: true
  data_sharding: true
  dataloader_type: single
  drop_last_partial_validation_sequence: true
  eod_mask_loss: false
  full_validation: null
  mid_level_dataset_surplus: 0.005
  mmap_bin_files: true
  mock: true
  multiple_validation_sets: null
  num_dataset_builder_threads: 1
  num_workers: 8
  object_storage_cache_path: null
  path_to_cache: null
  persistent_workers: false
  pin_memory: true
  random_seed: 1234
  reset_attention_mask: false
  reset_position_ids: false
  sequence_length: 8192
  split: 1,1,1
  split_matrix:
  - - 0
    - 0.3333333333333333
  - - 0.3333333333333333
    - 0.6666666666666666
  - - 0.6666666666666666
    - 1.0
  tokenizer: null
ddp:
  _target_: megatron.core.distributed.distributed_data_parallel_config.DistributedDataParallelConfig
  align_param_gather: false
  average_in_collective: true
  bucket_size: null
  check_for_large_grads: false
  check_for_nan_in_grad: true
  data_parallel_sharding_strategy: no_shard
  fp8_param_gather: false
  fsdp_double_buffer: false
  grad_reduce_in_fp32: true
  gradient_reduce_div_fusion: true
  keep_fp8_transpose_cache: false
  nccl_ub: false
  num_distributed_optimizer_instances: 1
  outer_dp_sharding_strategy: no_shard
  overlap_grad_reduce: true
  overlap_param_gather: true
  pad_buckets_for_high_nccl_busbw: false
  preserve_fp32_weights: true
  reuse_grad_buf_for_mxfp8_param_ag: false
  suggested_communication_unit_size: null
  use_custom_fsdp: false
  use_distributed_optimizer: true
  use_megatron_fsdp: false
dist:
  _target_: megatron.bridge.training.config.DistributedInitConfig
  align_grad_reduce: true
  distributed_backend: nccl
  distributed_timeout_minutes: 10
  enable_megatron_core_experimental: false
  external_gpu_device_mapping: false
  high_priority_stream_groups: null
  lazy_init: false
  local_rank: 5
  nccl_communicator_config_path: null
  sharp_enabled_group: null
  use_gloo_process_groups: true
  use_sharp: false
  use_torch_fsdp2: false
  use_tp_pp_dp_mapping: false
ft: null
logger:
  _target_: megatron.bridge.training.config.LoggerConfig
  filter_warnings: true
  log_energy: false
  log_interval: 10
  log_loss_scale_to_tensorboard: true
  log_memory_to_tensorboard: false
  log_params_norm: false
  log_progress: false
  log_throughput: false
  log_timers_to_tensorboard: false
  log_validation_ppl_to_tensorboard: false
  log_world_size_to_tensorboard: false
  logging_level: 20
  modules_to_filter: null
  set_level_for_all_loggers: false
  tensorboard_dir: /opt/Megatron-Bridge/examples/recipes/llama/nemo_experiments/default/tb_logs
  tensorboard_log_interval: 1
  tensorboard_queue_size: 1000
  timing_log_level: 0
  timing_log_option: minmax
  wandb_entity: null
  wandb_exp_name: null
  wandb_project: null
  wandb_save_dir: null
mixed_precision: bf16_mixed
model:
  _target_: megatron.bridge.models.llama.llama_provider.Llama3ModelProvider8B
  account_for_embedding_in_pipeline_split: false
  account_for_loss_in_pipeline_split: false
  activation_func:
    _call_: false
    _target_: torch.nn.functional.silu
  activation_func_fp8_input_store: false
  add_bias_linear: false
  add_qkv_bias: false
  apply_query_key_layer_scaling: false
  apply_residual_connection_post_layernorm: false
  apply_rope_fusion: true
  async_tensor_model_parallel_allreduce: false
  attention_backend:
    _args_:
    - 5
    _call_: true
    _target_: megatron.core.transformer.enums.AttnBackend
  attention_dropout: 0.0
  attention_softmax_in_fp32: false
  autocast_dtype:
    _call_: false
    _target_: torch.float32
  barrier_with_L1_time: true
  batch_p2p_comm: true
  batch_p2p_sync: true
  bf16: false
  bias_activation_fusion: true
  bias_dropout_fusion: true
  calculate_per_token_loss: false
  clone_scatter_output_in_embedding: true
  config_logger_dir: ''
  context_parallel_size: 2
  cp_comm_type: null
  cpu_offloading: false
  cpu_offloading_activations: true
  cpu_offloading_num_layers: 0
  cpu_offloading_weights: true
  cross_entropy_fusion_impl: native
  cross_entropy_loss_fusion: true
  cuda_graph_retain_backward_graph: false
  cuda_graph_scope: full
  cuda_graph_use_single_mempool: false
  cuda_graph_warmup_steps: 3
  deallocate_pipeline_outputs: true
  defer_embedding_wgrad_compute: false
  delay_wgrad_compute: false
  deterministic_mode: false
  disable_bf16_reduced_precision_matmul: false
  disable_parameter_transpose_cache: false
  distribute_saved_activations: null
  embedding_init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.01
  embedding_init_method_std: 0.01
  enable_autocast: false
  enable_cuda_graph: false
  expert_model_parallel_size: 1
  expert_tensor_parallel_size: 1
  external_cuda_graph: false
  ffn_hidden_size: 14336
  finalize_model_grads_func: null
  first_last_layers_bf16: false
  flash_decode: false
  fp16: false
  fp16_lm_cross_entropy: false
  fp32_residual_connection: false
  fp8: null
  fp8_amax_compute_algo: most_recent
  fp8_amax_history_len: 1
  fp8_dot_product_attention: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_multi_head_attention: false
  fp8_param: false
  fp8_recipe: delayed
  fp8_wgrad: true
  gated_linear_unit: true
  generation_config: null
  grad_scale_func: null
  grad_sync_func: null
  gradient_accumulation_fusion: true
  hetereogenous_dist_checkpoint: false
  heterogeneous_block_specs: false
  hidden_dropout: 0.0
  hidden_size: 4096
  hierarchical_context_parallel_sizes: null
  inference_rng_tracker: false
  init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.01
  init_method_std: 0.01
  init_model_with_meta_device: false
  is_hybrid_model: false
  kv_channels: 128
  layernorm_epsilon: 1.0e-05
  layernorm_zero_centered_gamma: false
  make_vocab_size_divisible_by: 128
  mamba_head_dim: 64
  mamba_num_groups: 8
  mamba_num_heads: null
  mamba_state_dim: 128
  masked_softmax_fusion: true
  memory_efficient_layer_norm: false
  microbatch_group_size_per_vp_stage: 1
  mlp_chunks_for_prefill: 1
  moe_apply_probs_on_input: false
  moe_aux_loss_coeff: 0.0
  moe_deepep_num_sms: 20
  moe_enable_deepep: false
  moe_expert_capacity_factor: null
  moe_extended_tp: false
  moe_ffn_hidden_size: null
  moe_grouped_gemm: false
  moe_input_jitter_eps: null
  moe_layer_freq: 1
  moe_layer_recompute: false
  moe_pad_expert_input_to_capacity: false
  moe_per_layer_logging: false
  moe_permute_fusion: false
  moe_router_bias_update_rate: 0.001
  moe_router_dtype: null
  moe_router_enable_expert_bias: false
  moe_router_force_load_balancing: false
  moe_router_fusion: false
  moe_router_group_topk: null
  moe_router_load_balancing_type: aux_loss
  moe_router_num_groups: null
  moe_router_padding_for_fp8: false
  moe_router_pre_softmax: false
  moe_router_score_function: softmax
  moe_router_topk: 2
  moe_router_topk_limited_devices: null
  moe_router_topk_scaling_factor: null
  moe_shared_expert_intermediate_size: null
  moe_shared_expert_overlap: false
  moe_token_dispatcher_type: allgather
  moe_token_drop_policy: probs
  moe_token_dropping: false
  moe_use_legacy_grouped_gemm: false
  moe_z_loss_coeff: null
  mrope_section: null
  mtp_enabled: false
  mtp_loss_scaling_factor: null
  mtp_num_layers: null
  multi_latent_attention: false
  no_rope_freq: null
  no_sync_func: null
  normalization: RMSNorm
  num_attention_heads: 32
  num_layers: 32
  num_layers_at_end_in_bf16: 1
  num_layers_at_start_in_bf16: 1
  num_layers_in_first_pipeline_stage: null
  num_layers_in_last_pipeline_stage: null
  num_microbatches_with_partial_activation_checkpoints: null
  num_moe_experts: null
  num_query_groups: 8
  output_layer_init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.00125
  overlap_moe_expert_parallel_comm: false
  overlap_p2p_comm: false
  overlap_p2p_comm_warmup_flush: false
  parallel_output: true
  param_sync_func: null
  params_dtype:
    _call_: false
    _target_: torch.float32
  perform_initialization: true
  persist_layer_norm: false
  pipeline_dtype: null
  pipeline_model_parallel_comm_backend: null
  pipeline_model_parallel_layout: null
  pipeline_model_parallel_size: 1
  position_embedding_type: rope
  qk_layernorm: false
  quant_recipe: null
  recompute_granularity: null
  recompute_method: null
  recompute_modules:
  - core_attn
  recompute_num_layers: null
  rotary_base: 500000
  rotary_interleaved: false
  rotary_percent: 1.0
  scatter_embedding_sequence_parallel: true
  seq_len_interpolation_factor: null
  seq_length: 8192
  sequence_parallel: false
  share_embeddings_and_output_weights: false
  should_pad_vocab: false
  softmax_scale: null
  symmetric_ar_type: null
  tensor_model_parallel_size: 1
  test_mode: false
  timers: null
  tp_comm_atomic_ag: false
  tp_comm_atomic_rs: false
  tp_comm_bootstrap_backend: nccl
  tp_comm_bulk_dgrad: true
  tp_comm_bulk_wgrad: true
  tp_comm_overlap: false
  tp_comm_overlap_ag: true
  tp_comm_overlap_cfg: null
  tp_comm_overlap_disable_fc1: false
  tp_comm_overlap_disable_qkv: false
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_split_ag: true
  tp_comm_split_rs: true
  tp_only_amax_red: false
  transformer_impl: transformer_engine
  transformer_layer_spec:
    _call_: false
    _target_: megatron.bridge.models.gpt_provider.default_layer_spec
  use_cpu_initialization: false
  use_fused_weighted_squared_relu: false
  use_kitchen: false
  use_mamba_mem_eff_path: true
  use_ring_exchange_p2p: false
  use_te_activation_func: false
  use_te_rng_tracker: false
  use_transformer_engine_full_layer_spec: false
  use_transformer_engine_op_fuser: null
  variable_seq_lengths: false
  virtual_pipeline_model_parallel_size: null
  vocab_size: null
  wgrad_deferral_limit: 0
  window_size: null
nvrx_straggler: null
optimizer:
  _target_: megatron.core.optimizer.optimizer_config.OptimizerConfig
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_eps: 1.0e-05
  barrier_with_L1_time: false
  bf16: true
  clip_grad: 1.0
  config_logger_dir: ''
  decoupled_lr: null
  decoupled_min_lr: null
  exp_avg_dtype:
    _call_: false
    _target_: torch.float32
  exp_avg_sq_dtype:
    _call_: false
    _target_: torch.float32
  fp16: false
  fp8_recipe: null
  hysteresis: 2
  initial_loss_scale: 4294967296
  log_num_zeros_in_grad: false
  loss_scale: null
  loss_scale_window: 1000
  lr: 0.0003
  main_grads_dtype:
    _call_: false
    _target_: torch.float32
  main_params_dtype:
    _call_: false
    _target_: torch.float32
  min_loss_scale: 1.0
  min_lr: 3.0e-05
  optimizer: adam
  optimizer_cpu_offload: false
  optimizer_offload_fraction: 0.0
  overlap_cpu_optimizer_d2h_h2d: false
  overlap_param_gather: false
  overlap_param_gather_with_optimizer_step: false
  params_dtype:
    _call_: false
    _target_: torch.float32
  pin_cpu_grads: true
  pin_cpu_params: true
  reuse_grad_buf_for_mxfp8_param_ag: false
  sgd_momentum: 0.9
  store_param_remainders: true
  timers: null
  use_distributed_optimizer: true
  use_megatron_fsdp: false
  use_precision_aware_optimizer: false
  use_torch_optimizer_for_cpu_offload: false
  weight_decay: 0.1
peft: null
profiling: null
rerun_state_machine:
  _target_: megatron.bridge.training.config.RerunStateMachineConfig
  error_injection_rate: 0
  error_injection_type: transient_error
  rerun_mode: disabled
rng:
  _target_: megatron.bridge.training.config.RNGConfig
  data_parallel_random_init: false
  inference_rng_tracker: false
  seed: 1234
  te_rng_tracker: false
scheduler:
  _target_: megatron.bridge.training.config.SchedulerConfig
  end_weight_decay: 0.033
  lr_decay_iters: 1168251
  lr_decay_steps: null
  lr_decay_style: cosine
  lr_warmup_fraction: null
  lr_warmup_init: 0.0
  lr_warmup_iters: 2000
  lr_warmup_steps: null
  lr_wsd_decay_iters: null
  lr_wsd_decay_style: exponential
  override_opt_param_scheduler: true
  start_weight_decay: 0.033
  use_checkpoint_opt_param_scheduler: false
  wd_incr_steps: null
  weight_decay_incr_style: constant
  wsd_decay_steps: null
straggler: null
tokenizer:
  _target_: megatron.bridge.training.tokenizers.config.TokenizerConfig
  image_tag_type: null
  merge_file: null
  special_tokens: null
  tiktoken_num_special_tokens: 1000
  tiktoken_pattern: null
  tiktoken_special_tokens: null
  tokenizer_model: null
  tokenizer_prompt_format: null
  tokenizer_type: NullTokenizer
  vocab_extra_ids: 0
  vocab_file: null
  vocab_size: 32000
train:
  _target_: megatron.bridge.training.config.TrainingConfig
  check_weight_hash_across_dp_replicas_interval: null
  decrease_batch_size_if_needed: false
  empty_unused_memory_level: 0
  eval_interval: 2000
  eval_iters: 32
  exit_duration_in_mins: null
  exit_interval: null
  exit_signal:
    _args_:
    - 15
    _call_: true
    _target_: signal.Signals
  exit_signal_handler: false
  exit_signal_handler_for_dataloader: false
  global_batch_size: 512
  manual_gc: true
  manual_gc_eval: 100
  manual_gc_interval: 100
  micro_batch_size: 1
  rampup_batch_size: null
  skip_train: false
  train_iters: 1168251
  train_sync_interval: null

_target_: megatron.bridge.training.config.ConfigContainer
checkpoint:
  _target_: megatron.bridge.training.config.CheckpointConfig
  async_save: false
  ckpt_assume_constant_structure: false
  ckpt_convert_format: null
  ckpt_convert_save: null
  ckpt_format: torch_dist
  ckpt_step: null
  dist_ckpt_strictness: assume_ok_unexpected
  exit_on_missing_checkpoint: false
  finetune: false
  fully_parallel_load: false
  fully_parallel_save: true
  load: null
  load_main_params_from_ckpt: false
  load_optim: true
  load_rng: true
  non_persistent_ckpt_type: null
  non_persistent_global_ckpt_dir: null
  non_persistent_local_ckpt_algo: fully_parallel
  non_persistent_local_ckpt_dir: null
  non_persistent_save_interval: null
  pretrained_checkpoint: null
  replication: false
  replication_factor: 2
  replication_jump: null
  save: null
  save_interval: 2000
  save_optim: true
  save_rng: true
  use_checkpoint_args: false
  use_persistent_ckpt_worker: true
comm_overlap:
  _target_: megatron.bridge.training.comm_overlap.CommOverlapConfig
  align_param_gather: null
  batch_p2p_comm: null
  bucket_size: null
  data_parallel_size: null
  defer_embedding_wgrad_compute: null
  overlap_grad_reduce: null
  overlap_p2p_comm: null
  overlap_param_gather: null
  overlap_param_gather_with_optimizer_step: null
  tp_comm_bootstrap_backend: null
  tp_comm_overlap: false
  tp_comm_overlap_cfg: null
  wgrad_deferral_limit: null
dataset:
  _target_: megatron.bridge.training.config.GPTDatasetConfig
  add_extra_token_to_sequence: true
  blend: null
  blend_per_split: null
  create_attention_mask: true
  data_sharding: true
  dataloader_type: single
  drop_last_partial_validation_sequence: true
  eod_mask_loss: false
  full_validation: null
  mid_level_dataset_surplus: 0.005
  mmap_bin_files: true
  mock: true
  multiple_validation_sets: null
  num_dataset_builder_threads: 1
  num_workers: 8
  object_storage_cache_path: null
  path_to_cache: null
  persistent_workers: false
  pin_memory: true
  random_seed: 1234
  reset_attention_mask: false
  reset_position_ids: false
  sequence_length: 4096
  split: 1,1,1
  split_matrix:
  - - 0
    - 0.3333333333333333
  - - 0.3333333333333333
    - 0.6666666666666666
  - - 0.6666666666666666
    - 1.0
  tokenizer: null
ddp:
  _target_: megatron.core.distributed.distributed_data_parallel_config.DistributedDataParallelConfig
  align_param_gather: false
  average_in_collective: true
  bucket_size: null
  check_for_large_grads: false
  check_for_nan_in_grad: true
  data_parallel_sharding_strategy: no_shard
  fp8_param_gather: false
  fsdp_double_buffer: false
  grad_reduce_in_fp32: true
  gradient_reduce_div_fusion: true
  keep_fp8_transpose_cache: false
  nccl_ub: false
  num_distributed_optimizer_instances: 1
  outer_dp_sharding_strategy: no_shard
  overlap_grad_reduce: true
  overlap_param_gather: true
  pad_buckets_for_high_nccl_busbw: false
  preserve_fp32_weights: true
  reuse_grad_buf_for_mxfp8_param_ag: false
  suggested_communication_unit_size: null
  use_custom_fsdp: false
  use_distributed_optimizer: true
  use_megatron_fsdp: false
dist:
  _target_: megatron.bridge.training.config.DistributedInitConfig
  align_grad_reduce: true
  distributed_backend: nccl
  distributed_timeout_minutes: 10
  enable_megatron_core_experimental: false
  external_gpu_device_mapping: false
  high_priority_stream_groups: null
  lazy_init: false
  local_rank: 5
  nccl_communicator_config_path: null
  sharp_enabled_group: null
  use_gloo_process_groups: true
  use_sharp: false
  use_torch_fsdp2: false
  use_tp_pp_dp_mapping: false
ft: null
logger:
  _target_: megatron.bridge.training.config.LoggerConfig
  filter_warnings: true
  log_energy: false
  log_interval: 1
  log_loss_scale_to_tensorboard: true
  log_memory_to_tensorboard: false
  log_params_norm: false
  log_progress: false
  log_throughput: false
  log_timers_to_tensorboard: false
  log_validation_ppl_to_tensorboard: false
  log_world_size_to_tensorboard: false
  logging_level: 20
  modules_to_filter: null
  set_level_for_all_loggers: false
  tensorboard_dir: /opt/Megatron-Bridge/examples/recipes/llama/nemo_experiments/default/tb_logs
  tensorboard_log_interval: 1
  tensorboard_queue_size: 1000
  timing_log_level: 0
  timing_log_option: minmax
  wandb_entity: nvidia
  wandb_exp_name: llama3-8b-pretrain
  wandb_project: megatron-bridge
  wandb_save_dir: /lustre/fsw/portfolios/coreai/users/boxiangw/wandb/megatron-bridge/llama3-8b-pretrain
mixed_precision: bf16_mixed
model:
  _target_: megatron.bridge.models.llama.llama_provider.Llama3ModelProvider8B
  account_for_embedding_in_pipeline_split: false
  account_for_loss_in_pipeline_split: false
  activation_func:
    _call_: false
    _target_: torch.nn.functional.silu
  activation_func_fp8_input_store: false
  add_bias_linear: false
  add_qkv_bias: false
  apply_query_key_layer_scaling: false
  apply_residual_connection_post_layernorm: false
  apply_rope_fusion: true
  async_tensor_model_parallel_allreduce: false
  attention_backend:
    _args_:
    - 5
    _call_: true
    _target_: megatron.core.transformer.enums.AttnBackend
  attention_dropout: 0.0
  attention_softmax_in_fp32: false
  autocast_dtype:
    _call_: false
    _target_: torch.float32
  barrier_with_L1_time: true
  batch_p2p_comm: true
  batch_p2p_sync: true
  bf16: false
  bias_activation_fusion: true
  bias_dropout_fusion: true
  calculate_per_token_loss: false
  clone_scatter_output_in_embedding: true
  config_logger_dir: ''
  context_parallel_size: 2
  cp_comm_type: null
  cpu_offloading: false
  cpu_offloading_activations: true
  cpu_offloading_num_layers: 0
  cpu_offloading_weights: true
  cross_entropy_fusion_impl: native
  cross_entropy_loss_fusion: true
  cuda_graph_retain_backward_graph: false
  cuda_graph_scope: full
  cuda_graph_use_single_mempool: false
  cuda_graph_warmup_steps: 3
  deallocate_pipeline_outputs: true
  defer_embedding_wgrad_compute: false
  delay_wgrad_compute: false
  deterministic_mode: false
  disable_bf16_reduced_precision_matmul: false
  disable_parameter_transpose_cache: false
  distribute_saved_activations: null
  embedding_init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.01
  embedding_init_method_std: 0.01
  enable_autocast: false
  enable_cuda_graph: false
  expert_model_parallel_size: 1
  expert_tensor_parallel_size: 1
  external_cuda_graph: false
  ffn_hidden_size: 14336
  finalize_model_grads_func: null
  first_last_layers_bf16: false
  flash_decode: false
  fp16: false
  fp16_lm_cross_entropy: false
  fp32_residual_connection: false
  fp8: null
  fp8_amax_compute_algo: most_recent
  fp8_amax_history_len: 1
  fp8_dot_product_attention: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_multi_head_attention: false
  fp8_param: false
  fp8_recipe: delayed
  fp8_wgrad: true
  gated_linear_unit: true
  generation_config: null
  grad_scale_func: null
  grad_sync_func: null
  gradient_accumulation_fusion: true
  hetereogenous_dist_checkpoint: false
  heterogeneous_block_specs: false
  hidden_dropout: 0.0
  hidden_size: 4096
  hierarchical_context_parallel_sizes: null
  inference_rng_tracker: false
  init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.01
  init_method_std: 0.01
  init_model_with_meta_device: false
  is_hybrid_model: false
  kv_channels: 128
  layernorm_epsilon: 1.0e-05
  layernorm_zero_centered_gamma: false
  make_vocab_size_divisible_by: 128
  mamba_head_dim: 64
  mamba_num_groups: 8
  mamba_num_heads: null
  mamba_state_dim: 128
  masked_softmax_fusion: true
  memory_efficient_layer_norm: false
  microbatch_group_size_per_vp_stage: 1
  mlp_chunks_for_prefill: 1
  moe_apply_probs_on_input: false
  moe_aux_loss_coeff: 0.0
  moe_deepep_num_sms: 20
  moe_enable_deepep: false
  moe_expert_capacity_factor: null
  moe_extended_tp: false
  moe_ffn_hidden_size: null
  moe_grouped_gemm: false
  moe_input_jitter_eps: null
  moe_layer_freq: 1
  moe_layer_recompute: false
  moe_pad_expert_input_to_capacity: false
  moe_per_layer_logging: false
  moe_permute_fusion: false
  moe_router_bias_update_rate: 0.001
  moe_router_dtype: null
  moe_router_enable_expert_bias: false
  moe_router_force_load_balancing: false
  moe_router_fusion: false
  moe_router_group_topk: null
  moe_router_load_balancing_type: aux_loss
  moe_router_num_groups: null
  moe_router_padding_for_fp8: false
  moe_router_pre_softmax: false
  moe_router_score_function: softmax
  moe_router_topk: 2
  moe_router_topk_limited_devices: null
  moe_router_topk_scaling_factor: null
  moe_shared_expert_intermediate_size: null
  moe_shared_expert_overlap: false
  moe_token_dispatcher_type: allgather
  moe_token_drop_policy: probs
  moe_token_dropping: false
  moe_use_legacy_grouped_gemm: false
  moe_z_loss_coeff: null
  mrope_section: null
  mtp_enabled: false
  mtp_loss_scaling_factor: null
  mtp_num_layers: null
  multi_latent_attention: false
  no_rope_freq: null
  no_sync_func: null
  normalization: RMSNorm
  num_attention_heads: 32
  num_layers: 32
  num_layers_at_end_in_bf16: 1
  num_layers_at_start_in_bf16: 1
  num_layers_in_first_pipeline_stage: null
  num_layers_in_last_pipeline_stage: null
  num_microbatches_with_partial_activation_checkpoints: null
  num_moe_experts: null
  num_query_groups: 8
  output_layer_init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.00125
  overlap_moe_expert_parallel_comm: false
  overlap_p2p_comm: false
  overlap_p2p_comm_warmup_flush: false
  parallel_output: true
  param_sync_func: null
  params_dtype:
    _call_: false
    _target_: torch.float32
  perform_initialization: true
  persist_layer_norm: false
  pipeline_dtype: null
  pipeline_model_parallel_comm_backend: null
  pipeline_model_parallel_layout: null
  pipeline_model_parallel_size: 1
  position_embedding_type: rope
  qk_layernorm: false
  quant_recipe: null
  recompute_granularity: null
  recompute_method: null
  recompute_modules:
  - core_attn
  recompute_num_layers: null
  rotary_base: 500000
  rotary_interleaved: false
  rotary_percent: 1.0
  scatter_embedding_sequence_parallel: true
  seq_len_interpolation_factor: null
  seq_length: 4096
  sequence_parallel: false
  share_embeddings_and_output_weights: false
  should_pad_vocab: false
  softmax_scale: null
  symmetric_ar_type: null
  tensor_model_parallel_size: 1
  test_mode: false
  timers: null
  tp_comm_atomic_ag: false
  tp_comm_atomic_rs: false
  tp_comm_bootstrap_backend: nccl
  tp_comm_bulk_dgrad: true
  tp_comm_bulk_wgrad: true
  tp_comm_overlap: false
  tp_comm_overlap_ag: true
  tp_comm_overlap_cfg: null
  tp_comm_overlap_disable_fc1: false
  tp_comm_overlap_disable_qkv: false
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_split_ag: true
  tp_comm_split_rs: true
  tp_only_amax_red: false
  transformer_impl: transformer_engine
  transformer_layer_spec:
    _call_: false
    _target_: megatron.bridge.models.gpt_provider.default_layer_spec
  use_cpu_initialization: false
  use_fused_weighted_squared_relu: false
  use_kitchen: false
  use_mamba_mem_eff_path: true
  use_ring_exchange_p2p: false
  use_te_activation_func: false
  use_te_rng_tracker: false
  use_transformer_engine_full_layer_spec: false
  use_transformer_engine_op_fuser: null
  variable_seq_lengths: false
  virtual_pipeline_model_parallel_size: null
  vocab_size: null
  wgrad_deferral_limit: 0
  window_size: null
nvrx_straggler: null
optimizer:
  _target_: megatron.core.optimizer.optimizer_config.OptimizerConfig
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_eps: 1.0e-05
  barrier_with_L1_time: false
  bf16: true
  clip_grad: 1.0
  config_logger_dir: ''
  decoupled_lr: null
  decoupled_min_lr: null
  exp_avg_dtype:
    _call_: false
    _target_: torch.float32
  exp_avg_sq_dtype:
    _call_: false
    _target_: torch.float32
  fp16: false
  fp8_recipe: null
  hysteresis: 2
  initial_loss_scale: 4294967296
  log_num_zeros_in_grad: false
  loss_scale: null
  loss_scale_window: 1000
  lr: 0.00025
  main_grads_dtype:
    _call_: false
    _target_: torch.float32
  main_params_dtype:
    _call_: false
    _target_: torch.float32
  min_loss_scale: 1.0
  min_lr: 2.5e-05
  optimizer: adam
  optimizer_cpu_offload: false
  optimizer_offload_fraction: 0.0
  overlap_cpu_optimizer_d2h_h2d: false
  overlap_param_gather: false
  overlap_param_gather_with_optimizer_step: false
  params_dtype:
    _call_: false
    _target_: torch.float32
  pin_cpu_grads: true
  pin_cpu_params: true
  reuse_grad_buf_for_mxfp8_param_ag: false
  sgd_momentum: 0.9
  store_param_remainders: true
  timers: null
  use_distributed_optimizer: true
  use_megatron_fsdp: false
  use_precision_aware_optimizer: false
  use_torch_optimizer_for_cpu_offload: false
  weight_decay: 0.1
peft: null
profiling:
  _target_: megatron.bridge.training.config.ProfilingConfig
  memory_snapshot_path: snapshot.pickle
  profile_ranks:
  - 0
  - 1
  profile_step_end: 10
  profile_step_start: 5
  record_memory_history: false
  record_shapes: true
  use_nsys_profiler: false
  use_pytorch_profiler: true
rerun_state_machine:
  _target_: megatron.bridge.training.config.RerunStateMachineConfig
  error_injection_rate: 0
  error_injection_type: transient_error
  rerun_mode: disabled
rng:
  _target_: megatron.bridge.training.config.RNGConfig
  data_parallel_random_init: false
  inference_rng_tracker: false
  seed: 42
  te_rng_tracker: false
scheduler:
  _target_: megatron.bridge.training.config.SchedulerConfig
  end_weight_decay: 0.033
  lr_decay_iters: 1168251
  lr_decay_steps: null
  lr_decay_style: cosine
  lr_warmup_fraction: null
  lr_warmup_init: 0.0
  lr_warmup_iters: 10
  lr_warmup_steps: null
  lr_wsd_decay_iters: null
  lr_wsd_decay_style: exponential
  override_opt_param_scheduler: true
  start_weight_decay: 0.033
  use_checkpoint_opt_param_scheduler: false
  wd_incr_steps: null
  weight_decay_incr_style: constant
  wsd_decay_steps: null
straggler: null
tokenizer:
  _target_: megatron.bridge.training.tokenizers.config.TokenizerConfig
  image_tag_type: null
  merge_file: null
  special_tokens: null
  tiktoken_num_special_tokens: 1000
  tiktoken_pattern: null
  tiktoken_special_tokens: null
  tokenizer_model: null
  tokenizer_prompt_format: null
  tokenizer_type: NullTokenizer
  vocab_extra_ids: 0
  vocab_file: null
  vocab_size: 32000
train:
  _target_: megatron.bridge.training.config.TrainingConfig
  check_weight_hash_across_dp_replicas_interval: null
  decrease_batch_size_if_needed: false
  empty_unused_memory_level: 0
  eval_interval: 2000
  eval_iters: 0
  exit_duration_in_mins: null
  exit_interval: null
  exit_signal:
    _args_:
    - 15
    _call_: true
    _target_: signal.Signals
  exit_signal_handler: false
  exit_signal_handler_for_dataloader: false
  global_batch_size: 8
  manual_gc: true
  manual_gc_eval: 100
  manual_gc_interval: 100
  micro_batch_size: 1
  rampup_batch_size: null
  skip_train: false
  train_iters: 20
  train_sync_interval: null

_target_: megatron.bridge.training.config.ConfigContainer
checkpoint:
  _target_: megatron.bridge.training.config.CheckpointConfig
  async_save: false
  ckpt_assume_constant_structure: false
  ckpt_convert_format: null
  ckpt_convert_save: null
  ckpt_format: torch_dist
  ckpt_step: null
  dist_ckpt_strictness: assume_ok_unexpected
  exit_on_missing_checkpoint: false
  finetune: false
  fully_parallel_load: false
  fully_parallel_save: true
  load: null
  load_main_params_from_ckpt: false
  load_optim: true
  load_rng: true
  non_persistent_ckpt_type: null
  non_persistent_global_ckpt_dir: null
  non_persistent_local_ckpt_algo: fully_parallel
  non_persistent_local_ckpt_dir: null
  non_persistent_save_interval: null
  pretrained_checkpoint: null
  replication: false
  replication_factor: 2
  replication_jump: null
  save: /opt/Megatron-Bridge/examples/recipes/llama/nemo_experiments/default/checkpoints
  save_interval: 2000
  save_optim: true
  save_rng: true
  use_checkpoint_args: false
  use_persistent_ckpt_worker: true
comm_overlap:
  _target_: megatron.bridge.training.comm_overlap.CommOverlapConfig
  align_param_gather: null
  batch_p2p_comm: null
  bucket_size: null
  data_parallel_size: null
  defer_embedding_wgrad_compute: null
  overlap_grad_reduce: null
  overlap_p2p_comm: null
  overlap_param_gather: null
  overlap_param_gather_with_optimizer_step: null
  tp_comm_bootstrap_backend: null
  tp_comm_overlap: false
  tp_comm_overlap_cfg: null
  wgrad_deferral_limit: null
dataset:
  _target_: megatron.bridge.training.config.GPTDatasetConfig
  add_extra_token_to_sequence: true
  blend: null
  blend_per_split: null
  create_attention_mask: true
  data_sharding: true
  dataloader_type: single
  drop_last_partial_validation_sequence: true
  eod_mask_loss: false
  full_validation: null
  mid_level_dataset_surplus: 0.005
  mmap_bin_files: true
  mock: true
  multiple_validation_sets: null
  num_dataset_builder_threads: 1
  num_workers: 8
  object_storage_cache_path: null
  path_to_cache: null
  persistent_workers: false
  pin_memory: true
  random_seed: 1234
  reset_attention_mask: false
  reset_position_ids: false
  sequence_length: 8192
  split: 1,1,1
  split_matrix:
  - - 0
    - 0.3333333333333333
  - - 0.3333333333333333
    - 0.6666666666666666
  - - 0.6666666666666666
    - 1.0
  tokenizer: null
ddp:
  _target_: megatron.core.distributed.distributed_data_parallel_config.DistributedDataParallelConfig
  align_param_gather: false
  average_in_collective: true
  bucket_size: null
  check_for_large_grads: false
  check_for_nan_in_grad: true
  data_parallel_sharding_strategy: no_shard
  fp8_param_gather: false
  fsdp_double_buffer: false
  grad_reduce_in_fp32: true
  gradient_reduce_div_fusion: true
  keep_fp8_transpose_cache: false
  nccl_ub: false
  num_distributed_optimizer_instances: 1
  outer_dp_sharding_strategy: no_shard
  overlap_grad_reduce: true
  overlap_param_gather: true
  pad_buckets_for_high_nccl_busbw: false
  preserve_fp32_weights: true
  reuse_grad_buf_for_mxfp8_param_ag: false
  suggested_communication_unit_size: null
  use_custom_fsdp: false
  use_distributed_optimizer: true
  use_megatron_fsdp: false
dist:
  _target_: megatron.bridge.training.config.DistributedInitConfig
  align_grad_reduce: true
  distributed_backend: nccl
  distributed_timeout_minutes: 10
  enable_megatron_core_experimental: false
  external_gpu_device_mapping: false
  high_priority_stream_groups: null
  lazy_init: false
  local_rank: 3
  nccl_communicator_config_path: null
  sharp_enabled_group: null
  use_gloo_process_groups: true
  use_sharp: false
  use_torch_fsdp2: false
  use_tp_pp_dp_mapping: false
ft: null
logger:
  _target_: megatron.bridge.training.config.LoggerConfig
  filter_warnings: true
  log_energy: false
  log_interval: 10
  log_loss_scale_to_tensorboard: true
  log_memory_to_tensorboard: false
  log_params_norm: false
  log_progress: false
  log_throughput: false
  log_timers_to_tensorboard: false
  log_validation_ppl_to_tensorboard: false
  log_world_size_to_tensorboard: false
  logging_level: 20
  modules_to_filter: null
  set_level_for_all_loggers: false
  tensorboard_dir: /opt/Megatron-Bridge/examples/recipes/llama/nemo_experiments/default/tb_logs
  tensorboard_log_interval: 1
  tensorboard_queue_size: 1000
  timing_log_level: 0
  timing_log_option: minmax
  wandb_entity: null
  wandb_exp_name: null
  wandb_project: null
  wandb_save_dir: null
mixed_precision: bf16_mixed
model:
  _target_: megatron.bridge.models.llama.llama_provider.Llama3ModelProvider8B
  account_for_embedding_in_pipeline_split: false
  account_for_loss_in_pipeline_split: false
  activation_func:
    _call_: false
    _target_: torch.nn.functional.silu
  activation_func_fp8_input_store: false
  add_bias_linear: false
  add_qkv_bias: false
  apply_query_key_layer_scaling: false
  apply_residual_connection_post_layernorm: false
  apply_rope_fusion: true
  async_tensor_model_parallel_allreduce: false
  attention_backend:
    _args_:
    - 5
    _call_: true
    _target_: megatron.core.transformer.enums.AttnBackend
  attention_dropout: 0.0
  attention_softmax_in_fp32: false
  autocast_dtype:
    _call_: false
    _target_: torch.float32
  barrier_with_L1_time: true
  batch_p2p_comm: true
  batch_p2p_sync: true
  bf16: false
  bias_activation_fusion: true
  bias_dropout_fusion: true
  calculate_per_token_loss: false
  clone_scatter_output_in_embedding: true
  config_logger_dir: ''
  context_parallel_size: 2
  cp_comm_type: null
  cpu_offloading: false
  cpu_offloading_activations: true
  cpu_offloading_num_layers: 0
  cpu_offloading_weights: true
  cross_entropy_fusion_impl: native
  cross_entropy_loss_fusion: true
  cuda_graph_retain_backward_graph: false
  cuda_graph_scope: full
  cuda_graph_use_single_mempool: false
  cuda_graph_warmup_steps: 3
  deallocate_pipeline_outputs: true
  defer_embedding_wgrad_compute: false
  delay_wgrad_compute: false
  deterministic_mode: false
  disable_bf16_reduced_precision_matmul: false
  disable_parameter_transpose_cache: false
  distribute_saved_activations: null
  embedding_init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.01
  embedding_init_method_std: 0.01
  enable_autocast: false
  enable_cuda_graph: false
  expert_model_parallel_size: 1
  expert_tensor_parallel_size: 1
  external_cuda_graph: false
  ffn_hidden_size: 14336
  finalize_model_grads_func: null
  first_last_layers_bf16: false
  flash_decode: false
  fp16: false
  fp16_lm_cross_entropy: false
  fp32_residual_connection: false
  fp8: null
  fp8_amax_compute_algo: most_recent
  fp8_amax_history_len: 1
  fp8_dot_product_attention: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_multi_head_attention: false
  fp8_param: false
  fp8_recipe: delayed
  fp8_wgrad: true
  gated_linear_unit: true
  generation_config: null
  grad_scale_func: null
  grad_sync_func: null
  gradient_accumulation_fusion: true
  hetereogenous_dist_checkpoint: false
  heterogeneous_block_specs: false
  hidden_dropout: 0.0
  hidden_size: 4096
  hierarchical_context_parallel_sizes: null
  inference_rng_tracker: false
  init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.01
  init_method_std: 0.01
  init_model_with_meta_device: false
  is_hybrid_model: false
  kv_channels: 128
  layernorm_epsilon: 1.0e-05
  layernorm_zero_centered_gamma: false
  make_vocab_size_divisible_by: 128
  mamba_head_dim: 64
  mamba_num_groups: 8
  mamba_num_heads: null
  mamba_state_dim: 128
  masked_softmax_fusion: true
  memory_efficient_layer_norm: false
  microbatch_group_size_per_vp_stage: 1
  mlp_chunks_for_prefill: 1
  moe_apply_probs_on_input: false
  moe_aux_loss_coeff: 0.0
  moe_deepep_num_sms: 20
  moe_enable_deepep: false
  moe_expert_capacity_factor: null
  moe_extended_tp: false
  moe_ffn_hidden_size: null
  moe_grouped_gemm: false
  moe_input_jitter_eps: null
  moe_layer_freq: 1
  moe_layer_recompute: false
  moe_pad_expert_input_to_capacity: false
  moe_per_layer_logging: false
  moe_permute_fusion: false
  moe_router_bias_update_rate: 0.001
  moe_router_dtype: null
  moe_router_enable_expert_bias: false
  moe_router_force_load_balancing: false
  moe_router_fusion: false
  moe_router_group_topk: null
  moe_router_load_balancing_type: aux_loss
  moe_router_num_groups: null
  moe_router_padding_for_fp8: false
  moe_router_pre_softmax: false
  moe_router_score_function: softmax
  moe_router_topk: 2
  moe_router_topk_limited_devices: null
  moe_router_topk_scaling_factor: null
  moe_shared_expert_intermediate_size: null
  moe_shared_expert_overlap: false
  moe_token_dispatcher_type: allgather
  moe_token_drop_policy: probs
  moe_token_dropping: false
  moe_use_legacy_grouped_gemm: false
  moe_z_loss_coeff: null
  mrope_section: null
  mtp_enabled: false
  mtp_loss_scaling_factor: null
  mtp_num_layers: null
  multi_latent_attention: false
  no_rope_freq: null
  no_sync_func: null
  normalization: RMSNorm
  num_attention_heads: 32
  num_layers: 32
  num_layers_at_end_in_bf16: 1
  num_layers_at_start_in_bf16: 1
  num_layers_in_first_pipeline_stage: null
  num_layers_in_last_pipeline_stage: null
  num_microbatches_with_partial_activation_checkpoints: null
  num_moe_experts: null
  num_query_groups: 8
  output_layer_init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.00125
  overlap_moe_expert_parallel_comm: false
  overlap_p2p_comm: false
  overlap_p2p_comm_warmup_flush: false
  parallel_output: true
  param_sync_func: null
  params_dtype:
    _call_: false
    _target_: torch.float32
  perform_initialization: true
  persist_layer_norm: false
  pipeline_dtype: null
  pipeline_model_parallel_comm_backend: null
  pipeline_model_parallel_layout: null
  pipeline_model_parallel_size: 1
  position_embedding_type: rope
  qk_layernorm: false
  quant_recipe: null
  recompute_granularity: null
  recompute_method: null
  recompute_modules:
  - core_attn
  recompute_num_layers: null
  rotary_base: 500000
  rotary_interleaved: false
  rotary_percent: 1.0
  scatter_embedding_sequence_parallel: true
  seq_len_interpolation_factor: null
  seq_length: 8192
  sequence_parallel: false
  share_embeddings_and_output_weights: false
  should_pad_vocab: false
  softmax_scale: null
  symmetric_ar_type: null
  tensor_model_parallel_size: 1
  test_mode: false
  timers: null
  tp_comm_atomic_ag: false
  tp_comm_atomic_rs: false
  tp_comm_bootstrap_backend: nccl
  tp_comm_bulk_dgrad: true
  tp_comm_bulk_wgrad: true
  tp_comm_overlap: false
  tp_comm_overlap_ag: true
  tp_comm_overlap_cfg: null
  tp_comm_overlap_disable_fc1: false
  tp_comm_overlap_disable_qkv: false
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_split_ag: true
  tp_comm_split_rs: true
  tp_only_amax_red: false
  transformer_impl: transformer_engine
  transformer_layer_spec:
    _call_: false
    _target_: megatron.bridge.models.gpt_provider.default_layer_spec
  use_cpu_initialization: false
  use_fused_weighted_squared_relu: false
  use_kitchen: false
  use_mamba_mem_eff_path: true
  use_ring_exchange_p2p: false
  use_te_activation_func: false
  use_te_rng_tracker: false
  use_transformer_engine_full_layer_spec: false
  use_transformer_engine_op_fuser: null
  variable_seq_lengths: false
  virtual_pipeline_model_parallel_size: null
  vocab_size: null
  wgrad_deferral_limit: 0
  window_size: null
nvrx_straggler: null
optimizer:
  _target_: megatron.core.optimizer.optimizer_config.OptimizerConfig
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_eps: 1.0e-05
  barrier_with_L1_time: false
  bf16: true
  clip_grad: 1.0
  config_logger_dir: ''
  decoupled_lr: null
  decoupled_min_lr: null
  exp_avg_dtype:
    _call_: false
    _target_: torch.float32
  exp_avg_sq_dtype:
    _call_: false
    _target_: torch.float32
  fp16: false
  fp8_recipe: null
  hysteresis: 2
  initial_loss_scale: 4294967296
  log_num_zeros_in_grad: false
  loss_scale: null
  loss_scale_window: 1000
  lr: 0.0003
  main_grads_dtype:
    _call_: false
    _target_: torch.float32
  main_params_dtype:
    _call_: false
    _target_: torch.float32
  min_loss_scale: 1.0
  min_lr: 3.0e-05
  optimizer: adam
  optimizer_cpu_offload: false
  optimizer_offload_fraction: 0.0
  overlap_cpu_optimizer_d2h_h2d: false
  overlap_param_gather: false
  overlap_param_gather_with_optimizer_step: false
  params_dtype:
    _call_: false
    _target_: torch.float32
  pin_cpu_grads: true
  pin_cpu_params: true
  reuse_grad_buf_for_mxfp8_param_ag: false
  sgd_momentum: 0.9
  store_param_remainders: true
  timers: null
  use_distributed_optimizer: true
  use_megatron_fsdp: false
  use_precision_aware_optimizer: false
  use_torch_optimizer_for_cpu_offload: false
  weight_decay: 0.1
peft: null
profiling: null
rerun_state_machine:
  _target_: megatron.bridge.training.config.RerunStateMachineConfig
  error_injection_rate: 0
  error_injection_type: transient_error
  rerun_mode: disabled
rng:
  _target_: megatron.bridge.training.config.RNGConfig
  data_parallel_random_init: false
  inference_rng_tracker: false
  seed: 1234
  te_rng_tracker: false
scheduler:
  _target_: megatron.bridge.training.config.SchedulerConfig
  end_weight_decay: 0.033
  lr_decay_iters: 1168251
  lr_decay_steps: null
  lr_decay_style: cosine
  lr_warmup_fraction: null
  lr_warmup_init: 0.0
  lr_warmup_iters: 2000
  lr_warmup_steps: null
  lr_wsd_decay_iters: null
  lr_wsd_decay_style: exponential
  override_opt_param_scheduler: true
  start_weight_decay: 0.033
  use_checkpoint_opt_param_scheduler: false
  wd_incr_steps: null
  weight_decay_incr_style: constant
  wsd_decay_steps: null
straggler: null
tokenizer:
  _target_: megatron.bridge.training.tokenizers.config.TokenizerConfig
  image_tag_type: null
  merge_file: null
  special_tokens: null
  tiktoken_num_special_tokens: 1000
  tiktoken_pattern: null
  tiktoken_special_tokens: null
  tokenizer_model: null
  tokenizer_prompt_format: null
  tokenizer_type: NullTokenizer
  vocab_extra_ids: 0
  vocab_file: null
  vocab_size: 32000
train:
  _target_: megatron.bridge.training.config.TrainingConfig
  check_weight_hash_across_dp_replicas_interval: null
  decrease_batch_size_if_needed: false
  empty_unused_memory_level: 0
  eval_interval: 2000
  eval_iters: 32
  exit_duration_in_mins: null
  exit_interval: null
  exit_signal:
    _args_:
    - 15
    _call_: true
    _target_: signal.Signals
  exit_signal_handler: false
  exit_signal_handler_for_dataloader: false
  global_batch_size: 512
  manual_gc: true
  manual_gc_eval: 100
  manual_gc_interval: 100
  micro_batch_size: 1
  rampup_batch_size: null
  skip_train: false
  train_iters: 1168251
  train_sync_interval: null

_target_: megatron.bridge.training.config.ConfigContainer
checkpoint:
  _target_: megatron.bridge.training.config.CheckpointConfig
  async_save: false
  ckpt_assume_constant_structure: false
  ckpt_convert_format: null
  ckpt_convert_save: null
  ckpt_format: torch_dist
  ckpt_step: null
  dist_ckpt_strictness: assume_ok_unexpected
  exit_on_missing_checkpoint: false
  finetune: false
  fully_parallel_load: false
  fully_parallel_save: true
  load: null
  load_main_params_from_ckpt: false
  load_optim: true
  load_rng: true
  non_persistent_ckpt_type: null
  non_persistent_global_ckpt_dir: null
  non_persistent_local_ckpt_algo: fully_parallel
  non_persistent_local_ckpt_dir: null
  non_persistent_save_interval: null
  pretrained_checkpoint: null
  replication: false
  replication_factor: 2
  replication_jump: null
  save: null
  save_interval: 2000
  save_optim: true
  save_rng: true
  use_checkpoint_args: false
  use_persistent_ckpt_worker: true
comm_overlap:
  _target_: megatron.bridge.training.comm_overlap.CommOverlapConfig
  align_param_gather: null
  batch_p2p_comm: null
  bucket_size: null
  data_parallel_size: null
  defer_embedding_wgrad_compute: null
  overlap_grad_reduce: null
  overlap_p2p_comm: null
  overlap_param_gather: null
  overlap_param_gather_with_optimizer_step: null
  tp_comm_bootstrap_backend: null
  tp_comm_overlap: false
  tp_comm_overlap_cfg: null
  wgrad_deferral_limit: null
dataset:
  _target_: megatron.bridge.training.config.GPTDatasetConfig
  add_extra_token_to_sequence: true
  blend: null
  blend_per_split: null
  create_attention_mask: true
  data_sharding: true
  dataloader_type: single
  drop_last_partial_validation_sequence: true
  eod_mask_loss: false
  full_validation: null
  mid_level_dataset_surplus: 0.005
  mmap_bin_files: true
  mock: true
  multiple_validation_sets: null
  num_dataset_builder_threads: 1
  num_workers: 8
  object_storage_cache_path: null
  path_to_cache: null
  persistent_workers: false
  pin_memory: true
  random_seed: 1234
  reset_attention_mask: false
  reset_position_ids: false
  sequence_length: 4096
  split: 1,1,1
  split_matrix:
  - - 0
    - 0.3333333333333333
  - - 0.3333333333333333
    - 0.6666666666666666
  - - 0.6666666666666666
    - 1.0
  tokenizer: null
ddp:
  _target_: megatron.core.distributed.distributed_data_parallel_config.DistributedDataParallelConfig
  align_param_gather: false
  average_in_collective: true
  bucket_size: null
  check_for_large_grads: false
  check_for_nan_in_grad: true
  data_parallel_sharding_strategy: no_shard
  fp8_param_gather: false
  fsdp_double_buffer: false
  grad_reduce_in_fp32: true
  gradient_reduce_div_fusion: true
  keep_fp8_transpose_cache: false
  nccl_ub: false
  num_distributed_optimizer_instances: 1
  outer_dp_sharding_strategy: no_shard
  overlap_grad_reduce: true
  overlap_param_gather: true
  pad_buckets_for_high_nccl_busbw: false
  preserve_fp32_weights: true
  reuse_grad_buf_for_mxfp8_param_ag: false
  suggested_communication_unit_size: null
  use_custom_fsdp: false
  use_distributed_optimizer: true
  use_megatron_fsdp: false
dist:
  _target_: megatron.bridge.training.config.DistributedInitConfig
  align_grad_reduce: true
  distributed_backend: nccl
  distributed_timeout_minutes: 10
  enable_megatron_core_experimental: false
  external_gpu_device_mapping: false
  high_priority_stream_groups: null
  lazy_init: false
  local_rank: 3
  nccl_communicator_config_path: null
  sharp_enabled_group: null
  use_gloo_process_groups: true
  use_sharp: false
  use_torch_fsdp2: false
  use_tp_pp_dp_mapping: false
ft: null
logger:
  _target_: megatron.bridge.training.config.LoggerConfig
  filter_warnings: true
  log_energy: false
  log_interval: 1
  log_loss_scale_to_tensorboard: true
  log_memory_to_tensorboard: false
  log_params_norm: false
  log_progress: false
  log_throughput: false
  log_timers_to_tensorboard: false
  log_validation_ppl_to_tensorboard: false
  log_world_size_to_tensorboard: false
  logging_level: 20
  modules_to_filter: null
  set_level_for_all_loggers: false
  tensorboard_dir: /opt/Megatron-Bridge/examples/recipes/llama/nemo_experiments/default/tb_logs
  tensorboard_log_interval: 1
  tensorboard_queue_size: 1000
  timing_log_level: 0
  timing_log_option: minmax
  wandb_entity: nvidia
  wandb_exp_name: llama3-8b-pretrain
  wandb_project: megatron-bridge
  wandb_save_dir: /lustre/fsw/portfolios/coreai/users/boxiangw/wandb/megatron-bridge/llama3-8b-pretrain
mixed_precision: bf16_mixed
model:
  _target_: megatron.bridge.models.llama.llama_provider.Llama3ModelProvider8B
  account_for_embedding_in_pipeline_split: false
  account_for_loss_in_pipeline_split: false
  activation_func:
    _call_: false
    _target_: torch.nn.functional.silu
  activation_func_fp8_input_store: false
  add_bias_linear: false
  add_qkv_bias: false
  apply_query_key_layer_scaling: false
  apply_residual_connection_post_layernorm: false
  apply_rope_fusion: true
  async_tensor_model_parallel_allreduce: false
  attention_backend:
    _args_:
    - 5
    _call_: true
    _target_: megatron.core.transformer.enums.AttnBackend
  attention_dropout: 0.0
  attention_softmax_in_fp32: false
  autocast_dtype:
    _call_: false
    _target_: torch.float32
  barrier_with_L1_time: true
  batch_p2p_comm: true
  batch_p2p_sync: true
  bf16: false
  bias_activation_fusion: true
  bias_dropout_fusion: true
  calculate_per_token_loss: false
  clone_scatter_output_in_embedding: true
  config_logger_dir: ''
  context_parallel_size: 2
  cp_comm_type: null
  cpu_offloading: false
  cpu_offloading_activations: true
  cpu_offloading_num_layers: 0
  cpu_offloading_weights: true
  cross_entropy_fusion_impl: native
  cross_entropy_loss_fusion: true
  cuda_graph_retain_backward_graph: false
  cuda_graph_scope: full
  cuda_graph_use_single_mempool: false
  cuda_graph_warmup_steps: 3
  deallocate_pipeline_outputs: true
  defer_embedding_wgrad_compute: false
  delay_wgrad_compute: false
  deterministic_mode: false
  disable_bf16_reduced_precision_matmul: false
  disable_parameter_transpose_cache: false
  distribute_saved_activations: null
  embedding_init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.01
  embedding_init_method_std: 0.01
  enable_autocast: false
  enable_cuda_graph: false
  expert_model_parallel_size: 1
  expert_tensor_parallel_size: 1
  external_cuda_graph: false
  ffn_hidden_size: 14336
  finalize_model_grads_func: null
  first_last_layers_bf16: false
  flash_decode: false
  fp16: false
  fp16_lm_cross_entropy: false
  fp32_residual_connection: false
  fp8: null
  fp8_amax_compute_algo: most_recent
  fp8_amax_history_len: 1
  fp8_dot_product_attention: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_multi_head_attention: false
  fp8_param: false
  fp8_recipe: delayed
  fp8_wgrad: true
  gated_linear_unit: true
  generation_config: null
  grad_scale_func: null
  grad_sync_func: null
  gradient_accumulation_fusion: true
  hetereogenous_dist_checkpoint: false
  heterogeneous_block_specs: false
  hidden_dropout: 0.0
  hidden_size: 4096
  hierarchical_context_parallel_sizes: null
  inference_rng_tracker: false
  init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.01
  init_method_std: 0.01
  init_model_with_meta_device: false
  is_hybrid_model: false
  kv_channels: 128
  layernorm_epsilon: 1.0e-05
  layernorm_zero_centered_gamma: false
  make_vocab_size_divisible_by: 128
  mamba_head_dim: 64
  mamba_num_groups: 8
  mamba_num_heads: null
  mamba_state_dim: 128
  masked_softmax_fusion: true
  memory_efficient_layer_norm: false
  microbatch_group_size_per_vp_stage: 1
  mlp_chunks_for_prefill: 1
  moe_apply_probs_on_input: false
  moe_aux_loss_coeff: 0.0
  moe_deepep_num_sms: 20
  moe_enable_deepep: false
  moe_expert_capacity_factor: null
  moe_extended_tp: false
  moe_ffn_hidden_size: null
  moe_grouped_gemm: false
  moe_input_jitter_eps: null
  moe_layer_freq: 1
  moe_layer_recompute: false
  moe_pad_expert_input_to_capacity: false
  moe_per_layer_logging: false
  moe_permute_fusion: false
  moe_router_bias_update_rate: 0.001
  moe_router_dtype: null
  moe_router_enable_expert_bias: false
  moe_router_force_load_balancing: false
  moe_router_fusion: false
  moe_router_group_topk: null
  moe_router_load_balancing_type: aux_loss
  moe_router_num_groups: null
  moe_router_padding_for_fp8: false
  moe_router_pre_softmax: false
  moe_router_score_function: softmax
  moe_router_topk: 2
  moe_router_topk_limited_devices: null
  moe_router_topk_scaling_factor: null
  moe_shared_expert_intermediate_size: null
  moe_shared_expert_overlap: false
  moe_token_dispatcher_type: allgather
  moe_token_drop_policy: probs
  moe_token_dropping: false
  moe_use_legacy_grouped_gemm: false
  moe_z_loss_coeff: null
  mrope_section: null
  mtp_enabled: false
  mtp_loss_scaling_factor: null
  mtp_num_layers: null
  multi_latent_attention: false
  no_rope_freq: null
  no_sync_func: null
  normalization: RMSNorm
  num_attention_heads: 32
  num_layers: 32
  num_layers_at_end_in_bf16: 1
  num_layers_at_start_in_bf16: 1
  num_layers_in_first_pipeline_stage: null
  num_layers_in_last_pipeline_stage: null
  num_microbatches_with_partial_activation_checkpoints: null
  num_moe_experts: null
  num_query_groups: 8
  output_layer_init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.00125
  overlap_moe_expert_parallel_comm: false
  overlap_p2p_comm: false
  overlap_p2p_comm_warmup_flush: false
  parallel_output: true
  param_sync_func: null
  params_dtype:
    _call_: false
    _target_: torch.float32
  perform_initialization: true
  persist_layer_norm: false
  pipeline_dtype: null
  pipeline_model_parallel_comm_backend: null
  pipeline_model_parallel_layout: null
  pipeline_model_parallel_size: 1
  position_embedding_type: rope
  qk_layernorm: false
  quant_recipe: null
  recompute_granularity: null
  recompute_method: null
  recompute_modules:
  - core_attn
  recompute_num_layers: null
  rotary_base: 500000
  rotary_interleaved: false
  rotary_percent: 1.0
  scatter_embedding_sequence_parallel: true
  seq_len_interpolation_factor: null
  seq_length: 4096
  sequence_parallel: false
  share_embeddings_and_output_weights: false
  should_pad_vocab: false
  softmax_scale: null
  symmetric_ar_type: null
  tensor_model_parallel_size: 1
  test_mode: false
  timers: null
  tp_comm_atomic_ag: false
  tp_comm_atomic_rs: false
  tp_comm_bootstrap_backend: nccl
  tp_comm_bulk_dgrad: true
  tp_comm_bulk_wgrad: true
  tp_comm_overlap: false
  tp_comm_overlap_ag: true
  tp_comm_overlap_cfg: null
  tp_comm_overlap_disable_fc1: false
  tp_comm_overlap_disable_qkv: false
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_split_ag: true
  tp_comm_split_rs: true
  tp_only_amax_red: false
  transformer_impl: transformer_engine
  transformer_layer_spec:
    _call_: false
    _target_: megatron.bridge.models.gpt_provider.default_layer_spec
  use_cpu_initialization: false
  use_fused_weighted_squared_relu: false
  use_kitchen: false
  use_mamba_mem_eff_path: true
  use_ring_exchange_p2p: false
  use_te_activation_func: false
  use_te_rng_tracker: false
  use_transformer_engine_full_layer_spec: false
  use_transformer_engine_op_fuser: null
  variable_seq_lengths: false
  virtual_pipeline_model_parallel_size: null
  vocab_size: null
  wgrad_deferral_limit: 0
  window_size: null
nvrx_straggler: null
optimizer:
  _target_: megatron.core.optimizer.optimizer_config.OptimizerConfig
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_eps: 1.0e-05
  barrier_with_L1_time: false
  bf16: true
  clip_grad: 1.0
  config_logger_dir: ''
  decoupled_lr: null
  decoupled_min_lr: null
  exp_avg_dtype:
    _call_: false
    _target_: torch.float32
  exp_avg_sq_dtype:
    _call_: false
    _target_: torch.float32
  fp16: false
  fp8_recipe: null
  hysteresis: 2
  initial_loss_scale: 4294967296
  log_num_zeros_in_grad: false
  loss_scale: null
  loss_scale_window: 1000
  lr: 0.00025
  main_grads_dtype:
    _call_: false
    _target_: torch.float32
  main_params_dtype:
    _call_: false
    _target_: torch.float32
  min_loss_scale: 1.0
  min_lr: 2.5e-05
  optimizer: adam
  optimizer_cpu_offload: false
  optimizer_offload_fraction: 0.0
  overlap_cpu_optimizer_d2h_h2d: false
  overlap_param_gather: false
  overlap_param_gather_with_optimizer_step: false
  params_dtype:
    _call_: false
    _target_: torch.float32
  pin_cpu_grads: true
  pin_cpu_params: true
  reuse_grad_buf_for_mxfp8_param_ag: false
  sgd_momentum: 0.9
  store_param_remainders: true
  timers: null
  use_distributed_optimizer: true
  use_megatron_fsdp: false
  use_precision_aware_optimizer: false
  use_torch_optimizer_for_cpu_offload: false
  weight_decay: 0.1
peft: null
profiling:
  _target_: megatron.bridge.training.config.ProfilingConfig
  memory_snapshot_path: snapshot.pickle
  profile_ranks:
  - 0
  - 1
  profile_step_end: 10
  profile_step_start: 5
  record_memory_history: false
  record_shapes: true
  use_nsys_profiler: false
  use_pytorch_profiler: true
rerun_state_machine:
  _target_: megatron.bridge.training.config.RerunStateMachineConfig
  error_injection_rate: 0
  error_injection_type: transient_error
  rerun_mode: disabled
rng:
  _target_: megatron.bridge.training.config.RNGConfig
  data_parallel_random_init: false
  inference_rng_tracker: false
  seed: 42
  te_rng_tracker: false
scheduler:
  _target_: megatron.bridge.training.config.SchedulerConfig
  end_weight_decay: 0.033
  lr_decay_iters: 1168251
  lr_decay_steps: null
  lr_decay_style: cosine
  lr_warmup_fraction: null
  lr_warmup_init: 0.0
  lr_warmup_iters: 10
  lr_warmup_steps: null
  lr_wsd_decay_iters: null
  lr_wsd_decay_style: exponential
  override_opt_param_scheduler: true
  start_weight_decay: 0.033
  use_checkpoint_opt_param_scheduler: false
  wd_incr_steps: null
  weight_decay_incr_style: constant
  wsd_decay_steps: null
straggler: null
tokenizer:
  _target_: megatron.bridge.training.tokenizers.config.TokenizerConfig
  image_tag_type: null
  merge_file: null
  special_tokens: null
  tiktoken_num_special_tokens: 1000
  tiktoken_pattern: null
  tiktoken_special_tokens: null
  tokenizer_model: null
  tokenizer_prompt_format: null
  tokenizer_type: NullTokenizer
  vocab_extra_ids: 0
  vocab_file: null
  vocab_size: 32000
train:
  _target_: megatron.bridge.training.config.TrainingConfig
  check_weight_hash_across_dp_replicas_interval: null
  decrease_batch_size_if_needed: false
  empty_unused_memory_level: 0
  eval_interval: 2000
  eval_iters: 0
  exit_duration_in_mins: null
  exit_interval: null
  exit_signal:
    _args_:
    - 15
    _call_: true
    _target_: signal.Signals
  exit_signal_handler: false
  exit_signal_handler_for_dataloader: false
  global_batch_size: 8
  manual_gc: true
  manual_gc_eval: 100
  manual_gc_interval: 100
  micro_batch_size: 1
  rampup_batch_size: null
  skip_train: false
  train_iters: 20
  train_sync_interval: null

_target_: megatron.bridge.training.config.ConfigContainer
checkpoint:
  _target_: megatron.bridge.training.config.CheckpointConfig
  async_save: false
  ckpt_assume_constant_structure: false
  ckpt_convert_format: null
  ckpt_convert_save: null
  ckpt_format: torch_dist
  ckpt_step: null
  dist_ckpt_strictness: assume_ok_unexpected
  exit_on_missing_checkpoint: false
  finetune: false
  fully_parallel_load: false
  fully_parallel_save: true
  load: null
  load_main_params_from_ckpt: false
  load_optim: true
  load_rng: true
  non_persistent_ckpt_type: null
  non_persistent_global_ckpt_dir: null
  non_persistent_local_ckpt_algo: fully_parallel
  non_persistent_local_ckpt_dir: null
  non_persistent_save_interval: null
  pretrained_checkpoint: null
  replication: false
  replication_factor: 2
  replication_jump: null
  save: /opt/Megatron-Bridge/examples/recipes/llama/nemo_experiments/default/checkpoints
  save_interval: 2000
  save_optim: true
  save_rng: true
  use_checkpoint_args: false
  use_persistent_ckpt_worker: true
comm_overlap:
  _target_: megatron.bridge.training.comm_overlap.CommOverlapConfig
  align_param_gather: null
  batch_p2p_comm: null
  bucket_size: null
  data_parallel_size: null
  defer_embedding_wgrad_compute: null
  overlap_grad_reduce: null
  overlap_p2p_comm: null
  overlap_param_gather: null
  overlap_param_gather_with_optimizer_step: null
  tp_comm_bootstrap_backend: null
  tp_comm_overlap: false
  tp_comm_overlap_cfg: null
  wgrad_deferral_limit: null
dataset:
  _target_: megatron.bridge.training.config.GPTDatasetConfig
  add_extra_token_to_sequence: true
  blend: null
  blend_per_split: null
  create_attention_mask: true
  data_sharding: true
  dataloader_type: single
  drop_last_partial_validation_sequence: true
  eod_mask_loss: false
  full_validation: null
  mid_level_dataset_surplus: 0.005
  mmap_bin_files: true
  mock: true
  multiple_validation_sets: null
  num_dataset_builder_threads: 1
  num_workers: 8
  object_storage_cache_path: null
  path_to_cache: null
  persistent_workers: false
  pin_memory: true
  random_seed: 1234
  reset_attention_mask: false
  reset_position_ids: false
  sequence_length: 8192
  split: 1,1,1
  split_matrix:
  - - 0
    - 0.3333333333333333
  - - 0.3333333333333333
    - 0.6666666666666666
  - - 0.6666666666666666
    - 1.0
  tokenizer: null
ddp:
  _target_: megatron.core.distributed.distributed_data_parallel_config.DistributedDataParallelConfig
  align_param_gather: false
  average_in_collective: true
  bucket_size: null
  check_for_large_grads: false
  check_for_nan_in_grad: true
  data_parallel_sharding_strategy: no_shard
  fp8_param_gather: false
  fsdp_double_buffer: false
  grad_reduce_in_fp32: true
  gradient_reduce_div_fusion: true
  keep_fp8_transpose_cache: false
  nccl_ub: false
  num_distributed_optimizer_instances: 1
  outer_dp_sharding_strategy: no_shard
  overlap_grad_reduce: true
  overlap_param_gather: true
  pad_buckets_for_high_nccl_busbw: false
  preserve_fp32_weights: true
  reuse_grad_buf_for_mxfp8_param_ag: false
  suggested_communication_unit_size: null
  use_custom_fsdp: false
  use_distributed_optimizer: true
  use_megatron_fsdp: false
dist:
  _target_: megatron.bridge.training.config.DistributedInitConfig
  align_grad_reduce: true
  distributed_backend: nccl
  distributed_timeout_minutes: 10
  enable_megatron_core_experimental: false
  external_gpu_device_mapping: false
  high_priority_stream_groups: null
  lazy_init: false
  local_rank: 7
  nccl_communicator_config_path: null
  sharp_enabled_group: null
  use_gloo_process_groups: true
  use_sharp: false
  use_torch_fsdp2: false
  use_tp_pp_dp_mapping: false
ft: null
logger:
  _target_: megatron.bridge.training.config.LoggerConfig
  filter_warnings: true
  log_energy: false
  log_interval: 10
  log_loss_scale_to_tensorboard: true
  log_memory_to_tensorboard: false
  log_params_norm: false
  log_progress: false
  log_throughput: false
  log_timers_to_tensorboard: false
  log_validation_ppl_to_tensorboard: false
  log_world_size_to_tensorboard: false
  logging_level: 20
  modules_to_filter: null
  set_level_for_all_loggers: false
  tensorboard_dir: /opt/Megatron-Bridge/examples/recipes/llama/nemo_experiments/default/tb_logs
  tensorboard_log_interval: 1
  tensorboard_queue_size: 1000
  timing_log_level: 0
  timing_log_option: minmax
  wandb_entity: null
  wandb_exp_name: null
  wandb_project: null
  wandb_save_dir: null
mixed_precision: bf16_mixed
model:
  _target_: megatron.bridge.models.llama.llama_provider.Llama3ModelProvider8B
  account_for_embedding_in_pipeline_split: false
  account_for_loss_in_pipeline_split: false
  activation_func:
    _call_: false
    _target_: torch.nn.functional.silu
  activation_func_fp8_input_store: false
  add_bias_linear: false
  add_qkv_bias: false
  apply_query_key_layer_scaling: false
  apply_residual_connection_post_layernorm: false
  apply_rope_fusion: true
  async_tensor_model_parallel_allreduce: false
  attention_backend:
    _args_:
    - 5
    _call_: true
    _target_: megatron.core.transformer.enums.AttnBackend
  attention_dropout: 0.0
  attention_softmax_in_fp32: false
  autocast_dtype:
    _call_: false
    _target_: torch.float32
  barrier_with_L1_time: true
  batch_p2p_comm: true
  batch_p2p_sync: true
  bf16: false
  bias_activation_fusion: true
  bias_dropout_fusion: true
  calculate_per_token_loss: false
  clone_scatter_output_in_embedding: true
  config_logger_dir: ''
  context_parallel_size: 2
  cp_comm_type: null
  cpu_offloading: false
  cpu_offloading_activations: true
  cpu_offloading_num_layers: 0
  cpu_offloading_weights: true
  cross_entropy_fusion_impl: native
  cross_entropy_loss_fusion: true
  cuda_graph_retain_backward_graph: false
  cuda_graph_scope: full
  cuda_graph_use_single_mempool: false
  cuda_graph_warmup_steps: 3
  deallocate_pipeline_outputs: true
  defer_embedding_wgrad_compute: false
  delay_wgrad_compute: false
  deterministic_mode: false
  disable_bf16_reduced_precision_matmul: false
  disable_parameter_transpose_cache: false
  distribute_saved_activations: null
  embedding_init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.01
  embedding_init_method_std: 0.01
  enable_autocast: false
  enable_cuda_graph: false
  expert_model_parallel_size: 1
  expert_tensor_parallel_size: 1
  external_cuda_graph: false
  ffn_hidden_size: 14336
  finalize_model_grads_func: null
  first_last_layers_bf16: false
  flash_decode: false
  fp16: false
  fp16_lm_cross_entropy: false
  fp32_residual_connection: false
  fp8: null
  fp8_amax_compute_algo: most_recent
  fp8_amax_history_len: 1
  fp8_dot_product_attention: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_multi_head_attention: false
  fp8_param: false
  fp8_recipe: delayed
  fp8_wgrad: true
  gated_linear_unit: true
  generation_config: null
  grad_scale_func: null
  grad_sync_func: null
  gradient_accumulation_fusion: true
  hetereogenous_dist_checkpoint: false
  heterogeneous_block_specs: false
  hidden_dropout: 0.0
  hidden_size: 4096
  hierarchical_context_parallel_sizes: null
  inference_rng_tracker: false
  init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.01
  init_method_std: 0.01
  init_model_with_meta_device: false
  is_hybrid_model: false
  kv_channels: 128
  layernorm_epsilon: 1.0e-05
  layernorm_zero_centered_gamma: false
  make_vocab_size_divisible_by: 128
  mamba_head_dim: 64
  mamba_num_groups: 8
  mamba_num_heads: null
  mamba_state_dim: 128
  masked_softmax_fusion: true
  memory_efficient_layer_norm: false
  microbatch_group_size_per_vp_stage: 1
  mlp_chunks_for_prefill: 1
  moe_apply_probs_on_input: false
  moe_aux_loss_coeff: 0.0
  moe_deepep_num_sms: 20
  moe_enable_deepep: false
  moe_expert_capacity_factor: null
  moe_extended_tp: false
  moe_ffn_hidden_size: null
  moe_grouped_gemm: false
  moe_input_jitter_eps: null
  moe_layer_freq: 1
  moe_layer_recompute: false
  moe_pad_expert_input_to_capacity: false
  moe_per_layer_logging: false
  moe_permute_fusion: false
  moe_router_bias_update_rate: 0.001
  moe_router_dtype: null
  moe_router_enable_expert_bias: false
  moe_router_force_load_balancing: false
  moe_router_fusion: false
  moe_router_group_topk: null
  moe_router_load_balancing_type: aux_loss
  moe_router_num_groups: null
  moe_router_padding_for_fp8: false
  moe_router_pre_softmax: false
  moe_router_score_function: softmax
  moe_router_topk: 2
  moe_router_topk_limited_devices: null
  moe_router_topk_scaling_factor: null
  moe_shared_expert_intermediate_size: null
  moe_shared_expert_overlap: false
  moe_token_dispatcher_type: allgather
  moe_token_drop_policy: probs
  moe_token_dropping: false
  moe_use_legacy_grouped_gemm: false
  moe_z_loss_coeff: null
  mrope_section: null
  mtp_enabled: false
  mtp_loss_scaling_factor: null
  mtp_num_layers: null
  multi_latent_attention: false
  no_rope_freq: null
  no_sync_func: null
  normalization: RMSNorm
  num_attention_heads: 32
  num_layers: 32
  num_layers_at_end_in_bf16: 1
  num_layers_at_start_in_bf16: 1
  num_layers_in_first_pipeline_stage: null
  num_layers_in_last_pipeline_stage: null
  num_microbatches_with_partial_activation_checkpoints: null
  num_moe_experts: null
  num_query_groups: 8
  output_layer_init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.00125
  overlap_moe_expert_parallel_comm: false
  overlap_p2p_comm: false
  overlap_p2p_comm_warmup_flush: false
  parallel_output: true
  param_sync_func: null
  params_dtype:
    _call_: false
    _target_: torch.float32
  perform_initialization: true
  persist_layer_norm: false
  pipeline_dtype: null
  pipeline_model_parallel_comm_backend: null
  pipeline_model_parallel_layout: null
  pipeline_model_parallel_size: 1
  position_embedding_type: rope
  qk_layernorm: false
  quant_recipe: null
  recompute_granularity: null
  recompute_method: null
  recompute_modules:
  - core_attn
  recompute_num_layers: null
  rotary_base: 500000
  rotary_interleaved: false
  rotary_percent: 1.0
  scatter_embedding_sequence_parallel: true
  seq_len_interpolation_factor: null
  seq_length: 8192
  sequence_parallel: false
  share_embeddings_and_output_weights: false
  should_pad_vocab: false
  softmax_scale: null
  symmetric_ar_type: null
  tensor_model_parallel_size: 1
  test_mode: false
  timers: null
  tp_comm_atomic_ag: false
  tp_comm_atomic_rs: false
  tp_comm_bootstrap_backend: nccl
  tp_comm_bulk_dgrad: true
  tp_comm_bulk_wgrad: true
  tp_comm_overlap: false
  tp_comm_overlap_ag: true
  tp_comm_overlap_cfg: null
  tp_comm_overlap_disable_fc1: false
  tp_comm_overlap_disable_qkv: false
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_split_ag: true
  tp_comm_split_rs: true
  tp_only_amax_red: false
  transformer_impl: transformer_engine
  transformer_layer_spec:
    _call_: false
    _target_: megatron.bridge.models.gpt_provider.default_layer_spec
  use_cpu_initialization: false
  use_fused_weighted_squared_relu: false
  use_kitchen: false
  use_mamba_mem_eff_path: true
  use_ring_exchange_p2p: false
  use_te_activation_func: false
  use_te_rng_tracker: false
  use_transformer_engine_full_layer_spec: false
  use_transformer_engine_op_fuser: null
  variable_seq_lengths: false
  virtual_pipeline_model_parallel_size: null
  vocab_size: null
  wgrad_deferral_limit: 0
  window_size: null
nvrx_straggler: null
optimizer:
  _target_: megatron.core.optimizer.optimizer_config.OptimizerConfig
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_eps: 1.0e-05
  barrier_with_L1_time: false
  bf16: true
  clip_grad: 1.0
  config_logger_dir: ''
  decoupled_lr: null
  decoupled_min_lr: null
  exp_avg_dtype:
    _call_: false
    _target_: torch.float32
  exp_avg_sq_dtype:
    _call_: false
    _target_: torch.float32
  fp16: false
  fp8_recipe: null
  hysteresis: 2
  initial_loss_scale: 4294967296
  log_num_zeros_in_grad: false
  loss_scale: null
  loss_scale_window: 1000
  lr: 0.0003
  main_grads_dtype:
    _call_: false
    _target_: torch.float32
  main_params_dtype:
    _call_: false
    _target_: torch.float32
  min_loss_scale: 1.0
  min_lr: 3.0e-05
  optimizer: adam
  optimizer_cpu_offload: false
  optimizer_offload_fraction: 0.0
  overlap_cpu_optimizer_d2h_h2d: false
  overlap_param_gather: false
  overlap_param_gather_with_optimizer_step: false
  params_dtype:
    _call_: false
    _target_: torch.float32
  pin_cpu_grads: true
  pin_cpu_params: true
  reuse_grad_buf_for_mxfp8_param_ag: false
  sgd_momentum: 0.9
  store_param_remainders: true
  timers: null
  use_distributed_optimizer: true
  use_megatron_fsdp: false
  use_precision_aware_optimizer: false
  use_torch_optimizer_for_cpu_offload: false
  weight_decay: 0.1
peft: null
profiling: null
rerun_state_machine:
  _target_: megatron.bridge.training.config.RerunStateMachineConfig
  error_injection_rate: 0
  error_injection_type: transient_error
  rerun_mode: disabled
rng:
  _target_: megatron.bridge.training.config.RNGConfig
  data_parallel_random_init: false
  inference_rng_tracker: false
  seed: 1234
  te_rng_tracker: false
scheduler:
  _target_: megatron.bridge.training.config.SchedulerConfig
  end_weight_decay: 0.033
  lr_decay_iters: 1168251
  lr_decay_steps: null
  lr_decay_style: cosine
  lr_warmup_fraction: null
  lr_warmup_init: 0.0
  lr_warmup_iters: 2000
  lr_warmup_steps: null
  lr_wsd_decay_iters: null
  lr_wsd_decay_style: exponential
  override_opt_param_scheduler: true
  start_weight_decay: 0.033
  use_checkpoint_opt_param_scheduler: false
  wd_incr_steps: null
  weight_decay_incr_style: constant
  wsd_decay_steps: null
straggler: null
tokenizer:
  _target_: megatron.bridge.training.tokenizers.config.TokenizerConfig
  image_tag_type: null
  merge_file: null
  special_tokens: null
  tiktoken_num_special_tokens: 1000
  tiktoken_pattern: null
  tiktoken_special_tokens: null
  tokenizer_model: null
  tokenizer_prompt_format: null
  tokenizer_type: NullTokenizer
  vocab_extra_ids: 0
  vocab_file: null
  vocab_size: 32000
train:
  _target_: megatron.bridge.training.config.TrainingConfig
  check_weight_hash_across_dp_replicas_interval: null
  decrease_batch_size_if_needed: false
  empty_unused_memory_level: 0
  eval_interval: 2000
  eval_iters: 32
  exit_duration_in_mins: null
  exit_interval: null
  exit_signal:
    _args_:
    - 15
    _call_: true
    _target_: signal.Signals
  exit_signal_handler: false
  exit_signal_handler_for_dataloader: false
  global_batch_size: 512
  manual_gc: true
  manual_gc_eval: 100
  manual_gc_interval: 100
  micro_batch_size: 1
  rampup_batch_size: null
  skip_train: false
  train_iters: 1168251
  train_sync_interval: null

_target_: megatron.bridge.training.config.ConfigContainer
checkpoint:
  _target_: megatron.bridge.training.config.CheckpointConfig
  async_save: false
  ckpt_assume_constant_structure: false
  ckpt_convert_format: null
  ckpt_convert_save: null
  ckpt_format: torch_dist
  ckpt_step: null
  dist_ckpt_strictness: assume_ok_unexpected
  exit_on_missing_checkpoint: false
  finetune: false
  fully_parallel_load: false
  fully_parallel_save: true
  load: null
  load_main_params_from_ckpt: false
  load_optim: true
  load_rng: true
  non_persistent_ckpt_type: null
  non_persistent_global_ckpt_dir: null
  non_persistent_local_ckpt_algo: fully_parallel
  non_persistent_local_ckpt_dir: null
  non_persistent_save_interval: null
  pretrained_checkpoint: null
  replication: false
  replication_factor: 2
  replication_jump: null
  save: null
  save_interval: 2000
  save_optim: true
  save_rng: true
  use_checkpoint_args: false
  use_persistent_ckpt_worker: true
comm_overlap:
  _target_: megatron.bridge.training.comm_overlap.CommOverlapConfig
  align_param_gather: null
  batch_p2p_comm: null
  bucket_size: null
  data_parallel_size: null
  defer_embedding_wgrad_compute: null
  overlap_grad_reduce: null
  overlap_p2p_comm: null
  overlap_param_gather: null
  overlap_param_gather_with_optimizer_step: null
  tp_comm_bootstrap_backend: null
  tp_comm_overlap: false
  tp_comm_overlap_cfg: null
  wgrad_deferral_limit: null
dataset:
  _target_: megatron.bridge.training.config.GPTDatasetConfig
  add_extra_token_to_sequence: true
  blend: null
  blend_per_split: null
  create_attention_mask: true
  data_sharding: true
  dataloader_type: single
  drop_last_partial_validation_sequence: true
  eod_mask_loss: false
  full_validation: null
  mid_level_dataset_surplus: 0.005
  mmap_bin_files: true
  mock: true
  multiple_validation_sets: null
  num_dataset_builder_threads: 1
  num_workers: 8
  object_storage_cache_path: null
  path_to_cache: null
  persistent_workers: false
  pin_memory: true
  random_seed: 1234
  reset_attention_mask: false
  reset_position_ids: false
  sequence_length: 4096
  split: 1,1,1
  split_matrix:
  - - 0
    - 0.3333333333333333
  - - 0.3333333333333333
    - 0.6666666666666666
  - - 0.6666666666666666
    - 1.0
  tokenizer: null
ddp:
  _target_: megatron.core.distributed.distributed_data_parallel_config.DistributedDataParallelConfig
  align_param_gather: false
  average_in_collective: true
  bucket_size: null
  check_for_large_grads: false
  check_for_nan_in_grad: true
  data_parallel_sharding_strategy: no_shard
  fp8_param_gather: false
  fsdp_double_buffer: false
  grad_reduce_in_fp32: true
  gradient_reduce_div_fusion: true
  keep_fp8_transpose_cache: false
  nccl_ub: false
  num_distributed_optimizer_instances: 1
  outer_dp_sharding_strategy: no_shard
  overlap_grad_reduce: true
  overlap_param_gather: true
  pad_buckets_for_high_nccl_busbw: false
  preserve_fp32_weights: true
  reuse_grad_buf_for_mxfp8_param_ag: false
  suggested_communication_unit_size: null
  use_custom_fsdp: false
  use_distributed_optimizer: true
  use_megatron_fsdp: false
dist:
  _target_: megatron.bridge.training.config.DistributedInitConfig
  align_grad_reduce: true
  distributed_backend: nccl
  distributed_timeout_minutes: 10
  enable_megatron_core_experimental: false
  external_gpu_device_mapping: false
  high_priority_stream_groups: null
  lazy_init: false
  local_rank: 7
  nccl_communicator_config_path: null
  sharp_enabled_group: null
  use_gloo_process_groups: true
  use_sharp: false
  use_torch_fsdp2: false
  use_tp_pp_dp_mapping: false
ft: null
logger:
  _target_: megatron.bridge.training.config.LoggerConfig
  filter_warnings: true
  log_energy: false
  log_interval: 1
  log_loss_scale_to_tensorboard: true
  log_memory_to_tensorboard: false
  log_params_norm: false
  log_progress: false
  log_throughput: false
  log_timers_to_tensorboard: false
  log_validation_ppl_to_tensorboard: false
  log_world_size_to_tensorboard: false
  logging_level: 20
  modules_to_filter: null
  set_level_for_all_loggers: false
  tensorboard_dir: /opt/Megatron-Bridge/examples/recipes/llama/nemo_experiments/default/tb_logs
  tensorboard_log_interval: 1
  tensorboard_queue_size: 1000
  timing_log_level: 0
  timing_log_option: minmax
  wandb_entity: nvidia
  wandb_exp_name: llama3-8b-pretrain
  wandb_project: megatron-bridge
  wandb_save_dir: /lustre/fsw/portfolios/coreai/users/boxiangw/wandb/megatron-bridge/llama3-8b-pretrain
mixed_precision: bf16_mixed
model:
  _target_: megatron.bridge.models.llama.llama_provider.Llama3ModelProvider8B
  account_for_embedding_in_pipeline_split: false
  account_for_loss_in_pipeline_split: false
  activation_func:
    _call_: false
    _target_: torch.nn.functional.silu
  activation_func_fp8_input_store: false
  add_bias_linear: false
  add_qkv_bias: false
  apply_query_key_layer_scaling: false
  apply_residual_connection_post_layernorm: false
  apply_rope_fusion: true
  async_tensor_model_parallel_allreduce: false
  attention_backend:
    _args_:
    - 5
    _call_: true
    _target_: megatron.core.transformer.enums.AttnBackend
  attention_dropout: 0.0
  attention_softmax_in_fp32: false
  autocast_dtype:
    _call_: false
    _target_: torch.float32
  barrier_with_L1_time: true
  batch_p2p_comm: true
  batch_p2p_sync: true
  bf16: false
  bias_activation_fusion: true
  bias_dropout_fusion: true
  calculate_per_token_loss: false
  clone_scatter_output_in_embedding: true
  config_logger_dir: ''
  context_parallel_size: 2
  cp_comm_type: null
  cpu_offloading: false
  cpu_offloading_activations: true
  cpu_offloading_num_layers: 0
  cpu_offloading_weights: true
  cross_entropy_fusion_impl: native
  cross_entropy_loss_fusion: true
  cuda_graph_retain_backward_graph: false
  cuda_graph_scope: full
  cuda_graph_use_single_mempool: false
  cuda_graph_warmup_steps: 3
  deallocate_pipeline_outputs: true
  defer_embedding_wgrad_compute: false
  delay_wgrad_compute: false
  deterministic_mode: false
  disable_bf16_reduced_precision_matmul: false
  disable_parameter_transpose_cache: false
  distribute_saved_activations: null
  embedding_init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.01
  embedding_init_method_std: 0.01
  enable_autocast: false
  enable_cuda_graph: false
  expert_model_parallel_size: 1
  expert_tensor_parallel_size: 1
  external_cuda_graph: false
  ffn_hidden_size: 14336
  finalize_model_grads_func: null
  first_last_layers_bf16: false
  flash_decode: false
  fp16: false
  fp16_lm_cross_entropy: false
  fp32_residual_connection: false
  fp8: null
  fp8_amax_compute_algo: most_recent
  fp8_amax_history_len: 1
  fp8_dot_product_attention: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_multi_head_attention: false
  fp8_param: false
  fp8_recipe: delayed
  fp8_wgrad: true
  gated_linear_unit: true
  generation_config: null
  grad_scale_func: null
  grad_sync_func: null
  gradient_accumulation_fusion: true
  hetereogenous_dist_checkpoint: false
  heterogeneous_block_specs: false
  hidden_dropout: 0.0
  hidden_size: 4096
  hierarchical_context_parallel_sizes: null
  inference_rng_tracker: false
  init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.01
  init_method_std: 0.01
  init_model_with_meta_device: false
  is_hybrid_model: false
  kv_channels: 128
  layernorm_epsilon: 1.0e-05
  layernorm_zero_centered_gamma: false
  make_vocab_size_divisible_by: 128
  mamba_head_dim: 64
  mamba_num_groups: 8
  mamba_num_heads: null
  mamba_state_dim: 128
  masked_softmax_fusion: true
  memory_efficient_layer_norm: false
  microbatch_group_size_per_vp_stage: 1
  mlp_chunks_for_prefill: 1
  moe_apply_probs_on_input: false
  moe_aux_loss_coeff: 0.0
  moe_deepep_num_sms: 20
  moe_enable_deepep: false
  moe_expert_capacity_factor: null
  moe_extended_tp: false
  moe_ffn_hidden_size: null
  moe_grouped_gemm: false
  moe_input_jitter_eps: null
  moe_layer_freq: 1
  moe_layer_recompute: false
  moe_pad_expert_input_to_capacity: false
  moe_per_layer_logging: false
  moe_permute_fusion: false
  moe_router_bias_update_rate: 0.001
  moe_router_dtype: null
  moe_router_enable_expert_bias: false
  moe_router_force_load_balancing: false
  moe_router_fusion: false
  moe_router_group_topk: null
  moe_router_load_balancing_type: aux_loss
  moe_router_num_groups: null
  moe_router_padding_for_fp8: false
  moe_router_pre_softmax: false
  moe_router_score_function: softmax
  moe_router_topk: 2
  moe_router_topk_limited_devices: null
  moe_router_topk_scaling_factor: null
  moe_shared_expert_intermediate_size: null
  moe_shared_expert_overlap: false
  moe_token_dispatcher_type: allgather
  moe_token_drop_policy: probs
  moe_token_dropping: false
  moe_use_legacy_grouped_gemm: false
  moe_z_loss_coeff: null
  mrope_section: null
  mtp_enabled: false
  mtp_loss_scaling_factor: null
  mtp_num_layers: null
  multi_latent_attention: false
  no_rope_freq: null
  no_sync_func: null
  normalization: RMSNorm
  num_attention_heads: 32
  num_layers: 32
  num_layers_at_end_in_bf16: 1
  num_layers_at_start_in_bf16: 1
  num_layers_in_first_pipeline_stage: null
  num_layers_in_last_pipeline_stage: null
  num_microbatches_with_partial_activation_checkpoints: null
  num_moe_experts: null
  num_query_groups: 8
  output_layer_init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.00125
  overlap_moe_expert_parallel_comm: false
  overlap_p2p_comm: false
  overlap_p2p_comm_warmup_flush: false
  parallel_output: true
  param_sync_func: null
  params_dtype:
    _call_: false
    _target_: torch.float32
  perform_initialization: true
  persist_layer_norm: false
  pipeline_dtype: null
  pipeline_model_parallel_comm_backend: null
  pipeline_model_parallel_layout: null
  pipeline_model_parallel_size: 1
  position_embedding_type: rope
  qk_layernorm: false
  quant_recipe: null
  recompute_granularity: null
  recompute_method: null
  recompute_modules:
  - core_attn
  recompute_num_layers: null
  rotary_base: 500000
  rotary_interleaved: false
  rotary_percent: 1.0
  scatter_embedding_sequence_parallel: true
  seq_len_interpolation_factor: null
  seq_length: 4096
  sequence_parallel: false
  share_embeddings_and_output_weights: false
  should_pad_vocab: false
  softmax_scale: null
  symmetric_ar_type: null
  tensor_model_parallel_size: 1
  test_mode: false
  timers: null
  tp_comm_atomic_ag: false
  tp_comm_atomic_rs: false
  tp_comm_bootstrap_backend: nccl
  tp_comm_bulk_dgrad: true
  tp_comm_bulk_wgrad: true
  tp_comm_overlap: false
  tp_comm_overlap_ag: true
  tp_comm_overlap_cfg: null
  tp_comm_overlap_disable_fc1: false
  tp_comm_overlap_disable_qkv: false
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_split_ag: true
  tp_comm_split_rs: true
  tp_only_amax_red: false
  transformer_impl: transformer_engine
  transformer_layer_spec:
    _call_: false
    _target_: megatron.bridge.models.gpt_provider.default_layer_spec
  use_cpu_initialization: false
  use_fused_weighted_squared_relu: false
  use_kitchen: false
  use_mamba_mem_eff_path: true
  use_ring_exchange_p2p: false
  use_te_activation_func: false
  use_te_rng_tracker: false
  use_transformer_engine_full_layer_spec: false
  use_transformer_engine_op_fuser: null
  variable_seq_lengths: false
  virtual_pipeline_model_parallel_size: null
  vocab_size: null
  wgrad_deferral_limit: 0
  window_size: null
nvrx_straggler: null
optimizer:
  _target_: megatron.core.optimizer.optimizer_config.OptimizerConfig
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_eps: 1.0e-05
  barrier_with_L1_time: false
  bf16: true
  clip_grad: 1.0
  config_logger_dir: ''
  decoupled_lr: null
  decoupled_min_lr: null
  exp_avg_dtype:
    _call_: false
    _target_: torch.float32
  exp_avg_sq_dtype:
    _call_: false
    _target_: torch.float32
  fp16: false
  fp8_recipe: null
  hysteresis: 2
  initial_loss_scale: 4294967296
  log_num_zeros_in_grad: false
  loss_scale: null
  loss_scale_window: 1000
  lr: 0.00025
  main_grads_dtype:
    _call_: false
    _target_: torch.float32
  main_params_dtype:
    _call_: false
    _target_: torch.float32
  min_loss_scale: 1.0
  min_lr: 2.5e-05
  optimizer: adam
  optimizer_cpu_offload: false
  optimizer_offload_fraction: 0.0
  overlap_cpu_optimizer_d2h_h2d: false
  overlap_param_gather: false
  overlap_param_gather_with_optimizer_step: false
  params_dtype:
    _call_: false
    _target_: torch.float32
  pin_cpu_grads: true
  pin_cpu_params: true
  reuse_grad_buf_for_mxfp8_param_ag: false
  sgd_momentum: 0.9
  store_param_remainders: true
  timers: null
  use_distributed_optimizer: true
  use_megatron_fsdp: false
  use_precision_aware_optimizer: false
  use_torch_optimizer_for_cpu_offload: false
  weight_decay: 0.1
peft: null
profiling:
  _target_: megatron.bridge.training.config.ProfilingConfig
  memory_snapshot_path: snapshot.pickle
  profile_ranks:
  - 0
  - 1
  profile_step_end: 10
  profile_step_start: 5
  record_memory_history: false
  record_shapes: true
  use_nsys_profiler: false
  use_pytorch_profiler: true
rerun_state_machine:
  _target_: megatron.bridge.training.config.RerunStateMachineConfig
  error_injection_rate: 0
  error_injection_type: transient_error
  rerun_mode: disabled
rng:
  _target_: megatron.bridge.training.config.RNGConfig
  data_parallel_random_init: false
  inference_rng_tracker: false
  seed: 42
  te_rng_tracker: false
scheduler:
  _target_: megatron.bridge.training.config.SchedulerConfig
  end_weight_decay: 0.033
  lr_decay_iters: 1168251
  lr_decay_steps: null
  lr_decay_style: cosine
  lr_warmup_fraction: null
  lr_warmup_init: 0.0
  lr_warmup_iters: 10
  lr_warmup_steps: null
  lr_wsd_decay_iters: null
  lr_wsd_decay_style: exponential
  override_opt_param_scheduler: true
  start_weight_decay: 0.033
  use_checkpoint_opt_param_scheduler: false
  wd_incr_steps: null
  weight_decay_incr_style: constant
  wsd_decay_steps: null
straggler: null
tokenizer:
  _target_: megatron.bridge.training.tokenizers.config.TokenizerConfig
  image_tag_type: null
  merge_file: null
  special_tokens: null
  tiktoken_num_special_tokens: 1000
  tiktoken_pattern: null
  tiktoken_special_tokens: null
  tokenizer_model: null
  tokenizer_prompt_format: null
  tokenizer_type: NullTokenizer
  vocab_extra_ids: 0
  vocab_file: null
  vocab_size: 32000
train:
  _target_: megatron.bridge.training.config.TrainingConfig
  check_weight_hash_across_dp_replicas_interval: null
  decrease_batch_size_if_needed: false
  empty_unused_memory_level: 0
  eval_interval: 2000
  eval_iters: 0
  exit_duration_in_mins: null
  exit_interval: null
  exit_signal:
    _args_:
    - 15
    _call_: true
    _target_: signal.Signals
  exit_signal_handler: false
  exit_signal_handler_for_dataloader: false
  global_batch_size: 8
  manual_gc: true
  manual_gc_eval: 100
  manual_gc_interval: 100
  micro_batch_size: 1
  rampup_batch_size: null
  skip_train: false
  train_iters: 20
  train_sync_interval: null

_target_: megatron.bridge.training.config.ConfigContainer
checkpoint:
  _target_: megatron.bridge.training.config.CheckpointConfig
  async_save: false
  ckpt_assume_constant_structure: false
  ckpt_convert_format: null
  ckpt_convert_save: null
  ckpt_format: torch_dist
  ckpt_step: null
  dist_ckpt_strictness: assume_ok_unexpected
  exit_on_missing_checkpoint: false
  finetune: false
  fully_parallel_load: false
  fully_parallel_save: true
  load: null
  load_main_params_from_ckpt: false
  load_optim: true
  load_rng: true
  non_persistent_ckpt_type: null
  non_persistent_global_ckpt_dir: null
  non_persistent_local_ckpt_algo: fully_parallel
  non_persistent_local_ckpt_dir: null
  non_persistent_save_interval: null
  pretrained_checkpoint: null
  replication: false
  replication_factor: 2
  replication_jump: null
  save: /opt/Megatron-Bridge/examples/recipes/llama/nemo_experiments/default/checkpoints
  save_interval: 2000
  save_optim: true
  save_rng: true
  use_checkpoint_args: false
  use_persistent_ckpt_worker: true
comm_overlap:
  _target_: megatron.bridge.training.comm_overlap.CommOverlapConfig
  align_param_gather: null
  batch_p2p_comm: null
  bucket_size: null
  data_parallel_size: null
  defer_embedding_wgrad_compute: null
  overlap_grad_reduce: null
  overlap_p2p_comm: null
  overlap_param_gather: null
  overlap_param_gather_with_optimizer_step: null
  tp_comm_bootstrap_backend: null
  tp_comm_overlap: false
  tp_comm_overlap_cfg: null
  wgrad_deferral_limit: null
dataset:
  _target_: megatron.bridge.training.config.GPTDatasetConfig
  add_extra_token_to_sequence: true
  blend: null
  blend_per_split: null
  create_attention_mask: true
  data_sharding: true
  dataloader_type: single
  drop_last_partial_validation_sequence: true
  eod_mask_loss: false
  full_validation: null
  mid_level_dataset_surplus: 0.005
  mmap_bin_files: true
  mock: true
  multiple_validation_sets: null
  num_dataset_builder_threads: 1
  num_workers: 8
  object_storage_cache_path: null
  path_to_cache: null
  persistent_workers: false
  pin_memory: true
  random_seed: 1234
  reset_attention_mask: false
  reset_position_ids: false
  sequence_length: 8192
  split: 1,1,1
  split_matrix:
  - - 0
    - 0.3333333333333333
  - - 0.3333333333333333
    - 0.6666666666666666
  - - 0.6666666666666666
    - 1.0
  tokenizer: null
ddp:
  _target_: megatron.core.distributed.distributed_data_parallel_config.DistributedDataParallelConfig
  align_param_gather: false
  average_in_collective: true
  bucket_size: null
  check_for_large_grads: false
  check_for_nan_in_grad: true
  data_parallel_sharding_strategy: no_shard
  fp8_param_gather: false
  fsdp_double_buffer: false
  grad_reduce_in_fp32: true
  gradient_reduce_div_fusion: true
  keep_fp8_transpose_cache: false
  nccl_ub: false
  num_distributed_optimizer_instances: 1
  outer_dp_sharding_strategy: no_shard
  overlap_grad_reduce: true
  overlap_param_gather: true
  pad_buckets_for_high_nccl_busbw: false
  preserve_fp32_weights: true
  reuse_grad_buf_for_mxfp8_param_ag: false
  suggested_communication_unit_size: null
  use_custom_fsdp: false
  use_distributed_optimizer: true
  use_megatron_fsdp: false
dist:
  _target_: megatron.bridge.training.config.DistributedInitConfig
  align_grad_reduce: true
  distributed_backend: nccl
  distributed_timeout_minutes: 10
  enable_megatron_core_experimental: false
  external_gpu_device_mapping: false
  high_priority_stream_groups: null
  lazy_init: false
  local_rank: 4
  nccl_communicator_config_path: null
  sharp_enabled_group: null
  use_gloo_process_groups: true
  use_sharp: false
  use_torch_fsdp2: false
  use_tp_pp_dp_mapping: false
ft: null
logger:
  _target_: megatron.bridge.training.config.LoggerConfig
  filter_warnings: true
  log_energy: false
  log_interval: 10
  log_loss_scale_to_tensorboard: true
  log_memory_to_tensorboard: false
  log_params_norm: false
  log_progress: false
  log_throughput: false
  log_timers_to_tensorboard: false
  log_validation_ppl_to_tensorboard: false
  log_world_size_to_tensorboard: false
  logging_level: 20
  modules_to_filter: null
  set_level_for_all_loggers: false
  tensorboard_dir: /opt/Megatron-Bridge/examples/recipes/llama/nemo_experiments/default/tb_logs
  tensorboard_log_interval: 1
  tensorboard_queue_size: 1000
  timing_log_level: 0
  timing_log_option: minmax
  wandb_entity: null
  wandb_exp_name: null
  wandb_project: null
  wandb_save_dir: null
mixed_precision: bf16_mixed
model:
  _target_: megatron.bridge.models.llama.llama_provider.Llama3ModelProvider8B
  account_for_embedding_in_pipeline_split: false
  account_for_loss_in_pipeline_split: false
  activation_func:
    _call_: false
    _target_: torch.nn.functional.silu
  activation_func_fp8_input_store: false
  add_bias_linear: false
  add_qkv_bias: false
  apply_query_key_layer_scaling: false
  apply_residual_connection_post_layernorm: false
  apply_rope_fusion: true
  async_tensor_model_parallel_allreduce: false
  attention_backend:
    _args_:
    - 5
    _call_: true
    _target_: megatron.core.transformer.enums.AttnBackend
  attention_dropout: 0.0
  attention_softmax_in_fp32: false
  autocast_dtype:
    _call_: false
    _target_: torch.float32
  barrier_with_L1_time: true
  batch_p2p_comm: true
  batch_p2p_sync: true
  bf16: false
  bias_activation_fusion: true
  bias_dropout_fusion: true
  calculate_per_token_loss: false
  clone_scatter_output_in_embedding: true
  config_logger_dir: ''
  context_parallel_size: 2
  cp_comm_type: null
  cpu_offloading: false
  cpu_offloading_activations: true
  cpu_offloading_num_layers: 0
  cpu_offloading_weights: true
  cross_entropy_fusion_impl: native
  cross_entropy_loss_fusion: true
  cuda_graph_retain_backward_graph: false
  cuda_graph_scope: full
  cuda_graph_use_single_mempool: false
  cuda_graph_warmup_steps: 3
  deallocate_pipeline_outputs: true
  defer_embedding_wgrad_compute: false
  delay_wgrad_compute: false
  deterministic_mode: false
  disable_bf16_reduced_precision_matmul: false
  disable_parameter_transpose_cache: false
  distribute_saved_activations: null
  embedding_init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.01
  embedding_init_method_std: 0.01
  enable_autocast: false
  enable_cuda_graph: false
  expert_model_parallel_size: 1
  expert_tensor_parallel_size: 1
  external_cuda_graph: false
  ffn_hidden_size: 14336
  finalize_model_grads_func: null
  first_last_layers_bf16: false
  flash_decode: false
  fp16: false
  fp16_lm_cross_entropy: false
  fp32_residual_connection: false
  fp8: null
  fp8_amax_compute_algo: most_recent
  fp8_amax_history_len: 1
  fp8_dot_product_attention: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_multi_head_attention: false
  fp8_param: false
  fp8_recipe: delayed
  fp8_wgrad: true
  gated_linear_unit: true
  generation_config: null
  grad_scale_func: null
  grad_sync_func: null
  gradient_accumulation_fusion: true
  hetereogenous_dist_checkpoint: false
  heterogeneous_block_specs: false
  hidden_dropout: 0.0
  hidden_size: 4096
  hierarchical_context_parallel_sizes: null
  inference_rng_tracker: false
  init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.01
  init_method_std: 0.01
  init_model_with_meta_device: false
  is_hybrid_model: false
  kv_channels: 128
  layernorm_epsilon: 1.0e-05
  layernorm_zero_centered_gamma: false
  make_vocab_size_divisible_by: 128
  mamba_head_dim: 64
  mamba_num_groups: 8
  mamba_num_heads: null
  mamba_state_dim: 128
  masked_softmax_fusion: true
  memory_efficient_layer_norm: false
  microbatch_group_size_per_vp_stage: 1
  mlp_chunks_for_prefill: 1
  moe_apply_probs_on_input: false
  moe_aux_loss_coeff: 0.0
  moe_deepep_num_sms: 20
  moe_enable_deepep: false
  moe_expert_capacity_factor: null
  moe_extended_tp: false
  moe_ffn_hidden_size: null
  moe_grouped_gemm: false
  moe_input_jitter_eps: null
  moe_layer_freq: 1
  moe_layer_recompute: false
  moe_pad_expert_input_to_capacity: false
  moe_per_layer_logging: false
  moe_permute_fusion: false
  moe_router_bias_update_rate: 0.001
  moe_router_dtype: null
  moe_router_enable_expert_bias: false
  moe_router_force_load_balancing: false
  moe_router_fusion: false
  moe_router_group_topk: null
  moe_router_load_balancing_type: aux_loss
  moe_router_num_groups: null
  moe_router_padding_for_fp8: false
  moe_router_pre_softmax: false
  moe_router_score_function: softmax
  moe_router_topk: 2
  moe_router_topk_limited_devices: null
  moe_router_topk_scaling_factor: null
  moe_shared_expert_intermediate_size: null
  moe_shared_expert_overlap: false
  moe_token_dispatcher_type: allgather
  moe_token_drop_policy: probs
  moe_token_dropping: false
  moe_use_legacy_grouped_gemm: false
  moe_z_loss_coeff: null
  mrope_section: null
  mtp_enabled: false
  mtp_loss_scaling_factor: null
  mtp_num_layers: null
  multi_latent_attention: false
  no_rope_freq: null
  no_sync_func: null
  normalization: RMSNorm
  num_attention_heads: 32
  num_layers: 32
  num_layers_at_end_in_bf16: 1
  num_layers_at_start_in_bf16: 1
  num_layers_in_first_pipeline_stage: null
  num_layers_in_last_pipeline_stage: null
  num_microbatches_with_partial_activation_checkpoints: null
  num_moe_experts: null
  num_query_groups: 8
  output_layer_init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.00125
  overlap_moe_expert_parallel_comm: false
  overlap_p2p_comm: false
  overlap_p2p_comm_warmup_flush: false
  parallel_output: true
  param_sync_func: null
  params_dtype:
    _call_: false
    _target_: torch.float32
  perform_initialization: true
  persist_layer_norm: false
  pipeline_dtype: null
  pipeline_model_parallel_comm_backend: null
  pipeline_model_parallel_layout: null
  pipeline_model_parallel_size: 1
  position_embedding_type: rope
  qk_layernorm: false
  quant_recipe: null
  recompute_granularity: null
  recompute_method: null
  recompute_modules:
  - core_attn
  recompute_num_layers: null
  rotary_base: 500000
  rotary_interleaved: false
  rotary_percent: 1.0
  scatter_embedding_sequence_parallel: true
  seq_len_interpolation_factor: null
  seq_length: 8192
  sequence_parallel: false
  share_embeddings_and_output_weights: false
  should_pad_vocab: false
  softmax_scale: null
  symmetric_ar_type: null
  tensor_model_parallel_size: 1
  test_mode: false
  timers: null
  tp_comm_atomic_ag: false
  tp_comm_atomic_rs: false
  tp_comm_bootstrap_backend: nccl
  tp_comm_bulk_dgrad: true
  tp_comm_bulk_wgrad: true
  tp_comm_overlap: false
  tp_comm_overlap_ag: true
  tp_comm_overlap_cfg: null
  tp_comm_overlap_disable_fc1: false
  tp_comm_overlap_disable_qkv: false
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_split_ag: true
  tp_comm_split_rs: true
  tp_only_amax_red: false
  transformer_impl: transformer_engine
  transformer_layer_spec:
    _call_: false
    _target_: megatron.bridge.models.gpt_provider.default_layer_spec
  use_cpu_initialization: false
  use_fused_weighted_squared_relu: false
  use_kitchen: false
  use_mamba_mem_eff_path: true
  use_ring_exchange_p2p: false
  use_te_activation_func: false
  use_te_rng_tracker: false
  use_transformer_engine_full_layer_spec: false
  use_transformer_engine_op_fuser: null
  variable_seq_lengths: false
  virtual_pipeline_model_parallel_size: null
  vocab_size: null
  wgrad_deferral_limit: 0
  window_size: null
nvrx_straggler: null
optimizer:
  _target_: megatron.core.optimizer.optimizer_config.OptimizerConfig
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_eps: 1.0e-05
  barrier_with_L1_time: false
  bf16: true
  clip_grad: 1.0
  config_logger_dir: ''
  decoupled_lr: null
  decoupled_min_lr: null
  exp_avg_dtype:
    _call_: false
    _target_: torch.float32
  exp_avg_sq_dtype:
    _call_: false
    _target_: torch.float32
  fp16: false
  fp8_recipe: null
  hysteresis: 2
  initial_loss_scale: 4294967296
  log_num_zeros_in_grad: false
  loss_scale: null
  loss_scale_window: 1000
  lr: 0.0003
  main_grads_dtype:
    _call_: false
    _target_: torch.float32
  main_params_dtype:
    _call_: false
    _target_: torch.float32
  min_loss_scale: 1.0
  min_lr: 3.0e-05
  optimizer: adam
  optimizer_cpu_offload: false
  optimizer_offload_fraction: 0.0
  overlap_cpu_optimizer_d2h_h2d: false
  overlap_param_gather: false
  overlap_param_gather_with_optimizer_step: false
  params_dtype:
    _call_: false
    _target_: torch.float32
  pin_cpu_grads: true
  pin_cpu_params: true
  reuse_grad_buf_for_mxfp8_param_ag: false
  sgd_momentum: 0.9
  store_param_remainders: true
  timers: null
  use_distributed_optimizer: true
  use_megatron_fsdp: false
  use_precision_aware_optimizer: false
  use_torch_optimizer_for_cpu_offload: false
  weight_decay: 0.1
peft: null
profiling: null
rerun_state_machine:
  _target_: megatron.bridge.training.config.RerunStateMachineConfig
  error_injection_rate: 0
  error_injection_type: transient_error
  rerun_mode: disabled
rng:
  _target_: megatron.bridge.training.config.RNGConfig
  data_parallel_random_init: false
  inference_rng_tracker: false
  seed: 1234
  te_rng_tracker: false
scheduler:
  _target_: megatron.bridge.training.config.SchedulerConfig
  end_weight_decay: 0.033
  lr_decay_iters: 1168251
  lr_decay_steps: null
  lr_decay_style: cosine
  lr_warmup_fraction: null
  lr_warmup_init: 0.0
  lr_warmup_iters: 2000
  lr_warmup_steps: null
  lr_wsd_decay_iters: null
  lr_wsd_decay_style: exponential
  override_opt_param_scheduler: true
  start_weight_decay: 0.033
  use_checkpoint_opt_param_scheduler: false
  wd_incr_steps: null
  weight_decay_incr_style: constant
  wsd_decay_steps: null
straggler: null
tokenizer:
  _target_: megatron.bridge.training.tokenizers.config.TokenizerConfig
  image_tag_type: null
  merge_file: null
  special_tokens: null
  tiktoken_num_special_tokens: 1000
  tiktoken_pattern: null
  tiktoken_special_tokens: null
  tokenizer_model: null
  tokenizer_prompt_format: null
  tokenizer_type: NullTokenizer
  vocab_extra_ids: 0
  vocab_file: null
  vocab_size: 32000
train:
  _target_: megatron.bridge.training.config.TrainingConfig
  check_weight_hash_across_dp_replicas_interval: null
  decrease_batch_size_if_needed: false
  empty_unused_memory_level: 0
  eval_interval: 2000
  eval_iters: 32
  exit_duration_in_mins: null
  exit_interval: null
  exit_signal:
    _args_:
    - 15
    _call_: true
    _target_: signal.Signals
  exit_signal_handler: false
  exit_signal_handler_for_dataloader: false
  global_batch_size: 512
  manual_gc: true
  manual_gc_eval: 100
  manual_gc_interval: 100
  micro_batch_size: 1
  rampup_batch_size: null
  skip_train: false
  train_iters: 1168251
  train_sync_interval: null

_target_: megatron.bridge.training.config.ConfigContainer
checkpoint:
  _target_: megatron.bridge.training.config.CheckpointConfig
  async_save: false
  ckpt_assume_constant_structure: false
  ckpt_convert_format: null
  ckpt_convert_save: null
  ckpt_format: torch_dist
  ckpt_step: null
  dist_ckpt_strictness: assume_ok_unexpected
  exit_on_missing_checkpoint: false
  finetune: false
  fully_parallel_load: false
  fully_parallel_save: true
  load: null
  load_main_params_from_ckpt: false
  load_optim: true
  load_rng: true
  non_persistent_ckpt_type: null
  non_persistent_global_ckpt_dir: null
  non_persistent_local_ckpt_algo: fully_parallel
  non_persistent_local_ckpt_dir: null
  non_persistent_save_interval: null
  pretrained_checkpoint: null
  replication: false
  replication_factor: 2
  replication_jump: null
  save: null
  save_interval: 2000
  save_optim: true
  save_rng: true
  use_checkpoint_args: false
  use_persistent_ckpt_worker: true
comm_overlap:
  _target_: megatron.bridge.training.comm_overlap.CommOverlapConfig
  align_param_gather: null
  batch_p2p_comm: null
  bucket_size: null
  data_parallel_size: null
  defer_embedding_wgrad_compute: null
  overlap_grad_reduce: null
  overlap_p2p_comm: null
  overlap_param_gather: null
  overlap_param_gather_with_optimizer_step: null
  tp_comm_bootstrap_backend: null
  tp_comm_overlap: false
  tp_comm_overlap_cfg: null
  wgrad_deferral_limit: null
dataset:
  _target_: megatron.bridge.training.config.GPTDatasetConfig
  add_extra_token_to_sequence: true
  blend: null
  blend_per_split: null
  create_attention_mask: true
  data_sharding: true
  dataloader_type: single
  drop_last_partial_validation_sequence: true
  eod_mask_loss: false
  full_validation: null
  mid_level_dataset_surplus: 0.005
  mmap_bin_files: true
  mock: true
  multiple_validation_sets: null
  num_dataset_builder_threads: 1
  num_workers: 8
  object_storage_cache_path: null
  path_to_cache: null
  persistent_workers: false
  pin_memory: true
  random_seed: 1234
  reset_attention_mask: false
  reset_position_ids: false
  sequence_length: 4096
  split: 1,1,1
  split_matrix:
  - - 0
    - 0.3333333333333333
  - - 0.3333333333333333
    - 0.6666666666666666
  - - 0.6666666666666666
    - 1.0
  tokenizer: null
ddp:
  _target_: megatron.core.distributed.distributed_data_parallel_config.DistributedDataParallelConfig
  align_param_gather: false
  average_in_collective: true
  bucket_size: null
  check_for_large_grads: false
  check_for_nan_in_grad: true
  data_parallel_sharding_strategy: no_shard
  fp8_param_gather: false
  fsdp_double_buffer: false
  grad_reduce_in_fp32: true
  gradient_reduce_div_fusion: true
  keep_fp8_transpose_cache: false
  nccl_ub: false
  num_distributed_optimizer_instances: 1
  outer_dp_sharding_strategy: no_shard
  overlap_grad_reduce: true
  overlap_param_gather: true
  pad_buckets_for_high_nccl_busbw: false
  preserve_fp32_weights: true
  reuse_grad_buf_for_mxfp8_param_ag: false
  suggested_communication_unit_size: null
  use_custom_fsdp: false
  use_distributed_optimizer: true
  use_megatron_fsdp: false
dist:
  _target_: megatron.bridge.training.config.DistributedInitConfig
  align_grad_reduce: true
  distributed_backend: nccl
  distributed_timeout_minutes: 10
  enable_megatron_core_experimental: false
  external_gpu_device_mapping: false
  high_priority_stream_groups: null
  lazy_init: false
  local_rank: 4
  nccl_communicator_config_path: null
  sharp_enabled_group: null
  use_gloo_process_groups: true
  use_sharp: false
  use_torch_fsdp2: false
  use_tp_pp_dp_mapping: false
ft: null
logger:
  _target_: megatron.bridge.training.config.LoggerConfig
  filter_warnings: true
  log_energy: false
  log_interval: 1
  log_loss_scale_to_tensorboard: true
  log_memory_to_tensorboard: false
  log_params_norm: false
  log_progress: false
  log_throughput: false
  log_timers_to_tensorboard: false
  log_validation_ppl_to_tensorboard: false
  log_world_size_to_tensorboard: false
  logging_level: 20
  modules_to_filter: null
  set_level_for_all_loggers: false
  tensorboard_dir: /opt/Megatron-Bridge/examples/recipes/llama/nemo_experiments/default/tb_logs
  tensorboard_log_interval: 1
  tensorboard_queue_size: 1000
  timing_log_level: 0
  timing_log_option: minmax
  wandb_entity: nvidia
  wandb_exp_name: llama3-8b-pretrain
  wandb_project: megatron-bridge
  wandb_save_dir: /lustre/fsw/portfolios/coreai/users/boxiangw/wandb/megatron-bridge/llama3-8b-pretrain
mixed_precision: bf16_mixed
model:
  _target_: megatron.bridge.models.llama.llama_provider.Llama3ModelProvider8B
  account_for_embedding_in_pipeline_split: false
  account_for_loss_in_pipeline_split: false
  activation_func:
    _call_: false
    _target_: torch.nn.functional.silu
  activation_func_fp8_input_store: false
  add_bias_linear: false
  add_qkv_bias: false
  apply_query_key_layer_scaling: false
  apply_residual_connection_post_layernorm: false
  apply_rope_fusion: true
  async_tensor_model_parallel_allreduce: false
  attention_backend:
    _args_:
    - 5
    _call_: true
    _target_: megatron.core.transformer.enums.AttnBackend
  attention_dropout: 0.0
  attention_softmax_in_fp32: false
  autocast_dtype:
    _call_: false
    _target_: torch.float32
  barrier_with_L1_time: true
  batch_p2p_comm: true
  batch_p2p_sync: true
  bf16: false
  bias_activation_fusion: true
  bias_dropout_fusion: true
  calculate_per_token_loss: false
  clone_scatter_output_in_embedding: true
  config_logger_dir: ''
  context_parallel_size: 2
  cp_comm_type: null
  cpu_offloading: false
  cpu_offloading_activations: true
  cpu_offloading_num_layers: 0
  cpu_offloading_weights: true
  cross_entropy_fusion_impl: native
  cross_entropy_loss_fusion: true
  cuda_graph_retain_backward_graph: false
  cuda_graph_scope: full
  cuda_graph_use_single_mempool: false
  cuda_graph_warmup_steps: 3
  deallocate_pipeline_outputs: true
  defer_embedding_wgrad_compute: false
  delay_wgrad_compute: false
  deterministic_mode: false
  disable_bf16_reduced_precision_matmul: false
  disable_parameter_transpose_cache: false
  distribute_saved_activations: null
  embedding_init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.01
  embedding_init_method_std: 0.01
  enable_autocast: false
  enable_cuda_graph: false
  expert_model_parallel_size: 1
  expert_tensor_parallel_size: 1
  external_cuda_graph: false
  ffn_hidden_size: 14336
  finalize_model_grads_func: null
  first_last_layers_bf16: false
  flash_decode: false
  fp16: false
  fp16_lm_cross_entropy: false
  fp32_residual_connection: false
  fp8: null
  fp8_amax_compute_algo: most_recent
  fp8_amax_history_len: 1
  fp8_dot_product_attention: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_multi_head_attention: false
  fp8_param: false
  fp8_recipe: delayed
  fp8_wgrad: true
  gated_linear_unit: true
  generation_config: null
  grad_scale_func: null
  grad_sync_func: null
  gradient_accumulation_fusion: true
  hetereogenous_dist_checkpoint: false
  heterogeneous_block_specs: false
  hidden_dropout: 0.0
  hidden_size: 4096
  hierarchical_context_parallel_sizes: null
  inference_rng_tracker: false
  init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.01
  init_method_std: 0.01
  init_model_with_meta_device: false
  is_hybrid_model: false
  kv_channels: 128
  layernorm_epsilon: 1.0e-05
  layernorm_zero_centered_gamma: false
  make_vocab_size_divisible_by: 128
  mamba_head_dim: 64
  mamba_num_groups: 8
  mamba_num_heads: null
  mamba_state_dim: 128
  masked_softmax_fusion: true
  memory_efficient_layer_norm: false
  microbatch_group_size_per_vp_stage: 1
  mlp_chunks_for_prefill: 1
  moe_apply_probs_on_input: false
  moe_aux_loss_coeff: 0.0
  moe_deepep_num_sms: 20
  moe_enable_deepep: false
  moe_expert_capacity_factor: null
  moe_extended_tp: false
  moe_ffn_hidden_size: null
  moe_grouped_gemm: false
  moe_input_jitter_eps: null
  moe_layer_freq: 1
  moe_layer_recompute: false
  moe_pad_expert_input_to_capacity: false
  moe_per_layer_logging: false
  moe_permute_fusion: false
  moe_router_bias_update_rate: 0.001
  moe_router_dtype: null
  moe_router_enable_expert_bias: false
  moe_router_force_load_balancing: false
  moe_router_fusion: false
  moe_router_group_topk: null
  moe_router_load_balancing_type: aux_loss
  moe_router_num_groups: null
  moe_router_padding_for_fp8: false
  moe_router_pre_softmax: false
  moe_router_score_function: softmax
  moe_router_topk: 2
  moe_router_topk_limited_devices: null
  moe_router_topk_scaling_factor: null
  moe_shared_expert_intermediate_size: null
  moe_shared_expert_overlap: false
  moe_token_dispatcher_type: allgather
  moe_token_drop_policy: probs
  moe_token_dropping: false
  moe_use_legacy_grouped_gemm: false
  moe_z_loss_coeff: null
  mrope_section: null
  mtp_enabled: false
  mtp_loss_scaling_factor: null
  mtp_num_layers: null
  multi_latent_attention: false
  no_rope_freq: null
  no_sync_func: null
  normalization: RMSNorm
  num_attention_heads: 32
  num_layers: 32
  num_layers_at_end_in_bf16: 1
  num_layers_at_start_in_bf16: 1
  num_layers_in_first_pipeline_stage: null
  num_layers_in_last_pipeline_stage: null
  num_microbatches_with_partial_activation_checkpoints: null
  num_moe_experts: null
  num_query_groups: 8
  output_layer_init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.00125
  overlap_moe_expert_parallel_comm: false
  overlap_p2p_comm: false
  overlap_p2p_comm_warmup_flush: false
  parallel_output: true
  param_sync_func: null
  params_dtype:
    _call_: false
    _target_: torch.float32
  perform_initialization: true
  persist_layer_norm: false
  pipeline_dtype: null
  pipeline_model_parallel_comm_backend: null
  pipeline_model_parallel_layout: null
  pipeline_model_parallel_size: 1
  position_embedding_type: rope
  qk_layernorm: false
  quant_recipe: null
  recompute_granularity: null
  recompute_method: null
  recompute_modules:
  - core_attn
  recompute_num_layers: null
  rotary_base: 500000
  rotary_interleaved: false
  rotary_percent: 1.0
  scatter_embedding_sequence_parallel: true
  seq_len_interpolation_factor: null
  seq_length: 4096
  sequence_parallel: false
  share_embeddings_and_output_weights: false
  should_pad_vocab: false
  softmax_scale: null
  symmetric_ar_type: null
  tensor_model_parallel_size: 1
  test_mode: false
  timers: null
  tp_comm_atomic_ag: false
  tp_comm_atomic_rs: false
  tp_comm_bootstrap_backend: nccl
  tp_comm_bulk_dgrad: true
  tp_comm_bulk_wgrad: true
  tp_comm_overlap: false
  tp_comm_overlap_ag: true
  tp_comm_overlap_cfg: null
  tp_comm_overlap_disable_fc1: false
  tp_comm_overlap_disable_qkv: false
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_split_ag: true
  tp_comm_split_rs: true
  tp_only_amax_red: false
  transformer_impl: transformer_engine
  transformer_layer_spec:
    _call_: false
    _target_: megatron.bridge.models.gpt_provider.default_layer_spec
  use_cpu_initialization: false
  use_fused_weighted_squared_relu: false
  use_kitchen: false
  use_mamba_mem_eff_path: true
  use_ring_exchange_p2p: false
  use_te_activation_func: false
  use_te_rng_tracker: false
  use_transformer_engine_full_layer_spec: false
  use_transformer_engine_op_fuser: null
  variable_seq_lengths: false
  virtual_pipeline_model_parallel_size: null
  vocab_size: null
  wgrad_deferral_limit: 0
  window_size: null
nvrx_straggler: null
optimizer:
  _target_: megatron.core.optimizer.optimizer_config.OptimizerConfig
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_eps: 1.0e-05
  barrier_with_L1_time: false
  bf16: true
  clip_grad: 1.0
  config_logger_dir: ''
  decoupled_lr: null
  decoupled_min_lr: null
  exp_avg_dtype:
    _call_: false
    _target_: torch.float32
  exp_avg_sq_dtype:
    _call_: false
    _target_: torch.float32
  fp16: false
  fp8_recipe: null
  hysteresis: 2
  initial_loss_scale: 4294967296
  log_num_zeros_in_grad: false
  loss_scale: null
  loss_scale_window: 1000
  lr: 0.00025
  main_grads_dtype:
    _call_: false
    _target_: torch.float32
  main_params_dtype:
    _call_: false
    _target_: torch.float32
  min_loss_scale: 1.0
  min_lr: 2.5e-05
  optimizer: adam
  optimizer_cpu_offload: false
  optimizer_offload_fraction: 0.0
  overlap_cpu_optimizer_d2h_h2d: false
  overlap_param_gather: false
  overlap_param_gather_with_optimizer_step: false
  params_dtype:
    _call_: false
    _target_: torch.float32
  pin_cpu_grads: true
  pin_cpu_params: true
  reuse_grad_buf_for_mxfp8_param_ag: false
  sgd_momentum: 0.9
  store_param_remainders: true
  timers: null
  use_distributed_optimizer: true
  use_megatron_fsdp: false
  use_precision_aware_optimizer: false
  use_torch_optimizer_for_cpu_offload: false
  weight_decay: 0.1
peft: null
profiling:
  _target_: megatron.bridge.training.config.ProfilingConfig
  memory_snapshot_path: snapshot.pickle
  profile_ranks:
  - 0
  - 1
  profile_step_end: 10
  profile_step_start: 5
  record_memory_history: false
  record_shapes: true
  use_nsys_profiler: false
  use_pytorch_profiler: true
rerun_state_machine:
  _target_: megatron.bridge.training.config.RerunStateMachineConfig
  error_injection_rate: 0
  error_injection_type: transient_error
  rerun_mode: disabled
rng:
  _target_: megatron.bridge.training.config.RNGConfig
  data_parallel_random_init: false
  inference_rng_tracker: false
  seed: 42
  te_rng_tracker: false
scheduler:
  _target_: megatron.bridge.training.config.SchedulerConfig
  end_weight_decay: 0.033
  lr_decay_iters: 1168251
  lr_decay_steps: null
  lr_decay_style: cosine
  lr_warmup_fraction: null
  lr_warmup_init: 0.0
  lr_warmup_iters: 10
  lr_warmup_steps: null
  lr_wsd_decay_iters: null
  lr_wsd_decay_style: exponential
  override_opt_param_scheduler: true
  start_weight_decay: 0.033
  use_checkpoint_opt_param_scheduler: false
  wd_incr_steps: null
  weight_decay_incr_style: constant
  wsd_decay_steps: null
straggler: null
tokenizer:
  _target_: megatron.bridge.training.tokenizers.config.TokenizerConfig
  image_tag_type: null
  merge_file: null
  special_tokens: null
  tiktoken_num_special_tokens: 1000
  tiktoken_pattern: null
  tiktoken_special_tokens: null
  tokenizer_model: null
  tokenizer_prompt_format: null
  tokenizer_type: NullTokenizer
  vocab_extra_ids: 0
  vocab_file: null
  vocab_size: 32000
train:
  _target_: megatron.bridge.training.config.TrainingConfig
  check_weight_hash_across_dp_replicas_interval: null
  decrease_batch_size_if_needed: false
  empty_unused_memory_level: 0
  eval_interval: 2000
  eval_iters: 0
  exit_duration_in_mins: null
  exit_interval: null
  exit_signal:
    _args_:
    - 15
    _call_: true
    _target_: signal.Signals
  exit_signal_handler: false
  exit_signal_handler_for_dataloader: false
  global_batch_size: 8
  manual_gc: true
  manual_gc_eval: 100
  manual_gc_interval: 100
  micro_batch_size: 1
  rampup_batch_size: null
  skip_train: false
  train_iters: 20
  train_sync_interval: null

_target_: megatron.bridge.training.config.ConfigContainer
checkpoint:
  _target_: megatron.bridge.training.config.CheckpointConfig
  async_save: false
  ckpt_assume_constant_structure: false
  ckpt_convert_format: null
  ckpt_convert_save: null
  ckpt_format: torch_dist
  ckpt_step: null
  dist_ckpt_strictness: assume_ok_unexpected
  exit_on_missing_checkpoint: false
  finetune: false
  fully_parallel_load: false
  fully_parallel_save: true
  load: null
  load_main_params_from_ckpt: false
  load_optim: true
  load_rng: true
  non_persistent_ckpt_type: null
  non_persistent_global_ckpt_dir: null
  non_persistent_local_ckpt_algo: fully_parallel
  non_persistent_local_ckpt_dir: null
  non_persistent_save_interval: null
  pretrained_checkpoint: null
  replication: false
  replication_factor: 2
  replication_jump: null
  save: /opt/Megatron-Bridge/examples/recipes/llama/nemo_experiments/default/checkpoints
  save_interval: 2000
  save_optim: true
  save_rng: true
  use_checkpoint_args: false
  use_persistent_ckpt_worker: true
comm_overlap:
  _target_: megatron.bridge.training.comm_overlap.CommOverlapConfig
  align_param_gather: null
  batch_p2p_comm: null
  bucket_size: null
  data_parallel_size: null
  defer_embedding_wgrad_compute: null
  overlap_grad_reduce: null
  overlap_p2p_comm: null
  overlap_param_gather: null
  overlap_param_gather_with_optimizer_step: null
  tp_comm_bootstrap_backend: null
  tp_comm_overlap: false
  tp_comm_overlap_cfg: null
  wgrad_deferral_limit: null
dataset:
  _target_: megatron.bridge.training.config.GPTDatasetConfig
  add_extra_token_to_sequence: true
  blend: null
  blend_per_split: null
  create_attention_mask: true
  data_sharding: true
  dataloader_type: single
  drop_last_partial_validation_sequence: true
  eod_mask_loss: false
  full_validation: null
  mid_level_dataset_surplus: 0.005
  mmap_bin_files: true
  mock: true
  multiple_validation_sets: null
  num_dataset_builder_threads: 1
  num_workers: 8
  object_storage_cache_path: null
  path_to_cache: null
  persistent_workers: false
  pin_memory: true
  random_seed: 1234
  reset_attention_mask: false
  reset_position_ids: false
  sequence_length: 8192
  split: 1,1,1
  split_matrix:
  - - 0
    - 0.3333333333333333
  - - 0.3333333333333333
    - 0.6666666666666666
  - - 0.6666666666666666
    - 1.0
  tokenizer: null
ddp:
  _target_: megatron.core.distributed.distributed_data_parallel_config.DistributedDataParallelConfig
  align_param_gather: false
  average_in_collective: true
  bucket_size: null
  check_for_large_grads: false
  check_for_nan_in_grad: true
  data_parallel_sharding_strategy: no_shard
  fp8_param_gather: false
  fsdp_double_buffer: false
  grad_reduce_in_fp32: true
  gradient_reduce_div_fusion: true
  keep_fp8_transpose_cache: false
  nccl_ub: false
  num_distributed_optimizer_instances: 1
  outer_dp_sharding_strategy: no_shard
  overlap_grad_reduce: true
  overlap_param_gather: true
  pad_buckets_for_high_nccl_busbw: false
  preserve_fp32_weights: true
  reuse_grad_buf_for_mxfp8_param_ag: false
  suggested_communication_unit_size: null
  use_custom_fsdp: false
  use_distributed_optimizer: true
  use_megatron_fsdp: false
dist:
  _target_: megatron.bridge.training.config.DistributedInitConfig
  align_grad_reduce: true
  distributed_backend: nccl
  distributed_timeout_minutes: 10
  enable_megatron_core_experimental: false
  external_gpu_device_mapping: false
  high_priority_stream_groups: null
  lazy_init: false
  local_rank: 1
  nccl_communicator_config_path: null
  sharp_enabled_group: null
  use_gloo_process_groups: true
  use_sharp: false
  use_torch_fsdp2: false
  use_tp_pp_dp_mapping: false
ft: null
logger:
  _target_: megatron.bridge.training.config.LoggerConfig
  filter_warnings: true
  log_energy: false
  log_interval: 10
  log_loss_scale_to_tensorboard: true
  log_memory_to_tensorboard: false
  log_params_norm: false
  log_progress: false
  log_throughput: false
  log_timers_to_tensorboard: false
  log_validation_ppl_to_tensorboard: false
  log_world_size_to_tensorboard: false
  logging_level: 20
  modules_to_filter: null
  set_level_for_all_loggers: false
  tensorboard_dir: /opt/Megatron-Bridge/examples/recipes/llama/nemo_experiments/default/tb_logs
  tensorboard_log_interval: 1
  tensorboard_queue_size: 1000
  timing_log_level: 0
  timing_log_option: minmax
  wandb_entity: null
  wandb_exp_name: null
  wandb_project: null
  wandb_save_dir: null
mixed_precision: bf16_mixed
model:
  _target_: megatron.bridge.models.llama.llama_provider.Llama3ModelProvider8B
  account_for_embedding_in_pipeline_split: false
  account_for_loss_in_pipeline_split: false
  activation_func:
    _call_: false
    _target_: torch.nn.functional.silu
  activation_func_fp8_input_store: false
  add_bias_linear: false
  add_qkv_bias: false
  apply_query_key_layer_scaling: false
  apply_residual_connection_post_layernorm: false
  apply_rope_fusion: true
  async_tensor_model_parallel_allreduce: false
  attention_backend:
    _args_:
    - 5
    _call_: true
    _target_: megatron.core.transformer.enums.AttnBackend
  attention_dropout: 0.0
  attention_softmax_in_fp32: false
  autocast_dtype:
    _call_: false
    _target_: torch.float32
  barrier_with_L1_time: true
  batch_p2p_comm: true
  batch_p2p_sync: true
  bf16: false
  bias_activation_fusion: true
  bias_dropout_fusion: true
  calculate_per_token_loss: false
  clone_scatter_output_in_embedding: true
  config_logger_dir: ''
  context_parallel_size: 2
  cp_comm_type: null
  cpu_offloading: false
  cpu_offloading_activations: true
  cpu_offloading_num_layers: 0
  cpu_offloading_weights: true
  cross_entropy_fusion_impl: native
  cross_entropy_loss_fusion: true
  cuda_graph_retain_backward_graph: false
  cuda_graph_scope: full
  cuda_graph_use_single_mempool: false
  cuda_graph_warmup_steps: 3
  deallocate_pipeline_outputs: true
  defer_embedding_wgrad_compute: false
  delay_wgrad_compute: false
  deterministic_mode: false
  disable_bf16_reduced_precision_matmul: false
  disable_parameter_transpose_cache: false
  distribute_saved_activations: null
  embedding_init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.01
  embedding_init_method_std: 0.01
  enable_autocast: false
  enable_cuda_graph: false
  expert_model_parallel_size: 1
  expert_tensor_parallel_size: 1
  external_cuda_graph: false
  ffn_hidden_size: 14336
  finalize_model_grads_func: null
  first_last_layers_bf16: false
  flash_decode: false
  fp16: false
  fp16_lm_cross_entropy: false
  fp32_residual_connection: false
  fp8: null
  fp8_amax_compute_algo: most_recent
  fp8_amax_history_len: 1
  fp8_dot_product_attention: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_multi_head_attention: false
  fp8_param: false
  fp8_recipe: delayed
  fp8_wgrad: true
  gated_linear_unit: true
  generation_config: null
  grad_scale_func: null
  grad_sync_func: null
  gradient_accumulation_fusion: true
  hetereogenous_dist_checkpoint: false
  heterogeneous_block_specs: false
  hidden_dropout: 0.0
  hidden_size: 4096
  hierarchical_context_parallel_sizes: null
  inference_rng_tracker: false
  init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.01
  init_method_std: 0.01
  init_model_with_meta_device: false
  is_hybrid_model: false
  kv_channels: 128
  layernorm_epsilon: 1.0e-05
  layernorm_zero_centered_gamma: false
  make_vocab_size_divisible_by: 128
  mamba_head_dim: 64
  mamba_num_groups: 8
  mamba_num_heads: null
  mamba_state_dim: 128
  masked_softmax_fusion: true
  memory_efficient_layer_norm: false
  microbatch_group_size_per_vp_stage: 1
  mlp_chunks_for_prefill: 1
  moe_apply_probs_on_input: false
  moe_aux_loss_coeff: 0.0
  moe_deepep_num_sms: 20
  moe_enable_deepep: false
  moe_expert_capacity_factor: null
  moe_extended_tp: false
  moe_ffn_hidden_size: null
  moe_grouped_gemm: false
  moe_input_jitter_eps: null
  moe_layer_freq: 1
  moe_layer_recompute: false
  moe_pad_expert_input_to_capacity: false
  moe_per_layer_logging: false
  moe_permute_fusion: false
  moe_router_bias_update_rate: 0.001
  moe_router_dtype: null
  moe_router_enable_expert_bias: false
  moe_router_force_load_balancing: false
  moe_router_fusion: false
  moe_router_group_topk: null
  moe_router_load_balancing_type: aux_loss
  moe_router_num_groups: null
  moe_router_padding_for_fp8: false
  moe_router_pre_softmax: false
  moe_router_score_function: softmax
  moe_router_topk: 2
  moe_router_topk_limited_devices: null
  moe_router_topk_scaling_factor: null
  moe_shared_expert_intermediate_size: null
  moe_shared_expert_overlap: false
  moe_token_dispatcher_type: allgather
  moe_token_drop_policy: probs
  moe_token_dropping: false
  moe_use_legacy_grouped_gemm: false
  moe_z_loss_coeff: null
  mrope_section: null
  mtp_enabled: false
  mtp_loss_scaling_factor: null
  mtp_num_layers: null
  multi_latent_attention: false
  no_rope_freq: null
  no_sync_func: null
  normalization: RMSNorm
  num_attention_heads: 32
  num_layers: 32
  num_layers_at_end_in_bf16: 1
  num_layers_at_start_in_bf16: 1
  num_layers_in_first_pipeline_stage: null
  num_layers_in_last_pipeline_stage: null
  num_microbatches_with_partial_activation_checkpoints: null
  num_moe_experts: null
  num_query_groups: 8
  output_layer_init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.00125
  overlap_moe_expert_parallel_comm: false
  overlap_p2p_comm: false
  overlap_p2p_comm_warmup_flush: false
  parallel_output: true
  param_sync_func: null
  params_dtype:
    _call_: false
    _target_: torch.float32
  perform_initialization: true
  persist_layer_norm: false
  pipeline_dtype: null
  pipeline_model_parallel_comm_backend: null
  pipeline_model_parallel_layout: null
  pipeline_model_parallel_size: 1
  position_embedding_type: rope
  qk_layernorm: false
  quant_recipe: null
  recompute_granularity: null
  recompute_method: null
  recompute_modules:
  - core_attn
  recompute_num_layers: null
  rotary_base: 500000
  rotary_interleaved: false
  rotary_percent: 1.0
  scatter_embedding_sequence_parallel: true
  seq_len_interpolation_factor: null
  seq_length: 8192
  sequence_parallel: false
  share_embeddings_and_output_weights: false
  should_pad_vocab: false
  softmax_scale: null
  symmetric_ar_type: null
  tensor_model_parallel_size: 1
  test_mode: false
  timers: null
  tp_comm_atomic_ag: false
  tp_comm_atomic_rs: false
  tp_comm_bootstrap_backend: nccl
  tp_comm_bulk_dgrad: true
  tp_comm_bulk_wgrad: true
  tp_comm_overlap: false
  tp_comm_overlap_ag: true
  tp_comm_overlap_cfg: null
  tp_comm_overlap_disable_fc1: false
  tp_comm_overlap_disable_qkv: false
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_split_ag: true
  tp_comm_split_rs: true
  tp_only_amax_red: false
  transformer_impl: transformer_engine
  transformer_layer_spec:
    _call_: false
    _target_: megatron.bridge.models.gpt_provider.default_layer_spec
  use_cpu_initialization: false
  use_fused_weighted_squared_relu: false
  use_kitchen: false
  use_mamba_mem_eff_path: true
  use_ring_exchange_p2p: false
  use_te_activation_func: false
  use_te_rng_tracker: false
  use_transformer_engine_full_layer_spec: false
  use_transformer_engine_op_fuser: null
  variable_seq_lengths: false
  virtual_pipeline_model_parallel_size: null
  vocab_size: null
  wgrad_deferral_limit: 0
  window_size: null
nvrx_straggler: null
optimizer:
  _target_: megatron.core.optimizer.optimizer_config.OptimizerConfig
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_eps: 1.0e-05
  barrier_with_L1_time: false
  bf16: true
  clip_grad: 1.0
  config_logger_dir: ''
  decoupled_lr: null
  decoupled_min_lr: null
  exp_avg_dtype:
    _call_: false
    _target_: torch.float32
  exp_avg_sq_dtype:
    _call_: false
    _target_: torch.float32
  fp16: false
  fp8_recipe: null
  hysteresis: 2
  initial_loss_scale: 4294967296
  log_num_zeros_in_grad: false
  loss_scale: null
  loss_scale_window: 1000
  lr: 0.0003
  main_grads_dtype:
    _call_: false
    _target_: torch.float32
  main_params_dtype:
    _call_: false
    _target_: torch.float32
  min_loss_scale: 1.0
  min_lr: 3.0e-05
  optimizer: adam
  optimizer_cpu_offload: false
  optimizer_offload_fraction: 0.0
  overlap_cpu_optimizer_d2h_h2d: false
  overlap_param_gather: false
  overlap_param_gather_with_optimizer_step: false
  params_dtype:
    _call_: false
    _target_: torch.float32
  pin_cpu_grads: true
  pin_cpu_params: true
  reuse_grad_buf_for_mxfp8_param_ag: false
  sgd_momentum: 0.9
  store_param_remainders: true
  timers: null
  use_distributed_optimizer: true
  use_megatron_fsdp: false
  use_precision_aware_optimizer: false
  use_torch_optimizer_for_cpu_offload: false
  weight_decay: 0.1
peft: null
profiling: null
rerun_state_machine:
  _target_: megatron.bridge.training.config.RerunStateMachineConfig
  error_injection_rate: 0
  error_injection_type: transient_error
  rerun_mode: disabled
rng:
  _target_: megatron.bridge.training.config.RNGConfig
  data_parallel_random_init: false
  inference_rng_tracker: false
  seed: 1234
  te_rng_tracker: false
scheduler:
  _target_: megatron.bridge.training.config.SchedulerConfig
  end_weight_decay: 0.033
  lr_decay_iters: 1168251
  lr_decay_steps: null
  lr_decay_style: cosine
  lr_warmup_fraction: null
  lr_warmup_init: 0.0
  lr_warmup_iters: 2000
  lr_warmup_steps: null
  lr_wsd_decay_iters: null
  lr_wsd_decay_style: exponential
  override_opt_param_scheduler: true
  start_weight_decay: 0.033
  use_checkpoint_opt_param_scheduler: false
  wd_incr_steps: null
  weight_decay_incr_style: constant
  wsd_decay_steps: null
straggler: null
tokenizer:
  _target_: megatron.bridge.training.tokenizers.config.TokenizerConfig
  image_tag_type: null
  merge_file: null
  special_tokens: null
  tiktoken_num_special_tokens: 1000
  tiktoken_pattern: null
  tiktoken_special_tokens: null
  tokenizer_model: null
  tokenizer_prompt_format: null
  tokenizer_type: NullTokenizer
  vocab_extra_ids: 0
  vocab_file: null
  vocab_size: 32000
train:
  _target_: megatron.bridge.training.config.TrainingConfig
  check_weight_hash_across_dp_replicas_interval: null
  decrease_batch_size_if_needed: false
  empty_unused_memory_level: 0
  eval_interval: 2000
  eval_iters: 32
  exit_duration_in_mins: null
  exit_interval: null
  exit_signal:
    _args_:
    - 15
    _call_: true
    _target_: signal.Signals
  exit_signal_handler: false
  exit_signal_handler_for_dataloader: false
  global_batch_size: 512
  manual_gc: true
  manual_gc_eval: 100
  manual_gc_interval: 100
  micro_batch_size: 1
  rampup_batch_size: null
  skip_train: false
  train_iters: 1168251
  train_sync_interval: null

_target_: megatron.bridge.training.config.ConfigContainer
checkpoint:
  _target_: megatron.bridge.training.config.CheckpointConfig
  async_save: false
  ckpt_assume_constant_structure: false
  ckpt_convert_format: null
  ckpt_convert_save: null
  ckpt_format: torch_dist
  ckpt_step: null
  dist_ckpt_strictness: assume_ok_unexpected
  exit_on_missing_checkpoint: false
  finetune: false
  fully_parallel_load: false
  fully_parallel_save: true
  load: null
  load_main_params_from_ckpt: false
  load_optim: true
  load_rng: true
  non_persistent_ckpt_type: null
  non_persistent_global_ckpt_dir: null
  non_persistent_local_ckpt_algo: fully_parallel
  non_persistent_local_ckpt_dir: null
  non_persistent_save_interval: null
  pretrained_checkpoint: null
  replication: false
  replication_factor: 2
  replication_jump: null
  save: null
  save_interval: 2000
  save_optim: true
  save_rng: true
  use_checkpoint_args: false
  use_persistent_ckpt_worker: true
comm_overlap:
  _target_: megatron.bridge.training.comm_overlap.CommOverlapConfig
  align_param_gather: null
  batch_p2p_comm: null
  bucket_size: null
  data_parallel_size: null
  defer_embedding_wgrad_compute: null
  overlap_grad_reduce: null
  overlap_p2p_comm: null
  overlap_param_gather: null
  overlap_param_gather_with_optimizer_step: null
  tp_comm_bootstrap_backend: null
  tp_comm_overlap: false
  tp_comm_overlap_cfg: null
  wgrad_deferral_limit: null
dataset:
  _target_: megatron.bridge.training.config.GPTDatasetConfig
  add_extra_token_to_sequence: true
  blend: null
  blend_per_split: null
  create_attention_mask: true
  data_sharding: true
  dataloader_type: single
  drop_last_partial_validation_sequence: true
  eod_mask_loss: false
  full_validation: null
  mid_level_dataset_surplus: 0.005
  mmap_bin_files: true
  mock: true
  multiple_validation_sets: null
  num_dataset_builder_threads: 1
  num_workers: 8
  object_storage_cache_path: null
  path_to_cache: null
  persistent_workers: false
  pin_memory: true
  random_seed: 1234
  reset_attention_mask: false
  reset_position_ids: false
  sequence_length: 4096
  split: 1,1,1
  split_matrix:
  - - 0
    - 0.3333333333333333
  - - 0.3333333333333333
    - 0.6666666666666666
  - - 0.6666666666666666
    - 1.0
  tokenizer: null
ddp:
  _target_: megatron.core.distributed.distributed_data_parallel_config.DistributedDataParallelConfig
  align_param_gather: false
  average_in_collective: true
  bucket_size: null
  check_for_large_grads: false
  check_for_nan_in_grad: true
  data_parallel_sharding_strategy: no_shard
  fp8_param_gather: false
  fsdp_double_buffer: false
  grad_reduce_in_fp32: true
  gradient_reduce_div_fusion: true
  keep_fp8_transpose_cache: false
  nccl_ub: false
  num_distributed_optimizer_instances: 1
  outer_dp_sharding_strategy: no_shard
  overlap_grad_reduce: true
  overlap_param_gather: true
  pad_buckets_for_high_nccl_busbw: false
  preserve_fp32_weights: true
  reuse_grad_buf_for_mxfp8_param_ag: false
  suggested_communication_unit_size: null
  use_custom_fsdp: false
  use_distributed_optimizer: true
  use_megatron_fsdp: false
dist:
  _target_: megatron.bridge.training.config.DistributedInitConfig
  align_grad_reduce: true
  distributed_backend: nccl
  distributed_timeout_minutes: 10
  enable_megatron_core_experimental: false
  external_gpu_device_mapping: false
  high_priority_stream_groups: null
  lazy_init: false
  local_rank: 1
  nccl_communicator_config_path: null
  sharp_enabled_group: null
  use_gloo_process_groups: true
  use_sharp: false
  use_torch_fsdp2: false
  use_tp_pp_dp_mapping: false
ft: null
logger:
  _target_: megatron.bridge.training.config.LoggerConfig
  filter_warnings: true
  log_energy: false
  log_interval: 1
  log_loss_scale_to_tensorboard: true
  log_memory_to_tensorboard: false
  log_params_norm: false
  log_progress: false
  log_throughput: false
  log_timers_to_tensorboard: false
  log_validation_ppl_to_tensorboard: false
  log_world_size_to_tensorboard: false
  logging_level: 20
  modules_to_filter: null
  set_level_for_all_loggers: false
  tensorboard_dir: /opt/Megatron-Bridge/examples/recipes/llama/nemo_experiments/default/tb_logs
  tensorboard_log_interval: 1
  tensorboard_queue_size: 1000
  timing_log_level: 0
  timing_log_option: minmax
  wandb_entity: nvidia
  wandb_exp_name: llama3-8b-pretrain
  wandb_project: megatron-bridge
  wandb_save_dir: /lustre/fsw/portfolios/coreai/users/boxiangw/wandb/megatron-bridge/llama3-8b-pretrain
mixed_precision: bf16_mixed
model:
  _target_: megatron.bridge.models.llama.llama_provider.Llama3ModelProvider8B
  account_for_embedding_in_pipeline_split: false
  account_for_loss_in_pipeline_split: false
  activation_func:
    _call_: false
    _target_: torch.nn.functional.silu
  activation_func_fp8_input_store: false
  add_bias_linear: false
  add_qkv_bias: false
  apply_query_key_layer_scaling: false
  apply_residual_connection_post_layernorm: false
  apply_rope_fusion: true
  async_tensor_model_parallel_allreduce: false
  attention_backend:
    _args_:
    - 5
    _call_: true
    _target_: megatron.core.transformer.enums.AttnBackend
  attention_dropout: 0.0
  attention_softmax_in_fp32: false
  autocast_dtype:
    _call_: false
    _target_: torch.float32
  barrier_with_L1_time: true
  batch_p2p_comm: true
  batch_p2p_sync: true
  bf16: false
  bias_activation_fusion: true
  bias_dropout_fusion: true
  calculate_per_token_loss: false
  clone_scatter_output_in_embedding: true
  config_logger_dir: ''
  context_parallel_size: 2
  cp_comm_type: null
  cpu_offloading: false
  cpu_offloading_activations: true
  cpu_offloading_num_layers: 0
  cpu_offloading_weights: true
  cross_entropy_fusion_impl: native
  cross_entropy_loss_fusion: true
  cuda_graph_retain_backward_graph: false
  cuda_graph_scope: full
  cuda_graph_use_single_mempool: false
  cuda_graph_warmup_steps: 3
  deallocate_pipeline_outputs: true
  defer_embedding_wgrad_compute: false
  delay_wgrad_compute: false
  deterministic_mode: false
  disable_bf16_reduced_precision_matmul: false
  disable_parameter_transpose_cache: false
  distribute_saved_activations: null
  embedding_init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.01
  embedding_init_method_std: 0.01
  enable_autocast: false
  enable_cuda_graph: false
  expert_model_parallel_size: 1
  expert_tensor_parallel_size: 1
  external_cuda_graph: false
  ffn_hidden_size: 14336
  finalize_model_grads_func: null
  first_last_layers_bf16: false
  flash_decode: false
  fp16: false
  fp16_lm_cross_entropy: false
  fp32_residual_connection: false
  fp8: null
  fp8_amax_compute_algo: most_recent
  fp8_amax_history_len: 1
  fp8_dot_product_attention: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_multi_head_attention: false
  fp8_param: false
  fp8_recipe: delayed
  fp8_wgrad: true
  gated_linear_unit: true
  generation_config: null
  grad_scale_func: null
  grad_sync_func: null
  gradient_accumulation_fusion: true
  hetereogenous_dist_checkpoint: false
  heterogeneous_block_specs: false
  hidden_dropout: 0.0
  hidden_size: 4096
  hierarchical_context_parallel_sizes: null
  inference_rng_tracker: false
  init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.01
  init_method_std: 0.01
  init_model_with_meta_device: false
  is_hybrid_model: false
  kv_channels: 128
  layernorm_epsilon: 1.0e-05
  layernorm_zero_centered_gamma: false
  make_vocab_size_divisible_by: 128
  mamba_head_dim: 64
  mamba_num_groups: 8
  mamba_num_heads: null
  mamba_state_dim: 128
  masked_softmax_fusion: true
  memory_efficient_layer_norm: false
  microbatch_group_size_per_vp_stage: 1
  mlp_chunks_for_prefill: 1
  moe_apply_probs_on_input: false
  moe_aux_loss_coeff: 0.0
  moe_deepep_num_sms: 20
  moe_enable_deepep: false
  moe_expert_capacity_factor: null
  moe_extended_tp: false
  moe_ffn_hidden_size: null
  moe_grouped_gemm: false
  moe_input_jitter_eps: null
  moe_layer_freq: 1
  moe_layer_recompute: false
  moe_pad_expert_input_to_capacity: false
  moe_per_layer_logging: false
  moe_permute_fusion: false
  moe_router_bias_update_rate: 0.001
  moe_router_dtype: null
  moe_router_enable_expert_bias: false
  moe_router_force_load_balancing: false
  moe_router_fusion: false
  moe_router_group_topk: null
  moe_router_load_balancing_type: aux_loss
  moe_router_num_groups: null
  moe_router_padding_for_fp8: false
  moe_router_pre_softmax: false
  moe_router_score_function: softmax
  moe_router_topk: 2
  moe_router_topk_limited_devices: null
  moe_router_topk_scaling_factor: null
  moe_shared_expert_intermediate_size: null
  moe_shared_expert_overlap: false
  moe_token_dispatcher_type: allgather
  moe_token_drop_policy: probs
  moe_token_dropping: false
  moe_use_legacy_grouped_gemm: false
  moe_z_loss_coeff: null
  mrope_section: null
  mtp_enabled: false
  mtp_loss_scaling_factor: null
  mtp_num_layers: null
  multi_latent_attention: false
  no_rope_freq: null
  no_sync_func: null
  normalization: RMSNorm
  num_attention_heads: 32
  num_layers: 32
  num_layers_at_end_in_bf16: 1
  num_layers_at_start_in_bf16: 1
  num_layers_in_first_pipeline_stage: null
  num_layers_in_last_pipeline_stage: null
  num_microbatches_with_partial_activation_checkpoints: null
  num_moe_experts: null
  num_query_groups: 8
  output_layer_init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.00125
  overlap_moe_expert_parallel_comm: false
  overlap_p2p_comm: false
  overlap_p2p_comm_warmup_flush: false
  parallel_output: true
  param_sync_func: null
  params_dtype:
    _call_: false
    _target_: torch.float32
  perform_initialization: true
  persist_layer_norm: false
  pipeline_dtype: null
  pipeline_model_parallel_comm_backend: null
  pipeline_model_parallel_layout: null
  pipeline_model_parallel_size: 1
  position_embedding_type: rope
  qk_layernorm: false
  quant_recipe: null
  recompute_granularity: null
  recompute_method: null
  recompute_modules:
  - core_attn
  recompute_num_layers: null
  rotary_base: 500000
  rotary_interleaved: false
  rotary_percent: 1.0
  scatter_embedding_sequence_parallel: true
  seq_len_interpolation_factor: null
  seq_length: 4096
  sequence_parallel: false
  share_embeddings_and_output_weights: false
  should_pad_vocab: false
  softmax_scale: null
  symmetric_ar_type: null
  tensor_model_parallel_size: 1
  test_mode: false
  timers: null
  tp_comm_atomic_ag: false
  tp_comm_atomic_rs: false
  tp_comm_bootstrap_backend: nccl
  tp_comm_bulk_dgrad: true
  tp_comm_bulk_wgrad: true
  tp_comm_overlap: false
  tp_comm_overlap_ag: true
  tp_comm_overlap_cfg: null
  tp_comm_overlap_disable_fc1: false
  tp_comm_overlap_disable_qkv: false
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_split_ag: true
  tp_comm_split_rs: true
  tp_only_amax_red: false
  transformer_impl: transformer_engine
  transformer_layer_spec:
    _call_: false
    _target_: megatron.bridge.models.gpt_provider.default_layer_spec
  use_cpu_initialization: false
  use_fused_weighted_squared_relu: false
  use_kitchen: false
  use_mamba_mem_eff_path: true
  use_ring_exchange_p2p: false
  use_te_activation_func: false
  use_te_rng_tracker: false
  use_transformer_engine_full_layer_spec: false
  use_transformer_engine_op_fuser: null
  variable_seq_lengths: false
  virtual_pipeline_model_parallel_size: null
  vocab_size: null
  wgrad_deferral_limit: 0
  window_size: null
nvrx_straggler: null
optimizer:
  _target_: megatron.core.optimizer.optimizer_config.OptimizerConfig
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_eps: 1.0e-05
  barrier_with_L1_time: false
  bf16: true
  clip_grad: 1.0
  config_logger_dir: ''
  decoupled_lr: null
  decoupled_min_lr: null
  exp_avg_dtype:
    _call_: false
    _target_: torch.float32
  exp_avg_sq_dtype:
    _call_: false
    _target_: torch.float32
  fp16: false
  fp8_recipe: null
  hysteresis: 2
  initial_loss_scale: 4294967296
  log_num_zeros_in_grad: false
  loss_scale: null
  loss_scale_window: 1000
  lr: 0.00025
  main_grads_dtype:
    _call_: false
    _target_: torch.float32
  main_params_dtype:
    _call_: false
    _target_: torch.float32
  min_loss_scale: 1.0
  min_lr: 2.5e-05
  optimizer: adam
  optimizer_cpu_offload: false
  optimizer_offload_fraction: 0.0
  overlap_cpu_optimizer_d2h_h2d: false
  overlap_param_gather: false
  overlap_param_gather_with_optimizer_step: false
  params_dtype:
    _call_: false
    _target_: torch.float32
  pin_cpu_grads: true
  pin_cpu_params: true
  reuse_grad_buf_for_mxfp8_param_ag: false
  sgd_momentum: 0.9
  store_param_remainders: true
  timers: null
  use_distributed_optimizer: true
  use_megatron_fsdp: false
  use_precision_aware_optimizer: false
  use_torch_optimizer_for_cpu_offload: false
  weight_decay: 0.1
peft: null
profiling:
  _target_: megatron.bridge.training.config.ProfilingConfig
  memory_snapshot_path: snapshot.pickle
  profile_ranks:
  - 0
  - 1
  profile_step_end: 10
  profile_step_start: 5
  record_memory_history: false
  record_shapes: true
  use_nsys_profiler: false
  use_pytorch_profiler: true
rerun_state_machine:
  _target_: megatron.bridge.training.config.RerunStateMachineConfig
  error_injection_rate: 0
  error_injection_type: transient_error
  rerun_mode: disabled
rng:
  _target_: megatron.bridge.training.config.RNGConfig
  data_parallel_random_init: false
  inference_rng_tracker: false
  seed: 42
  te_rng_tracker: false
scheduler:
  _target_: megatron.bridge.training.config.SchedulerConfig
  end_weight_decay: 0.033
  lr_decay_iters: 1168251
  lr_decay_steps: null
  lr_decay_style: cosine
  lr_warmup_fraction: null
  lr_warmup_init: 0.0
  lr_warmup_iters: 10
  lr_warmup_steps: null
  lr_wsd_decay_iters: null
  lr_wsd_decay_style: exponential
  override_opt_param_scheduler: true
  start_weight_decay: 0.033
  use_checkpoint_opt_param_scheduler: false
  wd_incr_steps: null
  weight_decay_incr_style: constant
  wsd_decay_steps: null
straggler: null
tokenizer:
  _target_: megatron.bridge.training.tokenizers.config.TokenizerConfig
  image_tag_type: null
  merge_file: null
  special_tokens: null
  tiktoken_num_special_tokens: 1000
  tiktoken_pattern: null
  tiktoken_special_tokens: null
  tokenizer_model: null
  tokenizer_prompt_format: null
  tokenizer_type: NullTokenizer
  vocab_extra_ids: 0
  vocab_file: null
  vocab_size: 32000
train:
  _target_: megatron.bridge.training.config.TrainingConfig
  check_weight_hash_across_dp_replicas_interval: null
  decrease_batch_size_if_needed: false
  empty_unused_memory_level: 0
  eval_interval: 2000
  eval_iters: 0
  exit_duration_in_mins: null
  exit_interval: null
  exit_signal:
    _args_:
    - 15
    _call_: true
    _target_: signal.Signals
  exit_signal_handler: false
  exit_signal_handler_for_dataloader: false
  global_batch_size: 8
  manual_gc: true
  manual_gc_eval: 100
  manual_gc_interval: 100
  micro_batch_size: 1
  rampup_batch_size: null
  skip_train: false
  train_iters: 20
  train_sync_interval: null

_target_: megatron.bridge.training.config.ConfigContainer
checkpoint:
  _target_: megatron.bridge.training.config.CheckpointConfig
  async_save: false
  ckpt_assume_constant_structure: false
  ckpt_convert_format: null
  ckpt_convert_save: null
  ckpt_format: torch_dist
  ckpt_step: null
  dist_ckpt_strictness: assume_ok_unexpected
  exit_on_missing_checkpoint: false
  finetune: false
  fully_parallel_load: false
  fully_parallel_save: true
  load: null
  load_main_params_from_ckpt: false
  load_optim: true
  load_rng: true
  non_persistent_ckpt_type: null
  non_persistent_global_ckpt_dir: null
  non_persistent_local_ckpt_algo: fully_parallel
  non_persistent_local_ckpt_dir: null
  non_persistent_save_interval: null
  pretrained_checkpoint: null
  replication: false
  replication_factor: 2
  replication_jump: null
  save: /opt/Megatron-Bridge/examples/recipes/llama/nemo_experiments/default/checkpoints
  save_interval: 2000
  save_optim: true
  save_rng: true
  use_checkpoint_args: false
  use_persistent_ckpt_worker: true
comm_overlap:
  _target_: megatron.bridge.training.comm_overlap.CommOverlapConfig
  align_param_gather: null
  batch_p2p_comm: null
  bucket_size: null
  data_parallel_size: null
  defer_embedding_wgrad_compute: null
  overlap_grad_reduce: null
  overlap_p2p_comm: null
  overlap_param_gather: null
  overlap_param_gather_with_optimizer_step: null
  tp_comm_bootstrap_backend: null
  tp_comm_overlap: false
  tp_comm_overlap_cfg: null
  wgrad_deferral_limit: null
dataset:
  _target_: megatron.bridge.training.config.GPTDatasetConfig
  add_extra_token_to_sequence: true
  blend: null
  blend_per_split: null
  create_attention_mask: true
  data_sharding: true
  dataloader_type: single
  drop_last_partial_validation_sequence: true
  eod_mask_loss: false
  full_validation: null
  mid_level_dataset_surplus: 0.005
  mmap_bin_files: true
  mock: true
  multiple_validation_sets: null
  num_dataset_builder_threads: 1
  num_workers: 8
  object_storage_cache_path: null
  path_to_cache: null
  persistent_workers: false
  pin_memory: true
  random_seed: 1234
  reset_attention_mask: false
  reset_position_ids: false
  sequence_length: 8192
  split: 1,1,1
  split_matrix:
  - - 0
    - 0.3333333333333333
  - - 0.3333333333333333
    - 0.6666666666666666
  - - 0.6666666666666666
    - 1.0
  tokenizer: null
ddp:
  _target_: megatron.core.distributed.distributed_data_parallel_config.DistributedDataParallelConfig
  align_param_gather: false
  average_in_collective: true
  bucket_size: null
  check_for_large_grads: false
  check_for_nan_in_grad: true
  data_parallel_sharding_strategy: no_shard
  fp8_param_gather: false
  fsdp_double_buffer: false
  grad_reduce_in_fp32: true
  gradient_reduce_div_fusion: true
  keep_fp8_transpose_cache: false
  nccl_ub: false
  num_distributed_optimizer_instances: 1
  outer_dp_sharding_strategy: no_shard
  overlap_grad_reduce: true
  overlap_param_gather: true
  pad_buckets_for_high_nccl_busbw: false
  preserve_fp32_weights: true
  reuse_grad_buf_for_mxfp8_param_ag: false
  suggested_communication_unit_size: null
  use_custom_fsdp: false
  use_distributed_optimizer: true
  use_megatron_fsdp: false
dist:
  _target_: megatron.bridge.training.config.DistributedInitConfig
  align_grad_reduce: true
  distributed_backend: nccl
  distributed_timeout_minutes: 10
  enable_megatron_core_experimental: false
  external_gpu_device_mapping: false
  high_priority_stream_groups: null
  lazy_init: false
  local_rank: 6
  nccl_communicator_config_path: null
  sharp_enabled_group: null
  use_gloo_process_groups: true
  use_sharp: false
  use_torch_fsdp2: false
  use_tp_pp_dp_mapping: false
ft: null
logger:
  _target_: megatron.bridge.training.config.LoggerConfig
  filter_warnings: true
  log_energy: false
  log_interval: 10
  log_loss_scale_to_tensorboard: true
  log_memory_to_tensorboard: false
  log_params_norm: false
  log_progress: false
  log_throughput: false
  log_timers_to_tensorboard: false
  log_validation_ppl_to_tensorboard: false
  log_world_size_to_tensorboard: false
  logging_level: 20
  modules_to_filter: null
  set_level_for_all_loggers: false
  tensorboard_dir: /opt/Megatron-Bridge/examples/recipes/llama/nemo_experiments/default/tb_logs
  tensorboard_log_interval: 1
  tensorboard_queue_size: 1000
  timing_log_level: 0
  timing_log_option: minmax
  wandb_entity: null
  wandb_exp_name: null
  wandb_project: null
  wandb_save_dir: null
mixed_precision: bf16_mixed
model:
  _target_: megatron.bridge.models.llama.llama_provider.Llama3ModelProvider8B
  account_for_embedding_in_pipeline_split: false
  account_for_loss_in_pipeline_split: false
  activation_func:
    _call_: false
    _target_: torch.nn.functional.silu
  activation_func_fp8_input_store: false
  add_bias_linear: false
  add_qkv_bias: false
  apply_query_key_layer_scaling: false
  apply_residual_connection_post_layernorm: false
  apply_rope_fusion: true
  async_tensor_model_parallel_allreduce: false
  attention_backend:
    _args_:
    - 5
    _call_: true
    _target_: megatron.core.transformer.enums.AttnBackend
  attention_dropout: 0.0
  attention_softmax_in_fp32: false
  autocast_dtype:
    _call_: false
    _target_: torch.float32
  barrier_with_L1_time: true
  batch_p2p_comm: true
  batch_p2p_sync: true
  bf16: false
  bias_activation_fusion: true
  bias_dropout_fusion: true
  calculate_per_token_loss: false
  clone_scatter_output_in_embedding: true
  config_logger_dir: ''
  context_parallel_size: 2
  cp_comm_type: null
  cpu_offloading: false
  cpu_offloading_activations: true
  cpu_offloading_num_layers: 0
  cpu_offloading_weights: true
  cross_entropy_fusion_impl: native
  cross_entropy_loss_fusion: true
  cuda_graph_retain_backward_graph: false
  cuda_graph_scope: full
  cuda_graph_use_single_mempool: false
  cuda_graph_warmup_steps: 3
  deallocate_pipeline_outputs: true
  defer_embedding_wgrad_compute: false
  delay_wgrad_compute: false
  deterministic_mode: false
  disable_bf16_reduced_precision_matmul: false
  disable_parameter_transpose_cache: false
  distribute_saved_activations: null
  embedding_init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.01
  embedding_init_method_std: 0.01
  enable_autocast: false
  enable_cuda_graph: false
  expert_model_parallel_size: 1
  expert_tensor_parallel_size: 1
  external_cuda_graph: false
  ffn_hidden_size: 14336
  finalize_model_grads_func: null
  first_last_layers_bf16: false
  flash_decode: false
  fp16: false
  fp16_lm_cross_entropy: false
  fp32_residual_connection: false
  fp8: null
  fp8_amax_compute_algo: most_recent
  fp8_amax_history_len: 1
  fp8_dot_product_attention: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_multi_head_attention: false
  fp8_param: false
  fp8_recipe: delayed
  fp8_wgrad: true
  gated_linear_unit: true
  generation_config: null
  grad_scale_func: null
  grad_sync_func: null
  gradient_accumulation_fusion: true
  hetereogenous_dist_checkpoint: false
  heterogeneous_block_specs: false
  hidden_dropout: 0.0
  hidden_size: 4096
  hierarchical_context_parallel_sizes: null
  inference_rng_tracker: false
  init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.01
  init_method_std: 0.01
  init_model_with_meta_device: false
  is_hybrid_model: false
  kv_channels: 128
  layernorm_epsilon: 1.0e-05
  layernorm_zero_centered_gamma: false
  make_vocab_size_divisible_by: 128
  mamba_head_dim: 64
  mamba_num_groups: 8
  mamba_num_heads: null
  mamba_state_dim: 128
  masked_softmax_fusion: true
  memory_efficient_layer_norm: false
  microbatch_group_size_per_vp_stage: 1
  mlp_chunks_for_prefill: 1
  moe_apply_probs_on_input: false
  moe_aux_loss_coeff: 0.0
  moe_deepep_num_sms: 20
  moe_enable_deepep: false
  moe_expert_capacity_factor: null
  moe_extended_tp: false
  moe_ffn_hidden_size: null
  moe_grouped_gemm: false
  moe_input_jitter_eps: null
  moe_layer_freq: 1
  moe_layer_recompute: false
  moe_pad_expert_input_to_capacity: false
  moe_per_layer_logging: false
  moe_permute_fusion: false
  moe_router_bias_update_rate: 0.001
  moe_router_dtype: null
  moe_router_enable_expert_bias: false
  moe_router_force_load_balancing: false
  moe_router_fusion: false
  moe_router_group_topk: null
  moe_router_load_balancing_type: aux_loss
  moe_router_num_groups: null
  moe_router_padding_for_fp8: false
  moe_router_pre_softmax: false
  moe_router_score_function: softmax
  moe_router_topk: 2
  moe_router_topk_limited_devices: null
  moe_router_topk_scaling_factor: null
  moe_shared_expert_intermediate_size: null
  moe_shared_expert_overlap: false
  moe_token_dispatcher_type: allgather
  moe_token_drop_policy: probs
  moe_token_dropping: false
  moe_use_legacy_grouped_gemm: false
  moe_z_loss_coeff: null
  mrope_section: null
  mtp_enabled: false
  mtp_loss_scaling_factor: null
  mtp_num_layers: null
  multi_latent_attention: false
  no_rope_freq: null
  no_sync_func: null
  normalization: RMSNorm
  num_attention_heads: 32
  num_layers: 32
  num_layers_at_end_in_bf16: 1
  num_layers_at_start_in_bf16: 1
  num_layers_in_first_pipeline_stage: null
  num_layers_in_last_pipeline_stage: null
  num_microbatches_with_partial_activation_checkpoints: null
  num_moe_experts: null
  num_query_groups: 8
  output_layer_init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.00125
  overlap_moe_expert_parallel_comm: false
  overlap_p2p_comm: false
  overlap_p2p_comm_warmup_flush: false
  parallel_output: true
  param_sync_func: null
  params_dtype:
    _call_: false
    _target_: torch.float32
  perform_initialization: true
  persist_layer_norm: false
  pipeline_dtype: null
  pipeline_model_parallel_comm_backend: null
  pipeline_model_parallel_layout: null
  pipeline_model_parallel_size: 1
  position_embedding_type: rope
  qk_layernorm: false
  quant_recipe: null
  recompute_granularity: null
  recompute_method: null
  recompute_modules:
  - core_attn
  recompute_num_layers: null
  rotary_base: 500000
  rotary_interleaved: false
  rotary_percent: 1.0
  scatter_embedding_sequence_parallel: true
  seq_len_interpolation_factor: null
  seq_length: 8192
  sequence_parallel: false
  share_embeddings_and_output_weights: false
  should_pad_vocab: false
  softmax_scale: null
  symmetric_ar_type: null
  tensor_model_parallel_size: 1
  test_mode: false
  timers: null
  tp_comm_atomic_ag: false
  tp_comm_atomic_rs: false
  tp_comm_bootstrap_backend: nccl
  tp_comm_bulk_dgrad: true
  tp_comm_bulk_wgrad: true
  tp_comm_overlap: false
  tp_comm_overlap_ag: true
  tp_comm_overlap_cfg: null
  tp_comm_overlap_disable_fc1: false
  tp_comm_overlap_disable_qkv: false
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_split_ag: true
  tp_comm_split_rs: true
  tp_only_amax_red: false
  transformer_impl: transformer_engine
  transformer_layer_spec:
    _call_: false
    _target_: megatron.bridge.models.gpt_provider.default_layer_spec
  use_cpu_initialization: false
  use_fused_weighted_squared_relu: false
  use_kitchen: false
  use_mamba_mem_eff_path: true
  use_ring_exchange_p2p: false
  use_te_activation_func: false
  use_te_rng_tracker: false
  use_transformer_engine_full_layer_spec: false
  use_transformer_engine_op_fuser: null
  variable_seq_lengths: false
  virtual_pipeline_model_parallel_size: null
  vocab_size: null
  wgrad_deferral_limit: 0
  window_size: null
nvrx_straggler: null
optimizer:
  _target_: megatron.core.optimizer.optimizer_config.OptimizerConfig
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_eps: 1.0e-05
  barrier_with_L1_time: false
  bf16: true
  clip_grad: 1.0
  config_logger_dir: ''
  decoupled_lr: null
  decoupled_min_lr: null
  exp_avg_dtype:
    _call_: false
    _target_: torch.float32
  exp_avg_sq_dtype:
    _call_: false
    _target_: torch.float32
  fp16: false
  fp8_recipe: null
  hysteresis: 2
  initial_loss_scale: 4294967296
  log_num_zeros_in_grad: false
  loss_scale: null
  loss_scale_window: 1000
  lr: 0.0003
  main_grads_dtype:
    _call_: false
    _target_: torch.float32
  main_params_dtype:
    _call_: false
    _target_: torch.float32
  min_loss_scale: 1.0
  min_lr: 3.0e-05
  optimizer: adam
  optimizer_cpu_offload: false
  optimizer_offload_fraction: 0.0
  overlap_cpu_optimizer_d2h_h2d: false
  overlap_param_gather: false
  overlap_param_gather_with_optimizer_step: false
  params_dtype:
    _call_: false
    _target_: torch.float32
  pin_cpu_grads: true
  pin_cpu_params: true
  reuse_grad_buf_for_mxfp8_param_ag: false
  sgd_momentum: 0.9
  store_param_remainders: true
  timers: null
  use_distributed_optimizer: true
  use_megatron_fsdp: false
  use_precision_aware_optimizer: false
  use_torch_optimizer_for_cpu_offload: false
  weight_decay: 0.1
peft: null
profiling: null
rerun_state_machine:
  _target_: megatron.bridge.training.config.RerunStateMachineConfig
  error_injection_rate: 0
  error_injection_type: transient_error
  rerun_mode: disabled
rng:
  _target_: megatron.bridge.training.config.RNGConfig
  data_parallel_random_init: false
  inference_rng_tracker: false
  seed: 1234
  te_rng_tracker: false
scheduler:
  _target_: megatron.bridge.training.config.SchedulerConfig
  end_weight_decay: 0.033
  lr_decay_iters: 1168251
  lr_decay_steps: null
  lr_decay_style: cosine
  lr_warmup_fraction: null
  lr_warmup_init: 0.0
  lr_warmup_iters: 2000
  lr_warmup_steps: null
  lr_wsd_decay_iters: null
  lr_wsd_decay_style: exponential
  override_opt_param_scheduler: true
  start_weight_decay: 0.033
  use_checkpoint_opt_param_scheduler: false
  wd_incr_steps: null
  weight_decay_incr_style: constant
  wsd_decay_steps: null
straggler: null
tokenizer:
  _target_: megatron.bridge.training.tokenizers.config.TokenizerConfig
  image_tag_type: null
  merge_file: null
  special_tokens: null
  tiktoken_num_special_tokens: 1000
  tiktoken_pattern: null
  tiktoken_special_tokens: null
  tokenizer_model: null
  tokenizer_prompt_format: null
  tokenizer_type: NullTokenizer
  vocab_extra_ids: 0
  vocab_file: null
  vocab_size: 32000
train:
  _target_: megatron.bridge.training.config.TrainingConfig
  check_weight_hash_across_dp_replicas_interval: null
  decrease_batch_size_if_needed: false
  empty_unused_memory_level: 0
  eval_interval: 2000
  eval_iters: 32
  exit_duration_in_mins: null
  exit_interval: null
  exit_signal:
    _args_:
    - 15
    _call_: true
    _target_: signal.Signals
  exit_signal_handler: false
  exit_signal_handler_for_dataloader: false
  global_batch_size: 512
  manual_gc: true
  manual_gc_eval: 100
  manual_gc_interval: 100
  micro_batch_size: 1
  rampup_batch_size: null
  skip_train: false
  train_iters: 1168251
  train_sync_interval: null

_target_: megatron.bridge.training.config.ConfigContainer
checkpoint:
  _target_: megatron.bridge.training.config.CheckpointConfig
  async_save: false
  ckpt_assume_constant_structure: false
  ckpt_convert_format: null
  ckpt_convert_save: null
  ckpt_format: torch_dist
  ckpt_step: null
  dist_ckpt_strictness: assume_ok_unexpected
  exit_on_missing_checkpoint: false
  finetune: false
  fully_parallel_load: false
  fully_parallel_save: true
  load: null
  load_main_params_from_ckpt: false
  load_optim: true
  load_rng: true
  non_persistent_ckpt_type: null
  non_persistent_global_ckpt_dir: null
  non_persistent_local_ckpt_algo: fully_parallel
  non_persistent_local_ckpt_dir: null
  non_persistent_save_interval: null
  pretrained_checkpoint: null
  replication: false
  replication_factor: 2
  replication_jump: null
  save: /opt/Megatron-Bridge/examples/recipes/llama/nemo_experiments/default/checkpoints
  save_interval: 2000
  save_optim: true
  save_rng: true
  use_checkpoint_args: false
  use_persistent_ckpt_worker: true
comm_overlap:
  _target_: megatron.bridge.training.comm_overlap.CommOverlapConfig
  align_param_gather: null
  batch_p2p_comm: null
  bucket_size: null
  data_parallel_size: null
  defer_embedding_wgrad_compute: null
  overlap_grad_reduce: null
  overlap_p2p_comm: null
  overlap_param_gather: null
  overlap_param_gather_with_optimizer_step: null
  tp_comm_bootstrap_backend: null
  tp_comm_overlap: false
  tp_comm_overlap_cfg: null
  wgrad_deferral_limit: null
dataset:
  _target_: megatron.bridge.training.config.GPTDatasetConfig
  add_extra_token_to_sequence: true
  blend: null
  blend_per_split: null
  create_attention_mask: true
  data_sharding: true
  dataloader_type: single
  drop_last_partial_validation_sequence: true
  eod_mask_loss: false
  full_validation: null
  mid_level_dataset_surplus: 0.005
  mmap_bin_files: true
  mock: true
  multiple_validation_sets: null
  num_dataset_builder_threads: 1
  num_workers: 8
  object_storage_cache_path: null
  path_to_cache: null
  persistent_workers: false
  pin_memory: true
  random_seed: 1234
  reset_attention_mask: false
  reset_position_ids: false
  sequence_length: 8192
  split: 1,1,1
  split_matrix:
  - - 0
    - 0.3333333333333333
  - - 0.3333333333333333
    - 0.6666666666666666
  - - 0.6666666666666666
    - 1.0
  tokenizer: null
ddp:
  _target_: megatron.core.distributed.distributed_data_parallel_config.DistributedDataParallelConfig
  align_param_gather: false
  average_in_collective: true
  bucket_size: null
  check_for_large_grads: false
  check_for_nan_in_grad: true
  data_parallel_sharding_strategy: no_shard
  fp8_param_gather: false
  fsdp_double_buffer: false
  grad_reduce_in_fp32: true
  gradient_reduce_div_fusion: true
  keep_fp8_transpose_cache: false
  nccl_ub: false
  num_distributed_optimizer_instances: 1
  outer_dp_sharding_strategy: no_shard
  overlap_grad_reduce: true
  overlap_param_gather: true
  pad_buckets_for_high_nccl_busbw: false
  preserve_fp32_weights: true
  reuse_grad_buf_for_mxfp8_param_ag: false
  suggested_communication_unit_size: null
  use_custom_fsdp: false
  use_distributed_optimizer: true
  use_megatron_fsdp: false
dist:
  _target_: megatron.bridge.training.config.DistributedInitConfig
  align_grad_reduce: true
  distributed_backend: nccl
  distributed_timeout_minutes: 10
  enable_megatron_core_experimental: false
  external_gpu_device_mapping: false
  high_priority_stream_groups: null
  lazy_init: false
  local_rank: 0
  nccl_communicator_config_path: null
  sharp_enabled_group: null
  use_gloo_process_groups: true
  use_sharp: false
  use_torch_fsdp2: false
  use_tp_pp_dp_mapping: false
ft: null
logger:
  _target_: megatron.bridge.training.config.LoggerConfig
  filter_warnings: true
  log_energy: false
  log_interval: 10
  log_loss_scale_to_tensorboard: true
  log_memory_to_tensorboard: false
  log_params_norm: false
  log_progress: false
  log_throughput: false
  log_timers_to_tensorboard: false
  log_validation_ppl_to_tensorboard: false
  log_world_size_to_tensorboard: false
  logging_level: 20
  modules_to_filter: null
  set_level_for_all_loggers: false
  tensorboard_dir: /opt/Megatron-Bridge/examples/recipes/llama/nemo_experiments/default/tb_logs
  tensorboard_log_interval: 1
  tensorboard_queue_size: 1000
  timing_log_level: 0
  timing_log_option: minmax
  wandb_entity: null
  wandb_exp_name: null
  wandb_project: null
  wandb_save_dir: null
mixed_precision: bf16_mixed
model:
  _target_: megatron.bridge.models.llama.llama_provider.Llama3ModelProvider8B
  account_for_embedding_in_pipeline_split: false
  account_for_loss_in_pipeline_split: false
  activation_func:
    _call_: false
    _target_: torch.nn.functional.silu
  activation_func_fp8_input_store: false
  add_bias_linear: false
  add_qkv_bias: false
  apply_query_key_layer_scaling: false
  apply_residual_connection_post_layernorm: false
  apply_rope_fusion: true
  async_tensor_model_parallel_allreduce: false
  attention_backend:
    _args_:
    - 5
    _call_: true
    _target_: megatron.core.transformer.enums.AttnBackend
  attention_dropout: 0.0
  attention_softmax_in_fp32: false
  autocast_dtype:
    _call_: false
    _target_: torch.float32
  barrier_with_L1_time: true
  batch_p2p_comm: true
  batch_p2p_sync: true
  bf16: false
  bias_activation_fusion: true
  bias_dropout_fusion: true
  calculate_per_token_loss: false
  clone_scatter_output_in_embedding: true
  config_logger_dir: ''
  context_parallel_size: 2
  cp_comm_type: null
  cpu_offloading: false
  cpu_offloading_activations: true
  cpu_offloading_num_layers: 0
  cpu_offloading_weights: true
  cross_entropy_fusion_impl: native
  cross_entropy_loss_fusion: true
  cuda_graph_retain_backward_graph: false
  cuda_graph_scope: full
  cuda_graph_use_single_mempool: false
  cuda_graph_warmup_steps: 3
  deallocate_pipeline_outputs: true
  defer_embedding_wgrad_compute: false
  delay_wgrad_compute: false
  deterministic_mode: false
  disable_bf16_reduced_precision_matmul: false
  disable_parameter_transpose_cache: false
  distribute_saved_activations: null
  embedding_init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.01
  embedding_init_method_std: 0.01
  enable_autocast: false
  enable_cuda_graph: false
  expert_model_parallel_size: 1
  expert_tensor_parallel_size: 1
  external_cuda_graph: false
  ffn_hidden_size: 14336
  finalize_model_grads_func: null
  first_last_layers_bf16: false
  flash_decode: false
  fp16: false
  fp16_lm_cross_entropy: false
  fp32_residual_connection: false
  fp8: null
  fp8_amax_compute_algo: most_recent
  fp8_amax_history_len: 1
  fp8_dot_product_attention: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_multi_head_attention: false
  fp8_param: false
  fp8_recipe: delayed
  fp8_wgrad: true
  gated_linear_unit: true
  generation_config: null
  grad_scale_func: null
  grad_sync_func: null
  gradient_accumulation_fusion: true
  hetereogenous_dist_checkpoint: false
  heterogeneous_block_specs: false
  hidden_dropout: 0.0
  hidden_size: 4096
  hierarchical_context_parallel_sizes: null
  inference_rng_tracker: false
  init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.01
  init_method_std: 0.01
  init_model_with_meta_device: false
  is_hybrid_model: false
  kv_channels: 128
  layernorm_epsilon: 1.0e-05
  layernorm_zero_centered_gamma: false
  make_vocab_size_divisible_by: 128
  mamba_head_dim: 64
  mamba_num_groups: 8
  mamba_num_heads: null
  mamba_state_dim: 128
  masked_softmax_fusion: true
  memory_efficient_layer_norm: false
  microbatch_group_size_per_vp_stage: 1
  mlp_chunks_for_prefill: 1
  moe_apply_probs_on_input: false
  moe_aux_loss_coeff: 0.0
  moe_deepep_num_sms: 20
  moe_enable_deepep: false
  moe_expert_capacity_factor: null
  moe_extended_tp: false
  moe_ffn_hidden_size: null
  moe_grouped_gemm: false
  moe_input_jitter_eps: null
  moe_layer_freq: 1
  moe_layer_recompute: false
  moe_pad_expert_input_to_capacity: false
  moe_per_layer_logging: false
  moe_permute_fusion: false
  moe_router_bias_update_rate: 0.001
  moe_router_dtype: null
  moe_router_enable_expert_bias: false
  moe_router_force_load_balancing: false
  moe_router_fusion: false
  moe_router_group_topk: null
  moe_router_load_balancing_type: aux_loss
  moe_router_num_groups: null
  moe_router_padding_for_fp8: false
  moe_router_pre_softmax: false
  moe_router_score_function: softmax
  moe_router_topk: 2
  moe_router_topk_limited_devices: null
  moe_router_topk_scaling_factor: null
  moe_shared_expert_intermediate_size: null
  moe_shared_expert_overlap: false
  moe_token_dispatcher_type: allgather
  moe_token_drop_policy: probs
  moe_token_dropping: false
  moe_use_legacy_grouped_gemm: false
  moe_z_loss_coeff: null
  mrope_section: null
  mtp_enabled: false
  mtp_loss_scaling_factor: null
  mtp_num_layers: null
  multi_latent_attention: false
  no_rope_freq: null
  no_sync_func: null
  normalization: RMSNorm
  num_attention_heads: 32
  num_layers: 32
  num_layers_at_end_in_bf16: 1
  num_layers_at_start_in_bf16: 1
  num_layers_in_first_pipeline_stage: null
  num_layers_in_last_pipeline_stage: null
  num_microbatches_with_partial_activation_checkpoints: null
  num_moe_experts: null
  num_query_groups: 8
  output_layer_init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.00125
  overlap_moe_expert_parallel_comm: false
  overlap_p2p_comm: false
  overlap_p2p_comm_warmup_flush: false
  parallel_output: true
  param_sync_func: null
  params_dtype:
    _call_: false
    _target_: torch.float32
  perform_initialization: true
  persist_layer_norm: false
  pipeline_dtype: null
  pipeline_model_parallel_comm_backend: null
  pipeline_model_parallel_layout: null
  pipeline_model_parallel_size: 1
  position_embedding_type: rope
  qk_layernorm: false
  quant_recipe: null
  recompute_granularity: null
  recompute_method: null
  recompute_modules:
  - core_attn
  recompute_num_layers: null
  rotary_base: 500000
  rotary_interleaved: false
  rotary_percent: 1.0
  scatter_embedding_sequence_parallel: true
  seq_len_interpolation_factor: null
  seq_length: 8192
  sequence_parallel: false
  share_embeddings_and_output_weights: false
  should_pad_vocab: false
  softmax_scale: null
  symmetric_ar_type: null
  tensor_model_parallel_size: 1
  test_mode: false
  timers: null
  tp_comm_atomic_ag: false
  tp_comm_atomic_rs: false
  tp_comm_bootstrap_backend: nccl
  tp_comm_bulk_dgrad: true
  tp_comm_bulk_wgrad: true
  tp_comm_overlap: false
  tp_comm_overlap_ag: true
  tp_comm_overlap_cfg: null
  tp_comm_overlap_disable_fc1: false
  tp_comm_overlap_disable_qkv: false
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_split_ag: true
  tp_comm_split_rs: true
  tp_only_amax_red: false
  transformer_impl: transformer_engine
  transformer_layer_spec:
    _call_: false
    _target_: megatron.bridge.models.gpt_provider.default_layer_spec
  use_cpu_initialization: false
  use_fused_weighted_squared_relu: false
  use_kitchen: false
  use_mamba_mem_eff_path: true
  use_ring_exchange_p2p: false
  use_te_activation_func: false
  use_te_rng_tracker: false
  use_transformer_engine_full_layer_spec: false
  use_transformer_engine_op_fuser: null
  variable_seq_lengths: false
  virtual_pipeline_model_parallel_size: null
  vocab_size: null
  wgrad_deferral_limit: 0
  window_size: null
nvrx_straggler: null
optimizer:
  _target_: megatron.core.optimizer.optimizer_config.OptimizerConfig
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_eps: 1.0e-05
  barrier_with_L1_time: false
  bf16: true
  clip_grad: 1.0
  config_logger_dir: ''
  decoupled_lr: null
  decoupled_min_lr: null
  exp_avg_dtype:
    _call_: false
    _target_: torch.float32
  exp_avg_sq_dtype:
    _call_: false
    _target_: torch.float32
  fp16: false
  fp8_recipe: null
  hysteresis: 2
  initial_loss_scale: 4294967296
  log_num_zeros_in_grad: false
  loss_scale: null
  loss_scale_window: 1000
  lr: 0.0003
  main_grads_dtype:
    _call_: false
    _target_: torch.float32
  main_params_dtype:
    _call_: false
    _target_: torch.float32
  min_loss_scale: 1.0
  min_lr: 3.0e-05
  optimizer: adam
  optimizer_cpu_offload: false
  optimizer_offload_fraction: 0.0
  overlap_cpu_optimizer_d2h_h2d: false
  overlap_param_gather: false
  overlap_param_gather_with_optimizer_step: false
  params_dtype:
    _call_: false
    _target_: torch.float32
  pin_cpu_grads: true
  pin_cpu_params: true
  reuse_grad_buf_for_mxfp8_param_ag: false
  sgd_momentum: 0.9
  store_param_remainders: true
  timers: null
  use_distributed_optimizer: true
  use_megatron_fsdp: false
  use_precision_aware_optimizer: false
  use_torch_optimizer_for_cpu_offload: false
  weight_decay: 0.1
peft: null
profiling: null
rerun_state_machine:
  _target_: megatron.bridge.training.config.RerunStateMachineConfig
  error_injection_rate: 0
  error_injection_type: transient_error
  rerun_mode: disabled
rng:
  _target_: megatron.bridge.training.config.RNGConfig
  data_parallel_random_init: false
  inference_rng_tracker: false
  seed: 1234
  te_rng_tracker: false
scheduler:
  _target_: megatron.bridge.training.config.SchedulerConfig
  end_weight_decay: 0.033
  lr_decay_iters: 1168251
  lr_decay_steps: null
  lr_decay_style: cosine
  lr_warmup_fraction: null
  lr_warmup_init: 0.0
  lr_warmup_iters: 2000
  lr_warmup_steps: null
  lr_wsd_decay_iters: null
  lr_wsd_decay_style: exponential
  override_opt_param_scheduler: true
  start_weight_decay: 0.033
  use_checkpoint_opt_param_scheduler: false
  wd_incr_steps: null
  weight_decay_incr_style: constant
  wsd_decay_steps: null
straggler: null
tokenizer:
  _target_: megatron.bridge.training.tokenizers.config.TokenizerConfig
  image_tag_type: null
  merge_file: null
  special_tokens: null
  tiktoken_num_special_tokens: 1000
  tiktoken_pattern: null
  tiktoken_special_tokens: null
  tokenizer_model: null
  tokenizer_prompt_format: null
  tokenizer_type: NullTokenizer
  vocab_extra_ids: 0
  vocab_file: null
  vocab_size: 32000
train:
  _target_: megatron.bridge.training.config.TrainingConfig
  check_weight_hash_across_dp_replicas_interval: null
  decrease_batch_size_if_needed: false
  empty_unused_memory_level: 0
  eval_interval: 2000
  eval_iters: 32
  exit_duration_in_mins: null
  exit_interval: null
  exit_signal:
    _args_:
    - 15
    _call_: true
    _target_: signal.Signals
  exit_signal_handler: false
  exit_signal_handler_for_dataloader: false
  global_batch_size: 512
  manual_gc: true
  manual_gc_eval: 100
  manual_gc_interval: 100
  micro_batch_size: 1
  rampup_batch_size: null
  skip_train: false
  train_iters: 1168251
  train_sync_interval: null

_target_: megatron.bridge.training.config.ConfigContainer
checkpoint:
  _target_: megatron.bridge.training.config.CheckpointConfig
  async_save: false
  ckpt_assume_constant_structure: false
  ckpt_convert_format: null
  ckpt_convert_save: null
  ckpt_format: torch_dist
  ckpt_step: null
  dist_ckpt_strictness: assume_ok_unexpected
  exit_on_missing_checkpoint: false
  finetune: false
  fully_parallel_load: false
  fully_parallel_save: true
  load: null
  load_main_params_from_ckpt: false
  load_optim: true
  load_rng: true
  non_persistent_ckpt_type: null
  non_persistent_global_ckpt_dir: null
  non_persistent_local_ckpt_algo: fully_parallel
  non_persistent_local_ckpt_dir: null
  non_persistent_save_interval: null
  pretrained_checkpoint: null
  replication: false
  replication_factor: 2
  replication_jump: null
  save: null
  save_interval: 2000
  save_optim: true
  save_rng: true
  use_checkpoint_args: false
  use_persistent_ckpt_worker: true
comm_overlap:
  _target_: megatron.bridge.training.comm_overlap.CommOverlapConfig
  align_param_gather: null
  batch_p2p_comm: null
  bucket_size: null
  data_parallel_size: null
  defer_embedding_wgrad_compute: null
  overlap_grad_reduce: null
  overlap_p2p_comm: null
  overlap_param_gather: null
  overlap_param_gather_with_optimizer_step: null
  tp_comm_bootstrap_backend: null
  tp_comm_overlap: false
  tp_comm_overlap_cfg: null
  wgrad_deferral_limit: null
dataset:
  _target_: megatron.bridge.training.config.GPTDatasetConfig
  add_extra_token_to_sequence: true
  blend: null
  blend_per_split: null
  create_attention_mask: true
  data_sharding: true
  dataloader_type: single
  drop_last_partial_validation_sequence: true
  eod_mask_loss: false
  full_validation: null
  mid_level_dataset_surplus: 0.005
  mmap_bin_files: true
  mock: true
  multiple_validation_sets: null
  num_dataset_builder_threads: 1
  num_workers: 8
  object_storage_cache_path: null
  path_to_cache: null
  persistent_workers: false
  pin_memory: true
  random_seed: 1234
  reset_attention_mask: false
  reset_position_ids: false
  sequence_length: 4096
  split: 1,1,1
  split_matrix:
  - - 0
    - 0.3333333333333333
  - - 0.3333333333333333
    - 0.6666666666666666
  - - 0.6666666666666666
    - 1.0
  tokenizer: null
ddp:
  _target_: megatron.core.distributed.distributed_data_parallel_config.DistributedDataParallelConfig
  align_param_gather: false
  average_in_collective: true
  bucket_size: null
  check_for_large_grads: false
  check_for_nan_in_grad: true
  data_parallel_sharding_strategy: no_shard
  fp8_param_gather: false
  fsdp_double_buffer: false
  grad_reduce_in_fp32: true
  gradient_reduce_div_fusion: true
  keep_fp8_transpose_cache: false
  nccl_ub: false
  num_distributed_optimizer_instances: 1
  outer_dp_sharding_strategy: no_shard
  overlap_grad_reduce: true
  overlap_param_gather: true
  pad_buckets_for_high_nccl_busbw: false
  preserve_fp32_weights: true
  reuse_grad_buf_for_mxfp8_param_ag: false
  suggested_communication_unit_size: null
  use_custom_fsdp: false
  use_distributed_optimizer: true
  use_megatron_fsdp: false
dist:
  _target_: megatron.bridge.training.config.DistributedInitConfig
  align_grad_reduce: true
  distributed_backend: nccl
  distributed_timeout_minutes: 10
  enable_megatron_core_experimental: false
  external_gpu_device_mapping: false
  high_priority_stream_groups: null
  lazy_init: false
  local_rank: 6
  nccl_communicator_config_path: null
  sharp_enabled_group: null
  use_gloo_process_groups: true
  use_sharp: false
  use_torch_fsdp2: false
  use_tp_pp_dp_mapping: false
ft: null
logger:
  _target_: megatron.bridge.training.config.LoggerConfig
  filter_warnings: true
  log_energy: false
  log_interval: 1
  log_loss_scale_to_tensorboard: true
  log_memory_to_tensorboard: false
  log_params_norm: false
  log_progress: false
  log_throughput: false
  log_timers_to_tensorboard: false
  log_validation_ppl_to_tensorboard: false
  log_world_size_to_tensorboard: false
  logging_level: 20
  modules_to_filter: null
  set_level_for_all_loggers: false
  tensorboard_dir: /opt/Megatron-Bridge/examples/recipes/llama/nemo_experiments/default/tb_logs
  tensorboard_log_interval: 1
  tensorboard_queue_size: 1000
  timing_log_level: 0
  timing_log_option: minmax
  wandb_entity: nvidia
  wandb_exp_name: llama3-8b-pretrain
  wandb_project: megatron-bridge
  wandb_save_dir: /lustre/fsw/portfolios/coreai/users/boxiangw/wandb/megatron-bridge/llama3-8b-pretrain
mixed_precision: bf16_mixed
model:
  _target_: megatron.bridge.models.llama.llama_provider.Llama3ModelProvider8B
  account_for_embedding_in_pipeline_split: false
  account_for_loss_in_pipeline_split: false
  activation_func:
    _call_: false
    _target_: torch.nn.functional.silu
  activation_func_fp8_input_store: false
  add_bias_linear: false
  add_qkv_bias: false
  apply_query_key_layer_scaling: false
  apply_residual_connection_post_layernorm: false
  apply_rope_fusion: true
  async_tensor_model_parallel_allreduce: false
  attention_backend:
    _args_:
    - 5
    _call_: true
    _target_: megatron.core.transformer.enums.AttnBackend
  attention_dropout: 0.0
  attention_softmax_in_fp32: false
  autocast_dtype:
    _call_: false
    _target_: torch.float32
  barrier_with_L1_time: true
  batch_p2p_comm: true
  batch_p2p_sync: true
  bf16: false
  bias_activation_fusion: true
  bias_dropout_fusion: true
  calculate_per_token_loss: false
  clone_scatter_output_in_embedding: true
  config_logger_dir: ''
  context_parallel_size: 2
  cp_comm_type: null
  cpu_offloading: false
  cpu_offloading_activations: true
  cpu_offloading_num_layers: 0
  cpu_offloading_weights: true
  cross_entropy_fusion_impl: native
  cross_entropy_loss_fusion: true
  cuda_graph_retain_backward_graph: false
  cuda_graph_scope: full
  cuda_graph_use_single_mempool: false
  cuda_graph_warmup_steps: 3
  deallocate_pipeline_outputs: true
  defer_embedding_wgrad_compute: false
  delay_wgrad_compute: false
  deterministic_mode: false
  disable_bf16_reduced_precision_matmul: false
  disable_parameter_transpose_cache: false
  distribute_saved_activations: null
  embedding_init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.01
  embedding_init_method_std: 0.01
  enable_autocast: false
  enable_cuda_graph: false
  expert_model_parallel_size: 1
  expert_tensor_parallel_size: 1
  external_cuda_graph: false
  ffn_hidden_size: 14336
  finalize_model_grads_func: null
  first_last_layers_bf16: false
  flash_decode: false
  fp16: false
  fp16_lm_cross_entropy: false
  fp32_residual_connection: false
  fp8: null
  fp8_amax_compute_algo: most_recent
  fp8_amax_history_len: 1
  fp8_dot_product_attention: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_multi_head_attention: false
  fp8_param: false
  fp8_recipe: delayed
  fp8_wgrad: true
  gated_linear_unit: true
  generation_config: null
  grad_scale_func: null
  grad_sync_func: null
  gradient_accumulation_fusion: true
  hetereogenous_dist_checkpoint: false
  heterogeneous_block_specs: false
  hidden_dropout: 0.0
  hidden_size: 4096
  hierarchical_context_parallel_sizes: null
  inference_rng_tracker: false
  init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.01
  init_method_std: 0.01
  init_model_with_meta_device: false
  is_hybrid_model: false
  kv_channels: 128
  layernorm_epsilon: 1.0e-05
  layernorm_zero_centered_gamma: false
  make_vocab_size_divisible_by: 128
  mamba_head_dim: 64
  mamba_num_groups: 8
  mamba_num_heads: null
  mamba_state_dim: 128
  masked_softmax_fusion: true
  memory_efficient_layer_norm: false
  microbatch_group_size_per_vp_stage: 1
  mlp_chunks_for_prefill: 1
  moe_apply_probs_on_input: false
  moe_aux_loss_coeff: 0.0
  moe_deepep_num_sms: 20
  moe_enable_deepep: false
  moe_expert_capacity_factor: null
  moe_extended_tp: false
  moe_ffn_hidden_size: null
  moe_grouped_gemm: false
  moe_input_jitter_eps: null
  moe_layer_freq: 1
  moe_layer_recompute: false
  moe_pad_expert_input_to_capacity: false
  moe_per_layer_logging: false
  moe_permute_fusion: false
  moe_router_bias_update_rate: 0.001
  moe_router_dtype: null
  moe_router_enable_expert_bias: false
  moe_router_force_load_balancing: false
  moe_router_fusion: false
  moe_router_group_topk: null
  moe_router_load_balancing_type: aux_loss
  moe_router_num_groups: null
  moe_router_padding_for_fp8: false
  moe_router_pre_softmax: false
  moe_router_score_function: softmax
  moe_router_topk: 2
  moe_router_topk_limited_devices: null
  moe_router_topk_scaling_factor: null
  moe_shared_expert_intermediate_size: null
  moe_shared_expert_overlap: false
  moe_token_dispatcher_type: allgather
  moe_token_drop_policy: probs
  moe_token_dropping: false
  moe_use_legacy_grouped_gemm: false
  moe_z_loss_coeff: null
  mrope_section: null
  mtp_enabled: false
  mtp_loss_scaling_factor: null
  mtp_num_layers: null
  multi_latent_attention: false
  no_rope_freq: null
  no_sync_func: null
  normalization: RMSNorm
  num_attention_heads: 32
  num_layers: 32
  num_layers_at_end_in_bf16: 1
  num_layers_at_start_in_bf16: 1
  num_layers_in_first_pipeline_stage: null
  num_layers_in_last_pipeline_stage: null
  num_microbatches_with_partial_activation_checkpoints: null
  num_moe_experts: null
  num_query_groups: 8
  output_layer_init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.00125
  overlap_moe_expert_parallel_comm: false
  overlap_p2p_comm: false
  overlap_p2p_comm_warmup_flush: false
  parallel_output: true
  param_sync_func: null
  params_dtype:
    _call_: false
    _target_: torch.float32
  perform_initialization: true
  persist_layer_norm: false
  pipeline_dtype: null
  pipeline_model_parallel_comm_backend: null
  pipeline_model_parallel_layout: null
  pipeline_model_parallel_size: 1
  position_embedding_type: rope
  qk_layernorm: false
  quant_recipe: null
  recompute_granularity: null
  recompute_method: null
  recompute_modules:
  - core_attn
  recompute_num_layers: null
  rotary_base: 500000
  rotary_interleaved: false
  rotary_percent: 1.0
  scatter_embedding_sequence_parallel: true
  seq_len_interpolation_factor: null
  seq_length: 4096
  sequence_parallel: false
  share_embeddings_and_output_weights: false
  should_pad_vocab: false
  softmax_scale: null
  symmetric_ar_type: null
  tensor_model_parallel_size: 1
  test_mode: false
  timers: null
  tp_comm_atomic_ag: false
  tp_comm_atomic_rs: false
  tp_comm_bootstrap_backend: nccl
  tp_comm_bulk_dgrad: true
  tp_comm_bulk_wgrad: true
  tp_comm_overlap: false
  tp_comm_overlap_ag: true
  tp_comm_overlap_cfg: null
  tp_comm_overlap_disable_fc1: false
  tp_comm_overlap_disable_qkv: false
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_split_ag: true
  tp_comm_split_rs: true
  tp_only_amax_red: false
  transformer_impl: transformer_engine
  transformer_layer_spec:
    _call_: false
    _target_: megatron.bridge.models.gpt_provider.default_layer_spec
  use_cpu_initialization: false
  use_fused_weighted_squared_relu: false
  use_kitchen: false
  use_mamba_mem_eff_path: true
  use_ring_exchange_p2p: false
  use_te_activation_func: false
  use_te_rng_tracker: false
  use_transformer_engine_full_layer_spec: false
  use_transformer_engine_op_fuser: null
  variable_seq_lengths: false
  virtual_pipeline_model_parallel_size: null
  vocab_size: null
  wgrad_deferral_limit: 0
  window_size: null
nvrx_straggler: null
optimizer:
  _target_: megatron.core.optimizer.optimizer_config.OptimizerConfig
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_eps: 1.0e-05
  barrier_with_L1_time: false
  bf16: true
  clip_grad: 1.0
  config_logger_dir: ''
  decoupled_lr: null
  decoupled_min_lr: null
  exp_avg_dtype:
    _call_: false
    _target_: torch.float32
  exp_avg_sq_dtype:
    _call_: false
    _target_: torch.float32
  fp16: false
  fp8_recipe: null
  hysteresis: 2
  initial_loss_scale: 4294967296
  log_num_zeros_in_grad: false
  loss_scale: null
  loss_scale_window: 1000
  lr: 0.00025
  main_grads_dtype:
    _call_: false
    _target_: torch.float32
  main_params_dtype:
    _call_: false
    _target_: torch.float32
  min_loss_scale: 1.0
  min_lr: 2.5e-05
  optimizer: adam
  optimizer_cpu_offload: false
  optimizer_offload_fraction: 0.0
  overlap_cpu_optimizer_d2h_h2d: false
  overlap_param_gather: false
  overlap_param_gather_with_optimizer_step: false
  params_dtype:
    _call_: false
    _target_: torch.float32
  pin_cpu_grads: true
  pin_cpu_params: true
  reuse_grad_buf_for_mxfp8_param_ag: false
  sgd_momentum: 0.9
  store_param_remainders: true
  timers: null
  use_distributed_optimizer: true
  use_megatron_fsdp: false
  use_precision_aware_optimizer: false
  use_torch_optimizer_for_cpu_offload: false
  weight_decay: 0.1
peft: null
profiling:
  _target_: megatron.bridge.training.config.ProfilingConfig
  memory_snapshot_path: snapshot.pickle
  profile_ranks:
  - 0
  - 1
  profile_step_end: 10
  profile_step_start: 5
  record_memory_history: false
  record_shapes: true
  use_nsys_profiler: false
  use_pytorch_profiler: true
rerun_state_machine:
  _target_: megatron.bridge.training.config.RerunStateMachineConfig
  error_injection_rate: 0
  error_injection_type: transient_error
  rerun_mode: disabled
rng:
  _target_: megatron.bridge.training.config.RNGConfig
  data_parallel_random_init: false
  inference_rng_tracker: false
  seed: 42
  te_rng_tracker: false
scheduler:
  _target_: megatron.bridge.training.config.SchedulerConfig
  end_weight_decay: 0.033
  lr_decay_iters: 1168251
  lr_decay_steps: null
  lr_decay_style: cosine
  lr_warmup_fraction: null
  lr_warmup_init: 0.0
  lr_warmup_iters: 10
  lr_warmup_steps: null
  lr_wsd_decay_iters: null
  lr_wsd_decay_style: exponential
  override_opt_param_scheduler: true
  start_weight_decay: 0.033
  use_checkpoint_opt_param_scheduler: false
  wd_incr_steps: null
  weight_decay_incr_style: constant
  wsd_decay_steps: null
straggler: null
tokenizer:
  _target_: megatron.bridge.training.tokenizers.config.TokenizerConfig
  image_tag_type: null
  merge_file: null
  special_tokens: null
  tiktoken_num_special_tokens: 1000
  tiktoken_pattern: null
  tiktoken_special_tokens: null
  tokenizer_model: null
  tokenizer_prompt_format: null
  tokenizer_type: NullTokenizer
  vocab_extra_ids: 0
  vocab_file: null
  vocab_size: 32000
train:
  _target_: megatron.bridge.training.config.TrainingConfig
  check_weight_hash_across_dp_replicas_interval: null
  decrease_batch_size_if_needed: false
  empty_unused_memory_level: 0
  eval_interval: 2000
  eval_iters: 0
  exit_duration_in_mins: null
  exit_interval: null
  exit_signal:
    _args_:
    - 15
    _call_: true
    _target_: signal.Signals
  exit_signal_handler: false
  exit_signal_handler_for_dataloader: false
  global_batch_size: 8
  manual_gc: true
  manual_gc_eval: 100
  manual_gc_interval: 100
  micro_batch_size: 1
  rampup_batch_size: null
  skip_train: false
  train_iters: 20
  train_sync_interval: null

_target_: megatron.bridge.training.config.ConfigContainer
checkpoint:
  _target_: megatron.bridge.training.config.CheckpointConfig
  async_save: false
  ckpt_assume_constant_structure: false
  ckpt_convert_format: null
  ckpt_convert_save: null
  ckpt_format: torch_dist
  ckpt_step: null
  dist_ckpt_strictness: assume_ok_unexpected
  exit_on_missing_checkpoint: false
  finetune: false
  fully_parallel_load: false
  fully_parallel_save: true
  load: null
  load_main_params_from_ckpt: false
  load_optim: true
  load_rng: true
  non_persistent_ckpt_type: null
  non_persistent_global_ckpt_dir: null
  non_persistent_local_ckpt_algo: fully_parallel
  non_persistent_local_ckpt_dir: null
  non_persistent_save_interval: null
  pretrained_checkpoint: null
  replication: false
  replication_factor: 2
  replication_jump: null
  save: null
  save_interval: 2000
  save_optim: true
  save_rng: true
  use_checkpoint_args: false
  use_persistent_ckpt_worker: true
comm_overlap:
  _target_: megatron.bridge.training.comm_overlap.CommOverlapConfig
  align_param_gather: null
  batch_p2p_comm: null
  bucket_size: null
  data_parallel_size: null
  defer_embedding_wgrad_compute: null
  overlap_grad_reduce: null
  overlap_p2p_comm: null
  overlap_param_gather: null
  overlap_param_gather_with_optimizer_step: null
  tp_comm_bootstrap_backend: null
  tp_comm_overlap: false
  tp_comm_overlap_cfg: null
  wgrad_deferral_limit: null
dataset:
  _target_: megatron.bridge.training.config.GPTDatasetConfig
  add_extra_token_to_sequence: true
  blend: null
  blend_per_split: null
  create_attention_mask: true
  data_sharding: true
  dataloader_type: single
  drop_last_partial_validation_sequence: true
  eod_mask_loss: false
  full_validation: null
  mid_level_dataset_surplus: 0.005
  mmap_bin_files: true
  mock: true
  multiple_validation_sets: null
  num_dataset_builder_threads: 1
  num_workers: 8
  object_storage_cache_path: null
  path_to_cache: null
  persistent_workers: false
  pin_memory: true
  random_seed: 1234
  reset_attention_mask: false
  reset_position_ids: false
  sequence_length: 4096
  split: 1,1,1
  split_matrix:
  - - 0
    - 0.3333333333333333
  - - 0.3333333333333333
    - 0.6666666666666666
  - - 0.6666666666666666
    - 1.0
  tokenizer: null
ddp:
  _target_: megatron.core.distributed.distributed_data_parallel_config.DistributedDataParallelConfig
  align_param_gather: false
  average_in_collective: true
  bucket_size: null
  check_for_large_grads: false
  check_for_nan_in_grad: true
  data_parallel_sharding_strategy: no_shard
  fp8_param_gather: false
  fsdp_double_buffer: false
  grad_reduce_in_fp32: true
  gradient_reduce_div_fusion: true
  keep_fp8_transpose_cache: false
  nccl_ub: false
  num_distributed_optimizer_instances: 1
  outer_dp_sharding_strategy: no_shard
  overlap_grad_reduce: true
  overlap_param_gather: true
  pad_buckets_for_high_nccl_busbw: false
  preserve_fp32_weights: true
  reuse_grad_buf_for_mxfp8_param_ag: false
  suggested_communication_unit_size: null
  use_custom_fsdp: false
  use_distributed_optimizer: true
  use_megatron_fsdp: false
dist:
  _target_: megatron.bridge.training.config.DistributedInitConfig
  align_grad_reduce: true
  distributed_backend: nccl
  distributed_timeout_minutes: 10
  enable_megatron_core_experimental: false
  external_gpu_device_mapping: false
  high_priority_stream_groups: null
  lazy_init: false
  local_rank: 0
  nccl_communicator_config_path: null
  sharp_enabled_group: null
  use_gloo_process_groups: true
  use_sharp: false
  use_torch_fsdp2: false
  use_tp_pp_dp_mapping: false
ft: null
logger:
  _target_: megatron.bridge.training.config.LoggerConfig
  filter_warnings: true
  log_energy: false
  log_interval: 1
  log_loss_scale_to_tensorboard: true
  log_memory_to_tensorboard: false
  log_params_norm: false
  log_progress: false
  log_throughput: false
  log_timers_to_tensorboard: false
  log_validation_ppl_to_tensorboard: false
  log_world_size_to_tensorboard: false
  logging_level: 20
  modules_to_filter: null
  set_level_for_all_loggers: false
  tensorboard_dir: /opt/Megatron-Bridge/examples/recipes/llama/nemo_experiments/default/tb_logs
  tensorboard_log_interval: 1
  tensorboard_queue_size: 1000
  timing_log_level: 0
  timing_log_option: minmax
  wandb_entity: nvidia
  wandb_exp_name: llama3-8b-pretrain
  wandb_project: megatron-bridge
  wandb_save_dir: /lustre/fsw/portfolios/coreai/users/boxiangw/wandb/megatron-bridge/llama3-8b-pretrain
mixed_precision: bf16_mixed
model:
  _target_: megatron.bridge.models.llama.llama_provider.Llama3ModelProvider8B
  account_for_embedding_in_pipeline_split: false
  account_for_loss_in_pipeline_split: false
  activation_func:
    _call_: false
    _target_: torch.nn.functional.silu
  activation_func_fp8_input_store: false
  add_bias_linear: false
  add_qkv_bias: false
  apply_query_key_layer_scaling: false
  apply_residual_connection_post_layernorm: false
  apply_rope_fusion: true
  async_tensor_model_parallel_allreduce: false
  attention_backend:
    _args_:
    - 5
    _call_: true
    _target_: megatron.core.transformer.enums.AttnBackend
  attention_dropout: 0.0
  attention_softmax_in_fp32: false
  autocast_dtype:
    _call_: false
    _target_: torch.float32
  barrier_with_L1_time: true
  batch_p2p_comm: true
  batch_p2p_sync: true
  bf16: false
  bias_activation_fusion: true
  bias_dropout_fusion: true
  calculate_per_token_loss: false
  clone_scatter_output_in_embedding: true
  config_logger_dir: ''
  context_parallel_size: 2
  cp_comm_type: null
  cpu_offloading: false
  cpu_offloading_activations: true
  cpu_offloading_num_layers: 0
  cpu_offloading_weights: true
  cross_entropy_fusion_impl: native
  cross_entropy_loss_fusion: true
  cuda_graph_retain_backward_graph: false
  cuda_graph_scope: full
  cuda_graph_use_single_mempool: false
  cuda_graph_warmup_steps: 3
  deallocate_pipeline_outputs: true
  defer_embedding_wgrad_compute: false
  delay_wgrad_compute: false
  deterministic_mode: false
  disable_bf16_reduced_precision_matmul: false
  disable_parameter_transpose_cache: false
  distribute_saved_activations: null
  embedding_init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.01
  embedding_init_method_std: 0.01
  enable_autocast: false
  enable_cuda_graph: false
  expert_model_parallel_size: 1
  expert_tensor_parallel_size: 1
  external_cuda_graph: false
  ffn_hidden_size: 14336
  finalize_model_grads_func: null
  first_last_layers_bf16: false
  flash_decode: false
  fp16: false
  fp16_lm_cross_entropy: false
  fp32_residual_connection: false
  fp8: null
  fp8_amax_compute_algo: most_recent
  fp8_amax_history_len: 1
  fp8_dot_product_attention: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_multi_head_attention: false
  fp8_param: false
  fp8_recipe: delayed
  fp8_wgrad: true
  gated_linear_unit: true
  generation_config: null
  grad_scale_func: null
  grad_sync_func: null
  gradient_accumulation_fusion: true
  hetereogenous_dist_checkpoint: false
  heterogeneous_block_specs: false
  hidden_dropout: 0.0
  hidden_size: 4096
  hierarchical_context_parallel_sizes: null
  inference_rng_tracker: false
  init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.01
  init_method_std: 0.01
  init_model_with_meta_device: false
  is_hybrid_model: false
  kv_channels: 128
  layernorm_epsilon: 1.0e-05
  layernorm_zero_centered_gamma: false
  make_vocab_size_divisible_by: 128
  mamba_head_dim: 64
  mamba_num_groups: 8
  mamba_num_heads: null
  mamba_state_dim: 128
  masked_softmax_fusion: true
  memory_efficient_layer_norm: false
  microbatch_group_size_per_vp_stage: 1
  mlp_chunks_for_prefill: 1
  moe_apply_probs_on_input: false
  moe_aux_loss_coeff: 0.0
  moe_deepep_num_sms: 20
  moe_enable_deepep: false
  moe_expert_capacity_factor: null
  moe_extended_tp: false
  moe_ffn_hidden_size: null
  moe_grouped_gemm: false
  moe_input_jitter_eps: null
  moe_layer_freq: 1
  moe_layer_recompute: false
  moe_pad_expert_input_to_capacity: false
  moe_per_layer_logging: false
  moe_permute_fusion: false
  moe_router_bias_update_rate: 0.001
  moe_router_dtype: null
  moe_router_enable_expert_bias: false
  moe_router_force_load_balancing: false
  moe_router_fusion: false
  moe_router_group_topk: null
  moe_router_load_balancing_type: aux_loss
  moe_router_num_groups: null
  moe_router_padding_for_fp8: false
  moe_router_pre_softmax: false
  moe_router_score_function: softmax
  moe_router_topk: 2
  moe_router_topk_limited_devices: null
  moe_router_topk_scaling_factor: null
  moe_shared_expert_intermediate_size: null
  moe_shared_expert_overlap: false
  moe_token_dispatcher_type: allgather
  moe_token_drop_policy: probs
  moe_token_dropping: false
  moe_use_legacy_grouped_gemm: false
  moe_z_loss_coeff: null
  mrope_section: null
  mtp_enabled: false
  mtp_loss_scaling_factor: null
  mtp_num_layers: null
  multi_latent_attention: false
  no_rope_freq: null
  no_sync_func: null
  normalization: RMSNorm
  num_attention_heads: 32
  num_layers: 32
  num_layers_at_end_in_bf16: 1
  num_layers_at_start_in_bf16: 1
  num_layers_in_first_pipeline_stage: null
  num_layers_in_last_pipeline_stage: null
  num_microbatches_with_partial_activation_checkpoints: null
  num_moe_experts: null
  num_query_groups: 8
  output_layer_init_method:
    _args_: []
    _partial_: true
    _target_: torch.nn.init.normal_
    mean: 0.0
    std: 0.00125
  overlap_moe_expert_parallel_comm: false
  overlap_p2p_comm: false
  overlap_p2p_comm_warmup_flush: false
  parallel_output: true
  param_sync_func: null
  params_dtype:
    _call_: false
    _target_: torch.float32
  perform_initialization: true
  persist_layer_norm: false
  pipeline_dtype: null
  pipeline_model_parallel_comm_backend: null
  pipeline_model_parallel_layout: null
  pipeline_model_parallel_size: 1
  position_embedding_type: rope
  qk_layernorm: false
  quant_recipe: null
  recompute_granularity: null
  recompute_method: null
  recompute_modules:
  - core_attn
  recompute_num_layers: null
  rotary_base: 500000
  rotary_interleaved: false
  rotary_percent: 1.0
  scatter_embedding_sequence_parallel: true
  seq_len_interpolation_factor: null
  seq_length: 4096
  sequence_parallel: false
  share_embeddings_and_output_weights: false
  should_pad_vocab: false
  softmax_scale: null
  symmetric_ar_type: null
  tensor_model_parallel_size: 1
  test_mode: false
  timers: null
  tp_comm_atomic_ag: false
  tp_comm_atomic_rs: false
  tp_comm_bootstrap_backend: nccl
  tp_comm_bulk_dgrad: true
  tp_comm_bulk_wgrad: true
  tp_comm_overlap: false
  tp_comm_overlap_ag: true
  tp_comm_overlap_cfg: null
  tp_comm_overlap_disable_fc1: false
  tp_comm_overlap_disable_qkv: false
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_split_ag: true
  tp_comm_split_rs: true
  tp_only_amax_red: false
  transformer_impl: transformer_engine
  transformer_layer_spec:
    _call_: false
    _target_: megatron.bridge.models.gpt_provider.default_layer_spec
  use_cpu_initialization: false
  use_fused_weighted_squared_relu: false
  use_kitchen: false
  use_mamba_mem_eff_path: true
  use_ring_exchange_p2p: false
  use_te_activation_func: false
  use_te_rng_tracker: false
  use_transformer_engine_full_layer_spec: false
  use_transformer_engine_op_fuser: null
  variable_seq_lengths: false
  virtual_pipeline_model_parallel_size: null
  vocab_size: null
  wgrad_deferral_limit: 0
  window_size: null
nvrx_straggler: null
optimizer:
  _target_: megatron.core.optimizer.optimizer_config.OptimizerConfig
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_eps: 1.0e-05
  barrier_with_L1_time: false
  bf16: true
  clip_grad: 1.0
  config_logger_dir: ''
  decoupled_lr: null
  decoupled_min_lr: null
  exp_avg_dtype:
    _call_: false
    _target_: torch.float32
  exp_avg_sq_dtype:
    _call_: false
    _target_: torch.float32
  fp16: false
  fp8_recipe: null
  hysteresis: 2
  initial_loss_scale: 4294967296
  log_num_zeros_in_grad: false
  loss_scale: null
  loss_scale_window: 1000
  lr: 0.00025
  main_grads_dtype:
    _call_: false
    _target_: torch.float32
  main_params_dtype:
    _call_: false
    _target_: torch.float32
  min_loss_scale: 1.0
  min_lr: 2.5e-05
  optimizer: adam
  optimizer_cpu_offload: false
  optimizer_offload_fraction: 0.0
  overlap_cpu_optimizer_d2h_h2d: false
  overlap_param_gather: false
  overlap_param_gather_with_optimizer_step: false
  params_dtype:
    _call_: false
    _target_: torch.float32
  pin_cpu_grads: true
  pin_cpu_params: true
  reuse_grad_buf_for_mxfp8_param_ag: false
  sgd_momentum: 0.9
  store_param_remainders: true
  timers: null
  use_distributed_optimizer: true
  use_megatron_fsdp: false
  use_precision_aware_optimizer: false
  use_torch_optimizer_for_cpu_offload: false
  weight_decay: 0.1
peft: null
profiling:
  _target_: megatron.bridge.training.config.ProfilingConfig
  memory_snapshot_path: snapshot.pickle
  profile_ranks:
  - 0
  - 1
  profile_step_end: 10
  profile_step_start: 5
  record_memory_history: false
  record_shapes: true
  use_nsys_profiler: false
  use_pytorch_profiler: true
rerun_state_machine:
  _target_: megatron.bridge.training.config.RerunStateMachineConfig
  error_injection_rate: 0
  error_injection_type: transient_error
  rerun_mode: disabled
rng:
  _target_: megatron.bridge.training.config.RNGConfig
  data_parallel_random_init: false
  inference_rng_tracker: false
  seed: 42
  te_rng_tracker: false
scheduler:
  _target_: megatron.bridge.training.config.SchedulerConfig
  end_weight_decay: 0.033
  lr_decay_iters: 1168251
  lr_decay_steps: null
  lr_decay_style: cosine
  lr_warmup_fraction: null
  lr_warmup_init: 0.0
  lr_warmup_iters: 10
  lr_warmup_steps: null
  lr_wsd_decay_iters: null
  lr_wsd_decay_style: exponential
  override_opt_param_scheduler: true
  start_weight_decay: 0.033
  use_checkpoint_opt_param_scheduler: false
  wd_incr_steps: null
  weight_decay_incr_style: constant
  wsd_decay_steps: null
straggler: null
tokenizer:
  _target_: megatron.bridge.training.tokenizers.config.TokenizerConfig
  image_tag_type: null
  merge_file: null
  special_tokens: null
  tiktoken_num_special_tokens: 1000
  tiktoken_pattern: null
  tiktoken_special_tokens: null
  tokenizer_model: null
  tokenizer_prompt_format: null
  tokenizer_type: NullTokenizer
  vocab_extra_ids: 0
  vocab_file: null
  vocab_size: 32000
train:
  _target_: megatron.bridge.training.config.TrainingConfig
  check_weight_hash_across_dp_replicas_interval: null
  decrease_batch_size_if_needed: false
  empty_unused_memory_level: 0
  eval_interval: 2000
  eval_iters: 0
  exit_duration_in_mins: null
  exit_interval: null
  exit_signal:
    _args_:
    - 15
    _call_: true
    _target_: signal.Signals
  exit_signal_handler: false
  exit_signal_handler_for_dataloader: false
  global_batch_size: 8
  manual_gc: true
  manual_gc_eval: 100
  manual_gc_interval: 100
  micro_batch_size: 1
  rampup_batch_size: null
  skip_train: false
  train_iters: 20
  train_sync_interval: null

> initializing torch distributed ...
NCCL version 2.27.3+cuda12.9
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank [Gloo] Rank 5 is connected to 73 peer ranks.  is connected to Expected number of connected peer ranks is : 77 peer ranks. Expected number of connected peer ranks is : 
7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : [Gloo] Rank 7
6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank [Gloo] Rank 21 is connected to  is connected to 33 peer ranks.  peer ranks. Expected number of connected peer ranks is : Expected number of connected peer ranks is : 33

[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. [Gloo] Rank Expected number of connected peer ranks is : 47 is connected to 
7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank [Gloo] Rank 53 is connected to  is connected to 77 peer ranks.  peer ranks. Expected number of connected peer ranks is : Expected number of connected peer ranks is : 77

[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : [Gloo] Rank 76
 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
> initialized tensor model parallel with size 1
> initialized pipeline model parallel with size 1
> setting random seeds to 42 ...
time to initialize megatron (seconds): 7.088
[after megatron is initialized] datetime: 2025-08-29 17:02:36 
> building NullTokenizer tokenizer ...
[after tokenizer is built] datetime: 2025-08-29 17:02:36 
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 7241732096
[after model, optimizer, and learning rate scheduler are built] datetime: 2025-08-29 17:02:36 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      160
    validation: 0
    test:       0
> building train, validation, and test datasets for GPT ...
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2025-08-29 17:02:37 
done with setup ...
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (238.91, 239.10)
    train/valid/test-data-iterators-setup ..........: (369.94, 454.21)
Training ...
Setting rerun_state_machine.current_iteration to 0...
NCCL version 2.27.3+cuda12.9
NCCL version 2.27.3+cuda12.9
NCCL version 2.27.3+cuda12.9
NCCL version 2.27.3+cuda12.9
NCCL version 2.27.3+cuda12.9
NCCL version 2.27.3+cuda12.9
NCCL version 2.27.3+cuda12.9
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> setting tensorboard ...
 [2025-08-29 17:02:50] iteration        1/      20 | consumed samples:            8 | elapsed time per iteration (ms): 12953.0 | learning rate: 2.500000E-05 | global batch size:     8 | lm loss: 1.056832E+01 | loss scale: 1.0 | grad norm: 20.435 | number of skipped iterations:   0 | number of nan iterations:   0 | > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
Number of parameters in transformer layers in billions:  6.98
Number of parameters in embedding layers in billions: 0.26
Total number of parameters in billions: 7.24
Number of parameters in most loaded shard in billions: 7.2420
Theoretical memory footprints: weight and optimizer=62158.57 MB

[Rank 0] (after 1 iterations) memory (MB) | allocated: 52563.13134765625 | max allocated: 55251.01708984375 | reserved: 56088.0 | max reserved: 56088.0
[Rank 1] (after 1 iterations) memory (MB) | allocated: 52562.16455078125 | max allocated: 55251.01708984375 | reserved: 55530.0 | max reserved: 55530.0
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 [2025-08-29 17:02:51] iteration        2/      20 | consumed samples:           16 | elapsed time per iteration (ms): 751.4 | learning rate: 5.000000E-05 | global batch size:     8 | lm loss: 1.057679E+01 | loss scale: 1.0 | grad norm: 16.560 | number of skipped iterations:   0 | number of nan iterations:   0 |
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 [2025-08-29 17:02:51] iteration        3/      20 | consumed samples:           24 | elapsed time per iteration (ms): 450.8 | learning rate: 7.500000E-05 | global batch size:     8 | lm loss: 8.717794E+00 | loss scale: 1.0 | grad norm: 65.837 | number of skipped iterations:   0 | number of nan iterations:   0 |
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 [2025-08-29 17:02:52] iteration        4/      20 | consumed samples:           32 | elapsed time per iteration (ms): 561.1 | learning rate: 1.000000E-04 | global batch size:     8 | lm loss: 7.790546E+00 | loss scale: 1.0 | grad norm: 124.174 | number of skipped iterations:   0 | number of nan iterations:   0 |
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 [2025-08-29 17:02:52] iteration        5/      20 | consumed samples:           40 | elapsed time per iteration (ms): 589.9 | learning rate: 1.250000E-04 | global batch size:     8 | lm loss: 7.444427E+00 | loss scale: 1.0 | grad norm: 388.653 | number of skipped iterations:   0 | number of nan iterations:   0 |
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 [2025-08-29 17:02:53] iteration        6/      20 | consumed samples:           48 | elapsed time per iteration (ms): 592.0 | learning rate: 1.500000E-04 | global batch size:     8 | lm loss: 9.106550E+00 | loss scale: 1.0 | grad norm: 95.078 | number of skipped iterations:   0 | number of nan iterations:   0 |
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 [2025-08-29 17:02:53] iteration        7/      20 | consumed samples:           56 | elapsed time per iteration (ms): 585.1 | learning rate: 1.750000E-04 | global batch size:     8 | lm loss: 9.259748E+00 | loss scale: 1.0 | grad norm: 263.300 | number of skipped iterations:   0 | number of nan iterations:   0 |
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 [2025-08-29 17:02:54] iteration        8/      20 | consumed samples:           64 | elapsed time per iteration (ms): 585.9 | learning rate: 2.000000E-04 | global batch size:     8 | lm loss: 9.654572E+00 | loss scale: 1.0 | grad norm: 48.924 | number of skipped iterations:   0 | number of nan iterations:   0 |
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 [2025-08-29 17:02:54] iteration        9/      20 | consumed samples:           72 | elapsed time per iteration (ms): 591.3 | learning rate: 2.250000E-04 | global batch size:     8 | lm loss: 1.013823E+01 | loss scale: 1.0 | grad norm: 42.755 | number of skipped iterations:   0 | number of nan iterations:   0 |
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 [2025-08-29 17:03:04] iteration       10/      20 | consumed samples:           80 | elapsed time per iteration (ms): 9141.6 | learning rate: 2.500000E-04 | global batch size:     8 | lm loss: 1.006481E+01 | loss scale: 1.0 | grad norm: 32.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 [2025-08-29 17:03:04] iteration       11/      20 | consumed samples:           88 | elapsed time per iteration (ms): 451.3 | learning rate: 2.500000E-04 | global batch size:     8 | lm loss: 9.745323E+00 | loss scale: 1.0 | grad norm: 2.872 | number of skipped iterations:   0 | number of nan iterations:   0 |
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 [2025-08-29 17:03:05] iteration       12/      20 | consumed samples:           96 | elapsed time per iteration (ms): 458.8 | learning rate: 2.500000E-04 | global batch size:     8 | lm loss: 1.017088E+01 | loss scale: 1.0 | grad norm: 31.177 | number of skipped iterations:   0 | number of nan iterations:   0 |
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 [2025-08-29 17:03:05] iteration       13/      20 | consumed samples:          104 | elapsed time per iteration (ms): 449.0 | learning rate: 2.500000E-04 | global batch size:     8 | lm loss: 9.546312E+00 | loss scale: 1.0 | grad norm: 2.331 | number of skipped iterations:   0 | number of nan iterations:   0 |
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 [2025-08-29 17:03:05] iteration       14/      20 | consumed samples:          112 | elapsed time per iteration (ms): 455.2 | learning rate: 2.500000E-04 | global batch size:     8 | lm loss: 9.086151E+00 | loss scale: 1.0 | grad norm: 7.441 | number of skipped iterations:   0 | number of nan iterations:   0 |
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 [2025-08-29 17:03:06] iteration       15/      20 | consumed samples:          120 | elapsed time per iteration (ms): 452.1 | learning rate: 2.500000E-04 | global batch size:     8 | lm loss: 8.780718E+00 | loss scale: 1.0 | grad norm: 1.164 | number of skipped iterations:   0 | number of nan iterations:   0 |
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 [2025-08-29 17:03:06] iteration       16/      20 | consumed samples:          128 | elapsed time per iteration (ms): 453.0 | learning rate: 2.500000E-04 | global batch size:     8 | lm loss: 8.512004E+00 | loss scale: 1.0 | grad norm: 0.640 | number of skipped iterations:   0 | number of nan iterations:   0 |
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 [2025-08-29 17:03:07] iteration       17/      20 | consumed samples:          136 | elapsed time per iteration (ms): 451.5 | learning rate: 2.500000E-04 | global batch size:     8 | lm loss: 8.441628E+00 | loss scale: 1.0 | grad norm: 0.407 | number of skipped iterations:   0 | number of nan iterations:   0 |
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 [2025-08-29 17:03:07] iteration       18/      20 | consumed samples:          144 | elapsed time per iteration (ms): 448.4 | learning rate: 2.500000E-04 | global batch size:     8 | lm loss: 8.180581E+00 | loss scale: 1.0 | grad norm: 0.362 | number of skipped iterations:   0 | number of nan iterations:   0 |
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 [2025-08-29 17:03:08] iteration       19/      20 | consumed samples:          152 | elapsed time per iteration (ms): 449.4 | learning rate: 2.500000E-04 | global batch size:     8 | lm loss: 8.258690E+00 | loss scale: 1.0 | grad norm: 0.303 | number of skipped iterations:   0 | number of nan iterations:   0 |
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
 [2025-08-29 17:03:08] iteration       20/      20 | consumed samples:          160 | elapsed time per iteration (ms): 449.2 | learning rate: 2.500000E-04 | global batch size:     8 | lm loss: 8.226928E+00 | loss scale: 1.0 | grad norm: 0.315 | number of skipped iterations:   0 | number of nan iterations:   0 |
[after training is done] datetime: 2025-08-29 17:03:08 
